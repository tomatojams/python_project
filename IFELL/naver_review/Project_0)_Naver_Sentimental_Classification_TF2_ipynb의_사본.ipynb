{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "553px",
    "left": "792px",
    "right": "61px",
    "top": "71px",
    "width": "375px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oq4UnWajC9u"
   },
   "source": [
    "# Sentiment Classification\n",
    "\n",
    "### Task\n",
    "* 네이버에서 영화평을 가지고 positive/negative인지 구분해보자.\n",
    "* 데이터 불러오기를 제외한 딥러닝 트레이닝 과정을 직접 구현해보는 것이 목표 입니다.\n",
    "\n",
    "### Dataset\n",
    "* [Naver sentiment movie corpus v1.0](https://github.com/e9t/nsmc/)\n",
    "\n",
    "### Base code\n",
    "* Dataset: train, val, test로 split\n",
    "* Input data shape: (`batch_size`, `max_sequence_length`)\n",
    "* Output data shape: (`batch_size`, 1)\n",
    "* Training\n",
    "* Evaluation\n",
    "\n",
    "### Try some techniques\n",
    "* Training-epochs 조절\n",
    "* Change model architectures (Custom model)\n",
    "  * Use another cells (LSTM, GRU, etc.)\n",
    "  * Use dropout layers\n",
    "* Embedding size 조절\n",
    "  * 또는 one-hot vector로 학습\n",
    "* Number of words in the vocabulary 변화\n",
    "* `pad` 옵션 변화\n",
    "* Data augmentation (if possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OW9rRFr4jC9w"
   },
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ibO_zjJmvN3s",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "989491d4-1eb8-4536-8828-9dbbdf3e21ad",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:07.495328Z",
     "start_time": "2024-01-17T02:39:07.482540Z"
    }
   },
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install sentencepiece"
   ],
   "metadata": {
    "id": "CgCS8iATktuT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2449f7f2-e1d1-47ed-88ab-669d4f0d0c5d",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:09.401879Z",
     "start_time": "2024-01-17T02:39:07.492417Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (0.1.99)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ct7ZVZ2EjC9x",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:14.311294Z",
     "start_time": "2024-01-17T02:39:09.409046Z"
    }
   },
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import urllib.request\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from collections import Counter, defaultdict\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-e3ucn5_jC90"
   },
   "source": [
    "## Load Data\n",
    "\n",
    "* ratings_train.txt: 훈련용으로 사용되는 15만 개의 리뷰\n",
    "* ratings_test.txt: 테스트용으로 보류된 5만 개의 리뷰\n",
    "* 모든 리뷰는 140자 이내입니다\n",
    "* 각 감정 클래스는 동등하게 샘플링되었습니다 (즉, 무작위 추측은 50%의 정확도를 보입니다)\n",
    "* 10만 개의 부정적 리뷰 (원래 1-4점의 리뷰)\n",
    "* 10만 개의 긍정적 리뷰 (원래 9-10점의 리뷰)\n",
    "* 중립적 리뷰 (원래 5-8점의 리뷰)는 제외되었습니다\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
   ],
   "metadata": {
    "id": "FBqk_wq-RIYp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ab35b0a1-4577-4407-b2ee-ca79bc5ae173",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:18.358173Z",
     "start_time": "2024-01-17T02:39:14.307319Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "('ratings_test.txt', <http.client.HTTPMessage at 0x110a0cdf0>)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_data = pd.read_table('ratings_train.txt')\n",
    "train_data = train_data.dropna()\n",
    "test_data = pd.read_table('ratings_test.txt')\n",
    "test_data = test_data.dropna()"
   ],
   "metadata": {
    "id": "pB4ds-e1RavF",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:19.107073Z",
     "start_time": "2024-01-17T02:39:18.360770Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_data.head()"
   ],
   "metadata": {
    "id": "xdMHf_rOSGXS",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "outputId": "abdbfd6d-1345-4a65-d24f-0e2615baaf27",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:19.126312Z",
     "start_time": "2024-01-17T02:39:19.102787Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "         id                                           document  label\n0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>document</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9976970</td>\n      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3819312</td>\n      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10265843</td>\n      <td>너무재밓었다그래서보는것을추천한다</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9045019</td>\n      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6483659</td>\n      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_data.head()"
   ],
   "metadata": {
    "id": "rGCz3J5N3JeT",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "outputId": "6806da1a-c1e5-4e84-fc48-681f6fe2aad4",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:19.128595Z",
     "start_time": "2024-01-17T02:39:19.116156Z"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "        id                                           document  label\n0  6270596                                                굳 ㅋ      1\n1  9274899                               GDNTOPCLASSINTHECLUB      0\n2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>document</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6270596</td>\n      <td>굳 ㅋ</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9274899</td>\n      <td>GDNTOPCLASSINTHECLUB</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8544678</td>\n      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6825595</td>\n      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6723715</td>\n      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenizing\n"
   ],
   "metadata": {
    "id": "Eq0jwEhcSThR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('naver_review.model')  # 모델 경로 설정\n",
    "\n",
    "# 토크나이저 함수 정의\n",
    "def tokenizer(text):\n",
    "    return sp.encode_as_pieces(text)"
   ],
   "metadata": {
    "id": "WEfUnWZyT8px",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:23.754901Z",
     "start_time": "2024-01-17T02:39:23.718041Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_data"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "oBepfmzlIvBV",
    "outputId": "53dd704b-39da-47a2-f715-e47e177f2dbc",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:24.828019Z",
     "start_time": "2024-01-17T02:39:24.796220Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "              id                                           document  label\n0        9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n1        3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n3        9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n...          ...                                                ...    ...\n149995   6222902                                인간이 문제지.. 소는 뭔죄인가..      0\n149996   8549745                                      평점이 너무 낮아서...      1\n149997   9311800                    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n149998   2376369                        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n149999   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n\n[149995 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>document</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9976970</td>\n      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3819312</td>\n      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10265843</td>\n      <td>너무재밓었다그래서보는것을추천한다</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9045019</td>\n      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6483659</td>\n      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>149995</th>\n      <td>6222902</td>\n      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>149996</th>\n      <td>8549745</td>\n      <td>평점이 너무 낮아서...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>149997</th>\n      <td>9311800</td>\n      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>149998</th>\n      <td>2376369</td>\n      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>149999</th>\n      <td>9619869</td>\n      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>149995 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for i, (line) in enumerate(train_data['document']):\n",
    "    print(f\"Original Sentence: {line}\")\n",
    "    print(f\"Tokenized Sentence: {sp.encode_as_pieces(line)}\")\n",
    "    print(f\"Token IDs: {sp.encode_as_ids(line)}\")\n",
    "    if i == 5:\n",
    "        break"
   ],
   "metadata": {
    "id": "xPWz7QU_UFBK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "dfbfb46c-3208-437d-971c-ab50300f4130",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:26.220279Z",
     "start_time": "2024-01-17T02:39:26.210955Z"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: 아 더빙.. 진짜 짜증나네요 목소리\n",
      "Tokenized Sentence: ['▁아', '▁더빙', '..', '▁진짜', '▁짜증나', '네요', '▁목소리']\n",
      "Token IDs: [14, 1226, 7, 88, 2990, 55, 2393]\n",
      "Original Sentence: 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "Tokenized Sentence: ['▁흠', '...', '포스터', '보고', '▁초딩', '영화', '줄', '....', '오', '버', '연기', '조차', '▁가볍', '지', '▁않', '구나']\n",
      "Token IDs: [1949, 16, 5829, 233, 1469, 10, 6601, 47, 6454, 6564, 355, 2103, 2338, 6387, 108, 508]\n",
      "Original Sentence: 너무재밓었다그래서보는것을추천한다\n",
      "Tokenized Sentence: ['▁너무', '재', '밓', '었다', '그래서', '보는', '것을', '추천', '한다']\n",
      "Token IDs: [39, 6416, 1, 164, 4556, 515, 1409, 2176, 367]\n",
      "Original Sentence: 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
      "Tokenized Sentence: ['▁교', '도', '소', '▁이야기', '구', '먼', '▁..', '솔직히', '▁재미는', '▁없다', '..', '평점', '▁조', '정']\n",
      "Token IDs: [729, 6392, 6487, 372, 6478, 6879, 516, 5346, 1686, 309, 7, 1187, 188, 6424]\n",
      "Original Sentence: 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n",
      "Tokenized Sentence: ['▁사이', '몬', '페', '그', '의', '▁익', '살', '스런', '▁연기가', '▁돋보', '였던', '▁영화', '!', '스', '파이', '더', '맨', '에서', '▁늙', '어', '보이', '기만', '▁했던', '▁커', '스', '틴', '▁던', '스트', '가', '▁너무나도', '▁이뻐', '보', '였다']\n",
      "Token IDs: [2855, 7223, 6958, 6415, 6400, 3665, 6598, 2275, 776, 1967, 2639, 11, 6409, 6418, 2182, 6470, 6720, 62, 2895, 6399, 2780, 1375, 3042, 1299, 6418, 7124, 3527, 692, 6391, 2677, 3827, 6398, 475]\n",
      "Original Sentence: 막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.\n",
      "Tokenized Sentence: ['▁막', '▁걸', '음', '마', '▁', '뗀', '▁3', '세', '부터', '▁초등', '학교', '▁1', '학년', '생', '인', '▁8', '살', '용', '영화', '.', 'ᄏᄏᄏ', '...', '별', '반', '개도', '▁아까움', '.']\n",
      "Token IDs: [419, 519, 6436, 6427, 6371, 1, 34, 6512, 340, 2325, 979, 8, 3847, 6460, 6414, 15, 6598, 6510, 10, 6372, 472, 16, 6551, 6534, 1521, 2060, 6372]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "eos_token = '[SEP]'\n",
    "eos_id = sp.piece_to_id(eos_token)\n",
    "\n",
    "print(f\"토큰 '{eos_token}'의 ID: {eos_id}\")"
   ],
   "metadata": {
    "id": "cHyID7qIRoWz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a11932eb-ea5c-4506-cded-08a949ff6fd8",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:26.890762Z",
     "start_time": "2024-01-17T02:39:26.863663Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰 '[SEP]'의 ID: 4\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sp.encode_as_ids(['[EOS]'])"
   ],
   "metadata": {
    "id": "fAB1othYFZi7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7359d172-65a0-40fb-cbcb-7ccab0e380ff",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:27.192815Z",
     "start_time": "2024-01-17T02:39:27.183895Z"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "[[4379, 7127, 6566, 6866, 7344]]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train_text = []\n",
    "for i, line in enumerate(train_data['document']):\n",
    "    # sp.encode_as_ids(line)의 결과를 TensorFlow 텐서로 변환\n",
    "    train_text.append(tf.convert_to_tensor(sp.encode_as_ids(line), dtype=tf.int32))\n",
    "\n",
    "test_text = []\n",
    "for i, line in enumerate(test_data['document']):\n",
    "    test_text.append(tf.convert_to_tensor(sp.encode_as_ids(line), dtype=tf.int32))"
   ],
   "metadata": {
    "id": "KFV87kQjeEDC",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:38.324677Z",
     "start_time": "2024-01-17T02:39:27.988009Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 11:39:27.888720: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-01-17 11:39:27.889623: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(train_text), len(test_text))"
   ],
   "metadata": {
    "id": "I4OeUYsx9OQ1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "83afc08e-94f6-4d6d-fb23-59685b00544a",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:38.329259Z",
     "start_time": "2024-01-17T02:39:38.299247Z"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149995 49997\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8yYy2OPjC-a"
   },
   "source": [
    "### Padding and truncating data using pad sequences\n",
    "* 전부 길이가 다른 리뷰들의 길이를 통일해주자"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "max_seq_length = 256"
   ],
   "metadata": {
    "id": "SPV-UWrwQMvX",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:38.372672Z",
     "start_time": "2024-01-17T02:39:38.308234Z"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_data_pad = pad_sequences(train_text,max_seq_length)\n",
    "test_data_pad = pad_sequences(test_text,max_seq_length)\n",
    "\n",
    "print(train_data_pad.shape, test_data_pad.shape)"
   ],
   "metadata": {
    "id": "_QB_qLXV3u_C",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d2366a2e-ff03-4bd2-c15d-6825fc85fd9e",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:57.432959Z",
     "start_time": "2024-01-17T02:39:38.344523Z"
    }
   },
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149995, 256) (49997, 256)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CPqHeI9y7g2"
   },
   "source": [
    "### Dataset 구성"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "\n",
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data_pad,train_data['label']))\n",
    "train_dataset = train_dataset.shuffle(10000).repeat().batch(batch_size=batch_size)\n",
    "print(train_dataset)\n",
    "\n",
    "# for test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data_pad,test_data['label']))\n",
    "test_dataset = test_dataset.batch(batch_size=batch_size)\n",
    "print(test_dataset)"
   ],
   "metadata": {
    "id": "pi43bh7G7NwG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3e582e6a-fd2d-471b-fcfd-a7f13bb6c5c2",
    "ExecuteTime": {
     "end_time": "2024-01-17T02:39:57.538406Z",
     "start_time": "2024-01-17T02:39:57.463665Z"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 256), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 256), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMG2MOXUjC-y"
   },
   "source": [
    "## Build the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5KBM5sdjC-v"
   },
   "source": [
    "## Setup hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "kargs = {'model_name': 'BERT',\n",
    "         'num_layers': 4,\n",
    "         'd_model': 32,\n",
    "         'num_heads': 8,\n",
    "         'dff': 64,\n",
    "         'input_vocab_size': sp.get_piece_size(),\n",
    "         'target_vocab_size': sp.get_piece_size(),\n",
    "         'maximum_position_encoding': 10000,\n",
    "         'segment_encoding': 2,\n",
    "         'end_token_idx': sp.piece_to_id('[EOS]'),\n",
    "         'rate': 0.1\n",
    "        }\n",
    "\n",
    "d_model =32\n",
    "dff = 64"
   ],
   "metadata": {
    "id": "cnhhKDlP8CqT",
    "ExecuteTime": {
     "end_time": "2024-01-17T03:46:39.804105Z",
     "start_time": "2024-01-17T03:46:39.751569Z"
    }
   },
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * i//2) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ],
   "metadata": {
    "id": "LvflfLNJsmYU",
    "ExecuteTime": {
     "end_time": "2024-01-17T03:46:39.920499Z",
     "start_time": "2024-01-17T03:46:39.906308Z"
    }
   },
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ],
   "metadata": {
    "id": "VMSqmsT9ssT9",
    "ExecuteTime": {
     "end_time": "2024-01-17T03:46:40.052028Z",
     "start_time": "2024-01-17T03:46:40.039568Z"
    }
   },
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead)\n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable\n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ],
   "metadata": {
    "id": "e5fQof6usttB",
    "ExecuteTime": {
     "end_time": "2024-01-17T03:46:40.207727Z",
     "start_time": "2024-01-17T03:46:40.193340Z"
    }
   },
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = kargs['num_heads']\n",
    "        self.d_model = kargs['d_model']\n",
    "\n",
    "        assert self.d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        self.wk = tf.keras.layers.Dense(kargs['d_model'])\n",
    "        self.wv = tf.keras.layers.Dense(kargs['d_model'])\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(kargs['d_model'])\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ],
   "metadata": {
    "id": "xWju3MTjsub-",
    "ExecuteTime": {
     "end_time": "2024-01-17T03:46:40.427783Z",
     "start_time": "2024-01-17T03:46:40.394859Z"
    }
   },
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# def point_wise_feed_forward_network(**kargs):\n",
    "#     return tf.keras.Sequential([\n",
    "#             tf.keras.layers.Conv1D(batch_size,256,2048),\n",
    "#             tf.keras.layers.Conv1D(batch_size,256,512)\n",
    "#         ])\n"
   ],
   "metadata": {
    "id": "10a_ZGMasyvr",
    "ExecuteTime": {
     "end_time": "2024-01-17T03:46:40.550531Z",
     "start_time": "2024-01-17T03:46:40.535306Z"
    }
   },
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def point_wise_feed_forward_network(**kargs):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=dff, kernel_size=1, activation='relu'),# (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Conv1D(filters=d_model, kernel_size=1, activation='linear')  # (batch_size, seq_len, d_model)# You can change 'linear' to another activation function if needed\n",
    "    ])\n"
   ],
   "metadata": {
    "id": "nUGkA_PPQcYU",
    "ExecuteTime": {
     "end_time": "2024-01-17T03:46:40.728558Z",
     "start_time": "2024-01-17T03:46:40.711791Z"
    }
   },
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(**kargs)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(**kargs)\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2, attn_output"
   ],
   "metadata": {
    "id": "MAeS5HH8Sb0L",
    "ExecuteTime": {
     "end_time": "2024-01-17T03:46:40.890426Z",
     "start_time": "2024-01-17T03:46:40.874059Z"
    }
   },
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kargs):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = kargs['d_model']\n",
    "        self.num_layers = kargs['num_layers']\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(kargs['input_vocab_size'],\n",
    "                                                   self.d_model)\n",
    "        self.seg_encoding = tf.keras.layers.Embedding(kargs['segment_encoding'], # 문장구분 0,1로 나누는 임베딩?\n",
    "                                                   self.d_model)\n",
    "        self.pos_encoding = positional_encoding(kargs['maximum_position_encoding'],\n",
    "                                                     self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(**kargs)\n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
    "\n",
    "    def get_seg_data(self, data, token_id=4):\n",
    "        token_found = tf.cumsum(tf.cast(data == token_id, tf.int32), axis=1)\n",
    "        modified_data = tf.cast(token_found >= 1, tf.int32)\n",
    "\n",
    "        return modified_data\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        attn = None\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        seg_data = self.get_seg_data(x)\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x += self.seg_encoding(seg_data)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, attn = self.enc_layers[i](x, mask)\n",
    "\n",
    "        return x, attn  # (batch_size, input_seq_len, d_model)\n"
   ],
   "metadata": {
    "id": "Hh-S4n1tTMro",
    "ExecuteTime": {
     "end_time": "2024-01-17T03:46:41.091780Z",
     "start_time": "2024-01-17T03:46:41.068656Z"
    }
   },
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class BERT(tf.keras.Model):\n",
    "    def __init__(self, **kargs):\n",
    "        super(BERT, self).__init__(name=kargs['model_name'])\n",
    "        self.end_token_idx = kargs['end_token_idx']\n",
    "        self.encoder = Encoder(**kargs)\n",
    "        self.outputs_layer = tf.keras.layers.Dense(kargs['d_model'],\n",
    "                                                   activation='tanh')\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(2)\n",
    "\n",
    "    def create_padding_mask(self, seq):\n",
    "        seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "        # add extra dimensions to add the padding\n",
    "        # to the attention logits.\n",
    "        return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "    def call(self, x):\n",
    "        inp = x\n",
    "        mask = self.create_padding_mask(inp)\n",
    "\n",
    "        enc_output, attn = self.encoder(inp, mask)  # (batch_size, inp_seq_len, d_model)\n",
    "        enc_output = self.outputs_layer(enc_output)  # (batch_size, inp_seq_len, d_model)\n",
    "        enc_output = tf.keras.layers.Flatten()(enc_output)  # (batch_size, inp_seq_len * d_model)\n",
    "        final_output = self.final_layer(enc_output)  # (batch_size, 1)\n",
    "\n",
    "        return final_output"
   ],
   "metadata": {
    "id": "5ECeQU2bV_nc",
    "ExecuteTime": {
     "end_time": "2024-01-17T03:46:41.251449Z",
     "start_time": "2024-01-17T03:46:41.230406Z"
    }
   },
   "execution_count": 57,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = BERT(**kargs)"
   ],
   "metadata": {
    "id": "QX6TMRTnWfjt",
    "ExecuteTime": {
     "end_time": "2024-01-17T03:46:41.555Z",
     "start_time": "2024-01-17T03:46:41.430891Z"
    }
   },
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cruLtRx2jC_C"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
    "\n",
    "def loss(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "def accuracy(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
    "    pred *= mask\n",
    "    acc = train_accuracy(real, pred)\n",
    "\n",
    "    return tf.reduce_mean(acc)"
   ],
   "metadata": {
    "id": "qswjggp_i0r1",
    "ExecuteTime": {
     "end_time": "2024-01-17T03:46:41.863940Z",
     "start_time": "2024-01-17T03:46:41.829308Z"
    }
   },
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss=loss,\n",
    "              metrics=[accuracy])"
   ],
   "metadata": {
    "id": "IHJgMz0AitbW",
    "ExecuteTime": {
     "end_time": "2024-01-17T03:46:42.059956Z",
     "start_time": "2024-01-17T03:46:42.023390Z"
    }
   },
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                     monitor='val_loss',\n",
    "                                                     restore_best_weights=True,\n",
    "                                                     verbose=1)"
   ],
   "metadata": {
    "id": "fE8AQSGyW0b5",
    "ExecuteTime": {
     "end_time": "2024-01-17T03:46:42.179139Z",
     "start_time": "2024-01-17T03:46:42.168432Z"
    }
   },
   "execution_count": 61,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Max_epochs = 5\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=train_data_pad.shape[0] // batch_size,\n",
    "    epochs=Max_epochs,\n",
    "\n",
    "    callbacks=[early_stopping_cb]\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UgBkD-JN20a",
    "outputId": "ee333ef6-4d8c-446c-f9b0-05e914e67d4d",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-17T03:46:42.677423Z"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-17 12:46:44.978908: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4687/4687 [==============================] - ETA: 0s - loss: 0.2308 - accuracy: 0.7659WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "4687/4687 [==============================] - 1540s 327ms/step - loss: 0.2308 - accuracy: 0.7659\n",
      "Epoch 2/5\n",
      "4687/4687 [==============================] - ETA: 0s - loss: 0.1743 - accuracy: 0.7587WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "4687/4687 [==============================] - 1831s 391ms/step - loss: 0.1743 - accuracy: 0.7587\n",
      "Epoch 3/5\n",
      "4687/4687 [==============================] - ETA: 0s - loss: 0.2142 - accuracy: 0.7574WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "4687/4687 [==============================] - 2160s 461ms/step - loss: 0.2142 - accuracy: 0.7574\n",
      "Epoch 4/5\n",
      "3521/4687 [=====================>........] - ETA: 6:50 - loss: 0.3607 - accuracy: 0.7598"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the model"
   ],
   "metadata": {
    "id": "YWB5yVQonm1m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "results = model.evaluate(test_dataset)\n",
    "# loss\n",
    "print(\"loss value: {:.3f}\".format(results[0]))\n",
    "# accuracy\n",
    "print(\"accuracy value: {:.3f}\".format(results[1]))"
   ],
   "metadata": {
    "id": "-c8qXnK1nobL",
    "ExecuteTime": {
     "start_time": "2024-01-17T03:45:43.236260Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "-40F3YgugDwS",
    "ExecuteTime": {
     "end_time": "2024-01-17T03:45:43.286658Z",
     "start_time": "2024-01-17T03:45:43.241164Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}

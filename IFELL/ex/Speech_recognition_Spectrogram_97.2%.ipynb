{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yu1-_CuosHDh"
   },
   "source": [
    "# Speech spectrogram classification\n",
    "\n",
    "## Spectrogram 데이터를 이용해 음성을 분류하는 모델을 제작해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TKZ-SAg2AIJ"
   },
   "source": [
    "## challenge\n",
    "### Task\n",
    "* 1초 길이의 오디오 음성데이터를 이용해 단어를 분류하는 것이 목표입니다.\n",
    "* 주어진 데이터를 이용해 딥러닝 트레이닝 과정을 구현해 보는것이 목표입니다.\n",
    "* This code is borrowed from [Kaggle/TensorFlow Speech Recognition Challenge](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge).\n",
    "* This is version 0.01 of the data set containing 64,727 audio files, released on August 3rd 2017.\n",
    "* **챌린지에서 사용하는 데이터는 Wave에서 Spectrogram으로 변환된 데이터입니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvN8sTwi4S0t"
   },
   "source": [
    "### Import packages\n",
    "\n",
    "* 우리가 사용할 packages 를 import 하는 부분 입니다.\n",
    "* 필요에 따른 packages를 선언합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "RlMAFGSosHDk",
    "outputId": "92261617-70c6-4909-93d6-341daa05d1e8",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:13:51.774063200Z",
     "start_time": "2024-01-04T14:13:47.866730500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'2.10.0'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import BatchNormalization, ReLU, LeakyReLU, Conv2D,GlobalAveragePooling2D, Dropout, Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import librosa\n",
    "# import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from os.path import isdir, join\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XgiqM0DsHDo"
   },
   "source": [
    "### Setting Dataset\n",
    "\n",
    "* Colab 적용을 위한 변수 지정 및 드라이브 마운트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MU1jUvnosMRH",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:13:51.834246600Z",
     "start_time": "2024-01-04T14:13:51.762061600Z"
    }
   },
   "outputs": [],
   "source": [
    "use_colab = True\n",
    "assert use_colab in [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aSd1jUmOym_t",
    "outputId": "d3750bc3-5484-4f4e-beb7-1373d07d7b9d",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:13:51.860696300Z",
     "start_time": "2024-01-04T14:13:51.777063300Z"
    }
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CXI0z5vEsHD9",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:13:51.917468Z",
     "start_time": "2024-01-04T14:13:51.794063600Z"
    }
   },
   "outputs": [],
   "source": [
    "if use_colab:\n",
    "    DATASET_PATH = \"./drive/\"\n",
    "else:\n",
    "    DATASET_PATH = \"./drive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeGRTrMHZaRL"
   },
   "source": [
    "### Dataset Shape\n",
    "* 불러온 데이터셋의 shape을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLIdaMhCyUFZ",
    "outputId": "742a9da4-8e3c-42c8-e4cf-50b5d126ff68",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:13:59.915906Z",
     "start_time": "2024-01-04T14:13:51.808247900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50620, 130, 126)\n"
     ]
    }
   ],
   "source": [
    "data_wav = np.load(\"./drive/speech_spec_8000.npy\")\n",
    "print(data_wav.shape)\n",
    "# 50620, 130, 126, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGk9vyKxNQlD",
    "outputId": "558edb5b-8087-48b3-bd45-50239a086950",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:13:59.917914Z",
     "start_time": "2024-01-04T14:13:52.656502100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(-1, 130, 126, 1)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-1, data_wav.shape[1], data_wav.shape[2], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auWK1iE3-eYh"
   },
   "source": [
    "* Spectrogram으로 변환한 데이터를 plot 해본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "LnIV5RMEsHEG",
    "outputId": "ce2dee54-b9fd-40b2-9780-f3066b964180",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:13:59.999167800Z",
     "start_time": "2024-01-04T14:13:52.670085800Z"
    }
   },
   "outputs": [],
   "source": [
    "# librosa.display.specshow(librosa.amplitude_to_db(data_wav[219], ref=np.max), x_axis='time')\n",
    "# plt.title('Power spectrogram')\n",
    "# plt.colorbar(format='%+2.0f dB')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7_lX4sP-l0r"
   },
   "source": [
    "* 전체 데이터셋의 wave 데이터를 spectrogram으로 변환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0w98NJJehoq"
   },
   "source": [
    "### Target_list 설정\n",
    "* 데이터셋은 기본적으로 총 12개의 클래스로 나누어져있다.\n",
    "```\n",
    "['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence']\n",
    "```\n",
    "* 해당 클래스로 나누어진 label을 학습 가능한 형태로 처리 후 데이터셋 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eu1VDu_p9Nq9",
    "outputId": "06b055df-b2f3-4ba4-8027-16d5f9c86254",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.000166900Z",
     "start_time": "2024-01-04T14:13:52.687085300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50620, 1)\n"
     ]
    }
   ],
   "source": [
    "data_label = np.load(\"./drive/speech_label_8000.npy\")\n",
    "print(data_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FUgL7TSsHEU",
    "outputId": "a5ef23c7-83a7-4727-bd7b-6c934f3df723",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.003168100Z",
     "start_time": "2024-01-04T14:13:52.701085700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45558, 130, 126, 1)\n",
      "(5062, 130, 126, 1)\n",
      "(45558, 1)\n",
      "(5062, 1)\n"
     ]
    }
   ],
   "source": [
    "train_wav, test_wav, train_label, test_label = train_test_split(data_wav, data_label, test_size = 0.1, shuffle =True)\n",
    "\n",
    "# train_wav, test_wav, train_label, test_label = train_test_split(speech_data[\"wav_vals\"], # wav 파일들의 데이터\n",
    "#                                                                 speech_data[\"label_vals\"], # label 파일들의 데이터\n",
    "#                                                                 test_size=0.1, # 비율 train, test를 몇퍼센트의 비율로 나눌지\n",
    "#                                                                 shuffle=True) # 섞을 것인지?\n",
    "#                                                                 #(파일, 정답) 이 형태로 섞어주게됩니다.\n",
    "\n",
    "\n",
    "# reshape for conv layers Conv2D -> 차원이 더 늘어납니다. 데이터 shape도 바뀝니다!\n",
    "train_wav = train_wav.reshape([-1,130,126,1])\n",
    "test_wav = test_wav.reshape([-1,130,126,1])\n",
    "\n",
    "print(train_wav.shape)\n",
    "print(test_wav.shape)\n",
    "print(train_label.shape)\n",
    "print(test_label.shape)\n",
    "\n",
    "del data_wav # 메모리 관리를 위해 변수 삭제\n",
    "del data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# label 전처리\n",
    "target_list = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence']\n",
    "##################################################\n",
    "##### 주어진 label => idx 형태로 변경해주셔야합니다. #####\n",
    "##################################################\n",
    "\n",
    "\n",
    "new_label_value = dict() # 사전에 입력\n",
    "for i, l in enumerate(target_list):\n",
    "    new_label_value[l] = i\n",
    "label_value = new_label_value # 일종의 번역사전을 만들게 됩니다."
   ],
   "metadata": {
    "id": "Q6RDioDCsHEX",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.019169300Z",
     "start_time": "2024-01-04T14:13:53.288955500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "{'yes': 0,\n 'no': 1,\n 'up': 2,\n 'down': 3,\n 'left': 4,\n 'right': 5,\n 'on': 6,\n 'off': 7,\n 'stop': 8,\n 'go': 9,\n 'unknown': 10,\n 'silence': 11}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_value"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7r-8auJBde2",
    "outputId": "b50f3cb3-28dd-4e16-e94b-42ed6bc78708",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.021168400Z",
     "start_time": "2024-01-04T14:13:53.303049400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([['off'],\n       ['up'],\n       ['go'],\n       ...,\n       ['down'],\n       ['yes'],\n       ['on']], dtype='<U7')"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_value['yes']\n",
    "test_label"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.022171500Z",
     "start_time": "2024-01-04T14:13:53.317647300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TJX3ilgsLTZj",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.023170400Z",
     "start_time": "2024-01-04T14:13:53.334646800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.int32"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for v in train_label:\n",
    "    #print(label_value[v[0]]) # v[0] => 리스트의 첫번째 값\n",
    "    temp.append(label_value[v[0]]) # [\"down\"] => \"down\"\n",
    "train_label = np.array(temp)\n",
    "\n",
    "temp = []\n",
    "for v in test_label:\n",
    "    temp.append(label_value[v[0]])\n",
    "test_label = np.array(temp)\n",
    "\n",
    "del temp\n",
    "type(train_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jENcWfGTLXE9",
    "outputId": "9bc3f293-5f5f-4246-bda2-ad3b92149602",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.040168800Z",
     "start_time": "2024-01-04T14:13:53.381163400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 6,  8,  1,  5,  6, 10,  7,  4,  4,  7])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bwwnfZisHES"
   },
   "source": [
    "### Model dataset setting\n",
    "* 변환된 데이터를 이용해서 학습에 활용할 데이터셋을 설정한다.\n",
    "    * data -> data_wav\n",
    "    * label -> data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "98yWshVhKcr3",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.042166100Z",
     "start_time": "2024-01-04T14:13:53.396161700Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_label(wav, label):\n",
    "    label = tf.one_hot(label, depth=12)\n",
    "    return wav, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZ9yGwb6sHEe",
    "outputId": "47be0fb9-0fda-498d-8a7f-e170aa1113f3",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.044166800Z",
     "start_time": "2024-01-04T14:13:53.415010100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Wav Demension : (45558, 130, 126, 1)\n",
      "Train_Label Demension : (45558,)\n",
      "Test_Wav Demension : (5062, 130, 126, 1)\n",
      "Test_Label Demension : (5062,)\n",
      "Number Of Labels : 12\n"
     ]
    }
   ],
   "source": [
    "print('Train_Wav Demension : ' + str(np.shape(train_wav)))\n",
    "print('Train_Label Demension : ' + str(np.shape(train_label)))\n",
    "print('Test_Wav Demension : ' + str(np.shape(test_wav)))\n",
    "print('Test_Label Demension : ' + str(np.shape(test_label)))\n",
    "print('Number Of Labels : ' + str(len(label_value)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSvck1awy3Md"
   },
   "source": [
    "### Hyper-parameters setting\n",
    "* 학습 전반에서 사용할 batch size, epoch, checkpoint dir을 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mt-aLitgy5xI",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.058068Z",
     "start_time": "2024-01-04T14:13:53.429602300Z"
    }
   },
   "outputs": [],
   "source": [
    "# the save point\n",
    "if use_colab:\n",
    "    checkpoint_dir ='./drive/MyDrive/train_ckpt/spectrogram/exp1'\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "else:\n",
    "    checkpoint_dir = 'spectrogram/exp1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JnCHOjS7H2q"
   },
   "source": [
    "### Dataset 구성\n",
    "* 전처리가 완료된 데이터들을 이용해서 Train, Test Dataset을 직접 구성해봅시다.\n",
    "* 학습에 사용할 Loss Function의 설정을 고려해 제작\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65xPCvazsHEm",
    "outputId": "90a68780-54a5-41d9-ba7e-7829b5f8e442",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.060064Z",
     "start_time": "2024-01-04T14:13:54.382007600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 130, 126, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 12), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# 전체 데이터셋 구성\n",
    "batch_size = 32\n",
    "\n",
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_wav, train_label))\n",
    "train_dataset = train_dataset.map(one_hot_label)\n",
    "train_dataset = train_dataset.shuffle(len(train_wav)).repeat().batch(batch_size=batch_size)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DKe_-yVCDCSP",
    "outputId": "86125efa-75f0-49c7-d8cc-92a3821fd407",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.062064700Z",
     "start_time": "2024-01-04T14:13:54.864834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 130, 126, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None, 12), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# for test\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_wav, test_label))\n",
    "test_dataset = test_dataset.map(one_hot_label)\n",
    "test_dataset = test_dataset.batch(batch_size=batch_size) # 테스트때는 섞을 필요가 없다!\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZmjKwlBSVnx"
   },
   "source": [
    "### Dataset 구성 검증\n",
    "```\n",
    "<BatchDataset shapes: ((None, 130, 126, 1), (None, 12)), types: (tf.float32, tf.float32)>\n",
    "<BatchDataset shapes: ((None, 130, 126, 1), (None, 12)), types: (tf.float32, tf.float32)>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykQEO8XP7FFD"
   },
   "source": [
    "## Model 구현\n",
    "* 제시된 모델을 구현해보고, 더 좋은 성능으로 튜닝해보자.\n",
    "\n",
    "    * inputs = [batch_size, 130, 126, 1]\n",
    "    * conv1 = [batch_size, 65, 63, 16]\n",
    "    * conv2 = [batch_size, 33, 32, 32]\n",
    "    * conv3 = [batch_size, 17, 16, 64]\n",
    "    * desne = [batch_size, 64]\n",
    "    * output = [batch_size, 12]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2MhnpbAHsHEo",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.063064800Z",
     "start_time": "2024-01-04T14:13:54.916962Z"
    }
   },
   "outputs": [],
   "source": [
    "# input_tensor = layers.Input(shape=(130,126, 1,))\n",
    "\n",
    "# x = # TODO\n",
    "# print(x.shape)\n",
    "\n",
    "# output_tensor =layers.Dense(12)\n",
    "\n",
    "# model = tf.keras.Model(# TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 810
    },
    "id": "nKP-a-yxFBfn",
    "outputId": "c932d099-8911-4151-f445-066b4c0036ef",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.064570300Z",
     "start_time": "2024-01-04T14:13:54.932966900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 클래스버젼 - Seperable Conv2D\n",
    "# class ResBlock(tf.keras.Model):\n",
    "#     def __init__(self, num_filter, stride=1, kernel_size=3, l2_reg=1e-4):\n",
    "#         super(ResBlock, self).__init__()\n",
    "#         self.num_filter = num_filter\n",
    "#         self.conv1 = layers.Conv2D(num_filter, kernel_size, strides=stride, padding='same',\n",
    "#                                    kernel_initializer=tf.keras.initializers.he_normal(),)\n",
    "#                                   #  kernel_regularizer=regularizers.l2(0.001))\n",
    "#         self.se_conv1 = layers.SeparableConv2D(num_filter, kernel_size, padding='same',\n",
    "#                                                depthwise_initializer=tf.keras.initializers.he_normal(),\n",
    "#                                                pointwise_initializer=tf.keras.initializers.he_normal(),)\n",
    "#                                               #  depthwise_regularizer=regularizers.l2(0.001),\n",
    "#                                               #  pointwise_regularizer=regularizers.l2(0.001))\n",
    "#         self.se_sonv2 = layers.SeparableConv2D(num_filter, kernel_size, padding='same',\n",
    "#                                                depthwise_initializer=tf.keras.initializers.he_normal(),\n",
    "#                                                pointwise_initializer=tf.keras.initializers.he_normal(),)\n",
    "#                                               #  depthwise_regularizer=regularizers.l2(0.001),\n",
    "#                                               #  pointwise_regularizer=regularizers.l2(0.001))\n",
    "#         self.con_short = Conv2D(num_filter, 1, strides=2, padding='same')\n",
    "#         self.bn1 = BatchNormalization()\n",
    "#         self.bn2 = BatchNormalization()\n",
    "#         self.bn3 = BatchNormalization()\n",
    "#         self.bn_short = BatchNormalization()\n",
    "# \n",
    "#     def call(self, input, stride=1):\n",
    "#         shortcut = input\n",
    "#         x = self.conv1(input)\n",
    "#         x = self.bn1(x)\n",
    "#         x = layers.Activation('relu')(x)\n",
    "#         x = self.se_conv1(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.se_sonv2(x)\n",
    "#         x = self.bn3(x)\n",
    "#         # 입력과 출력의 맞도록 숏컷을 조절\n",
    "#         if shortcut.shape[-1] != self.num_filter or stride != 1:\n",
    "#             shortcut = self.con_short(shortcut)\n",
    "#             shortcut = self.bn_short(shortcut)\n",
    "# \n",
    "#         x = layers.Add()([x, shortcut])\n",
    "#         x = layers.Activation('relu')(x)\n",
    "#         return x\n",
    "# \n",
    "# \n",
    "# class ResNet28(tf.keras.Model):\n",
    "#     def __init__(self):\n",
    "#         super(ResNet28, self).__init__()\n",
    "#         self.conv1 = layers.Conv2D(64, 7, strides=2,\n",
    "#                                    kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "#                                    padding='same')\n",
    "#         self.bn1 = BatchNormalization()\n",
    "#         self.bn2 = BatchNormalization()\n",
    "#         self.relu1 = ReLU()\n",
    "#         self.relu2 = ReLU()\n",
    "# \n",
    "#         self.blocks = [\n",
    "#                           ResBlock(64, stride=1) ] + [\n",
    "#                           ResBlock(128, stride=2),\n",
    "#                           ResBlock(256, stride=2)] + [\n",
    "#                           ResBlock(256, stride=1) ] + [\n",
    "#                           ResBlock(512, stride=2),\n",
    "#                           ResBlock(512, stride=1)]\n",
    "# \n",
    "#         #         self.blocks = [\n",
    "#         #                           ResBlock(64, stride=1) for _ in range(2)] + [\n",
    "#         #                           ResBlock(128, stride=2)] + [\n",
    "#         #                           ResBlock(128, stride=1) for _ in range(2)] + [\n",
    "#         #                           ResBlock(256, stride=2)] + [\n",
    "#         #                           ResBlock(256, stride=1) for _ in range(2)] + [\n",
    "#         #                           ResBlock(512, stride=2)] + [\n",
    "#         #                           ResBlock(512, stride=1)]\n",
    "# \n",
    "#         self.avg_pool = GlobalAveragePooling2D()\n",
    "# \n",
    "#         self.dropout = Dropout(0.5)\n",
    "#         self.out = Dense(12)\n",
    "# \n",
    "#     def call(self, inputs):\n",
    "#         x = self.conv1(inputs)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu1(x)\n",
    "# \n",
    "#         for block in self.blocks:\n",
    "#             x = block(x)\n",
    "# \n",
    "#         x = self.avg_pool(x)\n",
    "#         x = layers.Dense(1000)(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.out(x)\n",
    "# \n",
    "#         return x\n",
    "# \n",
    "# \n",
    "# # Specify input shape and number of classes\n",
    "# input_tensor = layers.Input(shape=(130, 126, 1,))  # Example input shape for image classification\n",
    "# \n",
    "# # Build ResNet model\n",
    "# Res = ResNet28()\n",
    "# output_tensor = Res.call(input_tensor)\n",
    "# model = tf.keras.Model(inputs=input_tensor, outputs=output_tensor)\n",
    "# \n",
    "# # Display model summary\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 130, 126, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 65, 63, 64)        3200      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 65, 63, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 65, 63, 64)        0         \n",
      "                                                                 \n",
      " res_block (ResBlock)        (None, 65, 63, 64)        59200     \n",
      "                                                                 \n",
      " res_block_1 (ResBlock)      (None, 33, 32, 128)       160000    \n",
      "                                                                 \n",
      " res_block_2 (ResBlock)      (None, 17, 16, 256)       614912    \n",
      "                                                                 \n",
      " res_block_3 (ResBlock)      (None, 17, 16, 256)       875776    \n",
      "                                                                 \n",
      " res_block_4 (ResBlock)      (None, 9, 8, 512)         2409472   \n",
      "                                                                 \n",
      " res_block_5 (ResBlock)      (None, 9, 8, 512)         3455488   \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1000)              513000    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 1000)             4000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 1000)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1000)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 12)                12012     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,107,316\n",
      "Trainable params: 8,086,116\n",
      "Non-trainable params: 21,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 클래스버젼 - Seperable Conv2D\n",
    "class ResBlock(tf.keras.Model):\n",
    "    def __init__(self, num_filter, stride=1, kernel_size=3, l2_reg=1e-4):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.num_filter = num_filter\n",
    "        self.conv1 = layers.Conv2D(num_filter, kernel_size, strides=stride, padding='same',\n",
    "                                   kernel_initializer=tf.keras.initializers.he_normal(),)\n",
    "        self.se_conv1 = layers.SeparableConv2D(num_filter, kernel_size, padding='same',\n",
    "                                               depthwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               pointwise_initializer=tf.keras.initializers.he_normal(),)\n",
    "        self.se_sonv2 = layers.SeparableConv2D(num_filter, kernel_size, padding='same',\n",
    "                                               depthwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               pointwise_initializer=tf.keras.initializers.he_normal(),)\n",
    "        self.se_conv3 = layers.SeparableConv2D(num_filter, 5, padding='same',\n",
    "                                               depthwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               pointwise_initializer=tf.keras.initializers.he_normal(),)\n",
    "        self.se_conv4 = layers.SeparableConv2D(num_filter, 5, padding='same',\n",
    "                                               depthwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               pointwise_initializer=tf.keras.initializers.he_normal(),)\n",
    "        self.con_short = Conv2D(num_filter, 1, strides=2, padding='same')\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.bn3 = BatchNormalization()\n",
    "        self.bn4 = BatchNormalization()\n",
    "        self.bn5 = BatchNormalization()\n",
    "        self.bn_short = BatchNormalization()\n",
    "\n",
    "    def call(self, input, stride=1):\n",
    "        shortcut = input \n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x2 = x\n",
    "        x = self.se_conv1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.se_sonv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x2 = self.se_conv3(x2)\n",
    "        x2 = self.bn4(x2)\n",
    "        x2 = self.se_conv4(x2)\n",
    "        x2 = self.bn5(x2)\n",
    "\n",
    "        if shortcut.shape[-1] != self.num_filter or stride != 1:\n",
    "            shortcut = self.con_short(shortcut)\n",
    "            shortcut = self.bn_short(shortcut)\n",
    "\n",
    "        x = layers.Add()([x,x2, shortcut])\n",
    "        x = layers.Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet28(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet28, self).__init__()\n",
    "        self.conv1 = layers.Conv2D(64, 7, strides=2,\n",
    "                                   kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                                   padding='same')\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.relu1 = ReLU()\n",
    "        self.relu2 = ReLU()\n",
    "\n",
    "        self.blocks = [\n",
    "                          ResBlock(64, stride=1) ] + [\n",
    "                          ResBlock(128, stride=2),\n",
    "                          ResBlock(256, stride=2)] + [\n",
    "                          ResBlock(256, stride=1)] + [\n",
    "                          ResBlock(512, stride=2),\n",
    "                          ResBlock(512, stride=1)]\n",
    "\n",
    "        self.avg_pool = GlobalAveragePooling2D()\n",
    "\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.out = Dense(12)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.avg_pool(x)\n",
    "        x = layers.Dense(1000)(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Specify input shape and number of classes\n",
    "input_tensor = layers.Input(shape=(130, 126, 1,))  # Example input shape for image classification\n",
    "\n",
    "# Build ResNet model\n",
    "Res = ResNet28()\n",
    "output_tensor = Res.call(input_tensor)\n",
    "model = tf.keras.Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.080576900Z",
     "start_time": "2024-01-04T14:13:54.993152900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeBqtI2tkAIo"
   },
   "source": [
    "* 구현된 모델을 어떻게 학습시킬 것인지 구성해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "90AizrGdsHEr",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.081580800Z",
     "start_time": "2024-01-04T14:13:55.634540100Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_learning_rate = 1e-4\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate),\n",
    "              # 0.0001~0.000001\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32hdTHj7SeCN"
   },
   "source": [
    "### 모델 Output 확인\n",
    "* 총 12개의 예측 데이터가 출력되는지 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Qg5pReaesHEt",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.082581300Z",
     "start_time": "2024-01-04T14:13:55.645901400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  [[ 0.01871599  0.05424325  0.0402809  -0.02300485  0.06109969 -0.03999165\n",
      "  -0.00589754  0.00385274 -0.03493483  0.01151454 -0.02498282  0.0178601 ]]\n"
     ]
    }
   ],
   "source": [
    "# without training, just inference a model:\n",
    "predictions = model(train_wav[0:1], training=False)\n",
    "print(\"Predictions: \", predictions.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDegoCoskGef"
   },
   "source": [
    "* 최종 모델 구성을 확인 후 모델을 저장할 체크포인트를 구성해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jfQfGlfNsHEy",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.101576600Z",
     "start_time": "2024-01-04T14:13:57.465461500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 130, 126, 1)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 65, 63, 64)        3200      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 65, 63, 64)       256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 65, 63, 64)        0         \n",
      "                                                                 \n",
      " res_block (ResBlock)        (None, 65, 63, 64)        59200     \n",
      "                                                                 \n",
      " res_block_1 (ResBlock)      (None, 33, 32, 128)       160000    \n",
      "                                                                 \n",
      " res_block_2 (ResBlock)      (None, 17, 16, 256)       614912    \n",
      "                                                                 \n",
      " res_block_3 (ResBlock)      (None, 17, 16, 256)       875776    \n",
      "                                                                 \n",
      " res_block_4 (ResBlock)      (None, 9, 8, 512)         2409472   \n",
      "                                                                 \n",
      " res_block_5 (ResBlock)      (None, 9, 8, 512)         3455488   \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1000)              513000    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 1000)             4000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 1000)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1000)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 12)                12012     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,107,316\n",
      "Trainable params: 8,086,116\n",
      "Non-trainable params: 21,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "3WEIV2-yzejQ",
    "ExecuteTime": {
     "end_time": "2024-01-04T14:14:00.102576500Z",
     "start_time": "2024-01-04T14:13:57.514056Z"
    }
   },
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 monitor='val_loss',\n",
    "                                                 mode='auto',\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20,\n",
    "                                                     monitor='val_loss',\n",
    "                                                     restore_best_weights=True,\n",
    "\n",
    "                                                     verbose=1)\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',  # 관찰할 지표\n",
    "                              factor=0.2,  # 학습률을 줄이는 비율\n",
    "                              patience=4,  # 몇 번의 에포크 동안 감소하지 않아야 하는지\n",
    "                              min_lr=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIr8LoVs6zdb"
   },
   "source": [
    "## Training\n",
    "* 위에서 구현한 데이터셋, 모델들을 fit 함수를 이용해 학습을 시켜봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "oo_KCHDbsHE2",
    "outputId": "40f5abb7-74c1-46b2-85a4-6b2928c0cae4",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-04T14:13:57.530056700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1423/1423 [==============================] - ETA: 0s - loss: 1.1212 - accuracy: 0.6295\n",
      "Epoch 1: val_loss improved from inf to 0.79866, saving model to ./drive/MyDrive/train_ckpt/spectrogram\\exp1\n",
      "1423/1423 [==============================] - 136s 90ms/step - loss: 1.1212 - accuracy: 0.6295 - val_loss: 0.7987 - val_accuracy: 0.7589 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "1423/1423 [==============================] - ETA: 0s - loss: 0.3107 - accuracy: 0.8991\n",
      "Epoch 2: val_loss improved from 0.79866 to 0.27260, saving model to ./drive/MyDrive/train_ckpt/spectrogram\\exp1\n",
      "1423/1423 [==============================] - 129s 90ms/step - loss: 0.3107 - accuracy: 0.8991 - val_loss: 0.2726 - val_accuracy: 0.9094 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "1423/1423 [==============================] - ETA: 0s - loss: 0.1992 - accuracy: 0.9355\n",
      "Epoch 3: val_loss did not improve from 0.27260\n",
      "1423/1423 [==============================] - 129s 90ms/step - loss: 0.1992 - accuracy: 0.9355 - val_loss: 0.2762 - val_accuracy: 0.9082 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "1423/1423 [==============================] - ETA: 0s - loss: 0.1548 - accuracy: 0.9496\n",
      "Epoch 4: val_loss improved from 0.27260 to 0.24942, saving model to ./drive/MyDrive/train_ckpt/spectrogram\\exp1\n",
      "1423/1423 [==============================] - 129s 91ms/step - loss: 0.1548 - accuracy: 0.9496 - val_loss: 0.2494 - val_accuracy: 0.9171 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "1423/1423 [==============================] - ETA: 0s - loss: 0.1234 - accuracy: 0.9594\n",
      "Epoch 5: val_loss improved from 0.24942 to 0.24672, saving model to ./drive/MyDrive/train_ckpt/spectrogram\\exp1\n",
      "1423/1423 [==============================] - 129s 91ms/step - loss: 0.1234 - accuracy: 0.9594 - val_loss: 0.2467 - val_accuracy: 0.9197 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "1423/1423 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.9638\n",
      "Epoch 6: val_loss improved from 0.24672 to 0.13940, saving model to ./drive/MyDrive/train_ckpt/spectrogram\\exp1\n",
      "1423/1423 [==============================] - 130s 91ms/step - loss: 0.1074 - accuracy: 0.9638 - val_loss: 0.1394 - val_accuracy: 0.9539 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "1423/1423 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9688\n",
      "Epoch 7: val_loss did not improve from 0.13940\n",
      "1423/1423 [==============================] - 129s 91ms/step - loss: 0.0931 - accuracy: 0.9688 - val_loss: 0.1642 - val_accuracy: 0.9434 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "1423/1423 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9709\n",
      "Epoch 8: val_loss did not improve from 0.13940\n",
      "1423/1423 [==============================] - 128s 90ms/step - loss: 0.0855 - accuracy: 0.9709 - val_loss: 0.2291 - val_accuracy: 0.9294 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "1423/1423 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9736\n",
      "Epoch 9: val_loss did not improve from 0.13940\n",
      "1423/1423 [==============================] - 126s 89ms/step - loss: 0.0770 - accuracy: 0.9736 - val_loss: 0.1455 - val_accuracy: 0.9523 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      " 454/1423 [========>.....................] - ETA: 1:23 - loss: 0.0615 - accuracy: 0.9778"
     ]
    }
   ],
   "source": [
    "# model.fit model.fit_generator는 model.fit으로 통일되었습니다.\n",
    "# tf.data.Dataset은 generator 입니다.\n",
    "history = model.fit(train_dataset,\n",
    "                    steps_per_epoch= len(train_wav)// batch_size,\n",
    "                    epochs=50,\n",
    "                    callbacks=[cp_callback,early_stopping_cb,reduce_lr],\n",
    "                    validation_data= test_dataset,\n",
    "                    validation_steps=len(test_wav) // batch_size,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMk4bWh1kquU"
   },
   "source": [
    "* 학습 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Ep7mink9sHE4",
    "outputId": "882765c8-6757-437d-a830-5c2a8323d993",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRjMdOed6B9U"
   },
   "source": [
    "## Evaluation\n",
    "* Test dataset을 이용해서 모델의 성능을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jtNWcBnq0LXs",
    "outputId": "746639d4-47e8-4ef9-915f-bc52586efb83",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "wjaqdVwjsHE7",
    "outputId": "0f1d5eea-6706-40ce-8c07-8a517525f4a6",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3oIE73DSxFR"
   },
   "source": [
    "### 스코어 결과\n",
    "* 위의 스코어는 분류모델에 적용되는 스코어입니다.\n",
    "* 모델의 크기 (MB) 와 정확도를 이용해 스코어를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "srOVztfNsHE_",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def final_score():\n",
    "    print(\"Model params num : \" + str(model.count_params()))\n",
    "    print(\"Accuracy : \" + str(results[1]))\n",
    "\n",
    "    s = (model.count_params() * 32) / (1024 ** 2)\n",
    "    score = 50 * (results[1] + min((1/s), 1))\n",
    "\n",
    "    print(\"score : \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "akh6IYvsSyg3",
    "outputId": "0c91252c-c9d4-48c6-df11-40839c8e14c9",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "final_score()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "wZmjKwlBSVnx"
   ],
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

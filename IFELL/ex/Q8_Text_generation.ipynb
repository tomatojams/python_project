{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcD2nPQvPOFM"
   },
   "source": [
    "# Text Generation with RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpaIgZGRrSaw"
   },
   "source": [
    "* RNN을 사용하여 어떻게 텍스트를 생성하는지 알아보자.\n",
    "* 주어진 텍스트를 기반으로 텍스트를 생성하는 모델을 구현해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwpJ5IffzRG6"
   },
   "source": [
    "### 생성의 한계\n",
    "\n",
    "문장 중 일부는 문법적으로 맞지만 대부분 자연스럽지 않다.\n",
    "\n",
    "이 모델은 단어의 의미를 학습하지는 않았지만, 고려해야 할 점으로:\n",
    "\n",
    "* 데이터는 문자 기반이다. 훈련이 시작되었을 때, 이 모델은 영어 단어의 철자를 모르고 심지어 텍스트의 단위가 단어라는 것도 모른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "## 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46TMSZVszoc9"
   },
   "source": [
    "### Drive 연결\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YiWZ-S31zokw",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:30.941980Z",
     "start_time": "2024-01-05T01:33:30.937343Z"
    }
   },
   "source": [
    "use_colab = True\n",
    "assert use_colab in [True, False]"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LjWuSiewzw-R",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d28198e0-0866-4c3f-c35c-ba04769954f7",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:30.946551Z",
     "start_time": "2024-01-05T01:33:30.940524Z"
    }
   },
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yG_n40gFzf9s",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:33.763628Z",
     "start_time": "2024-01-05T01:33:30.944294Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "### 셰익스피어 데이터셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pD_55cOxLkAb",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:33.767712Z",
     "start_time": "2024-01-05T01:33:33.764242Z"
    }
   },
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### 데이터 읽기"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aavnuByVymwK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "199bb592-92f6-4c9f-ac6b-5418dcf0b851",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:33.772699Z",
     "start_time": "2024-01-05T01:33:33.767901Z"
    }
   },
   "source": [
    "# 데이터를 불러와서 디코딩\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# 문자의 수\n",
    "print ('텍스트의 길이: {}'.format(len(text)))"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 길이: 1115394\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Duhg9NrUymwO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "575b91da-8e1b-4a0a-ad9f-9b0af179a323",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:33.781157Z",
     "start_time": "2024-01-05T01:33:33.772293Z"
    }
   },
   "source": [
    "# 텍스트 처음 250자 출력\n",
    "print(text[:250])"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IlCgQBRVymwR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fc7ca8ac-218e-4f46-b291-5d47a163afaf",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:33.830105Z",
     "start_time": "2024-01-05T01:33:33.787331Z"
    }
   },
   "source": [
    "# 파일의 고유 문자수를 출력\n",
    "vocab = sorted(set(text)) # 내가 불러온 text 데이터를 집합으로 만들어서 정렬시킨 상태입니다.\n",
    "print ('고유 문자수 {}개'.format(len(vocab)))"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "고유 문자수 65개\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:33.831770Z",
     "start_time": "2024-01-05T01:33:33.791034Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## 텍스트 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### 텍스트 벡터화\n",
    "\n",
    "* 학습을 위해서 텍스트들을 수치화할 필요가 있다.\n",
    "* 텍스트를 인덱스화 시켜 학습에 사용"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IalZLbvOzf-F",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:33.905617Z",
     "start_time": "2024-01-05T01:33:33.876402Z"
    }
   },
   "source": [
    "# 고유 문자에서 인덱스로 매핑 생성\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab) # idx <=> char 변환할 수 있는 사전을 들고있는 것이 중요합니다.\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text]) # 람다식 텍스트를 인덱스로 매칭"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "char2idx"
   ],
   "metadata": {
    "id": "tTiBTvW1b3Ma",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0fe650b1-6d09-483f-db07-97926c34127d",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:33.907564Z",
     "start_time": "2024-01-05T01:33:33.881688Z"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "{'\\n': 0,\n ' ': 1,\n '!': 2,\n '$': 3,\n '&': 4,\n \"'\": 5,\n ',': 6,\n '-': 7,\n '.': 8,\n '3': 9,\n ':': 10,\n ';': 11,\n '?': 12,\n 'A': 13,\n 'B': 14,\n 'C': 15,\n 'D': 16,\n 'E': 17,\n 'F': 18,\n 'G': 19,\n 'H': 20,\n 'I': 21,\n 'J': 22,\n 'K': 23,\n 'L': 24,\n 'M': 25,\n 'N': 26,\n 'O': 27,\n 'P': 28,\n 'Q': 29,\n 'R': 30,\n 'S': 31,\n 'T': 32,\n 'U': 33,\n 'V': 34,\n 'W': 35,\n 'X': 36,\n 'Y': 37,\n 'Z': 38,\n 'a': 39,\n 'b': 40,\n 'c': 41,\n 'd': 42,\n 'e': 43,\n 'f': 44,\n 'g': 45,\n 'h': 46,\n 'i': 47,\n 'j': 48,\n 'k': 49,\n 'l': 50,\n 'm': 51,\n 'n': 52,\n 'o': 53,\n 'p': 54,\n 'q': 55,\n 'r': 56,\n 's': 57,\n 't': 58,\n 'u': 59,\n 'v': 60,\n 'w': 61,\n 'x': 62,\n 'y': 63,\n 'z': 64}"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OPQ2A1mwOQUd",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d12d3605-f578-49f0-9f27-288859976064",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:33.917703Z",
     "start_time": "2024-01-05T01:33:33.887760Z"
    }
   },
   "source": [
    "idx2char"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?',\n       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n       'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n       'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'],\n      dtype='<U1')"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YdVhhjYa31Uk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b113769d-1956-4619-ea4f-60e63973b894",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:33.947154Z",
     "start_time": "2024-01-05T01:33:33.895849Z"
    }
   },
   "source": [
    "print(text_as_int)\n",
    "text_as_int.shape"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 47 56 ... 45  8  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "(1115394,)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZfqhkYCymwX"
   },
   "source": [
    "* 텍스트 0번부터 전체 텍스트 길이까지 인덱스화"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FYyNlCNXymwY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e7586b8f-4895-4137-8ec3-9ef669741402",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:33.950847Z",
     "start_time": "2024-01-05T01:33:33.903137Z"
    }
   },
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(65)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '$' :   3,\n",
      "  '&' :   4,\n",
      "  \"'\" :   5,\n",
      "  ',' :   6,\n",
      "  '-' :   7,\n",
      "  '.' :   8,\n",
      "  '3' :   9,\n",
      "  ':' :  10,\n",
      "  ';' :  11,\n",
      "  '?' :  12,\n",
      "  'A' :  13,\n",
      "  'B' :  14,\n",
      "  'C' :  15,\n",
      "  'D' :  16,\n",
      "  'E' :  17,\n",
      "  'F' :  18,\n",
      "  'G' :  19,\n",
      "  'H' :  20,\n",
      "  'I' :  21,\n",
      "  'J' :  22,\n",
      "  'K' :  23,\n",
      "  'L' :  24,\n",
      "  'M' :  25,\n",
      "  'N' :  26,\n",
      "  'O' :  27,\n",
      "  'P' :  28,\n",
      "  'Q' :  29,\n",
      "  'R' :  30,\n",
      "  'S' :  31,\n",
      "  'T' :  32,\n",
      "  'U' :  33,\n",
      "  'V' :  34,\n",
      "  'W' :  35,\n",
      "  'X' :  36,\n",
      "  'Y' :  37,\n",
      "  'Z' :  38,\n",
      "  'a' :  39,\n",
      "  'b' :  40,\n",
      "  'c' :  41,\n",
      "  'd' :  42,\n",
      "  'e' :  43,\n",
      "  'f' :  44,\n",
      "  'g' :  45,\n",
      "  'h' :  46,\n",
      "  'i' :  47,\n",
      "  'j' :  48,\n",
      "  'k' :  49,\n",
      "  'l' :  50,\n",
      "  'm' :  51,\n",
      "  'n' :  52,\n",
      "  'o' :  53,\n",
      "  'p' :  54,\n",
      "  'q' :  55,\n",
      "  'r' :  56,\n",
      "  's' :  57,\n",
      "  't' :  58,\n",
      "  'u' :  59,\n",
      "  'v' :  60,\n",
      "  'w' :  61,\n",
      "  'x' :  62,\n",
      "  'y' :  63,\n",
      "  'z' :  64,\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l1VKcQHcymwb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f64dfdaf-6911-450a-b326-65fe9f0c9ed3",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:33.966311Z",
     "start_time": "2024-01-05T01:33:33.906636Z"
    }
   },
   "source": [
    "# 텍스트 맵핑\n",
    "print ('{} ---- Index ---- > {}'.format(repr(text[:13]), text_as_int[:13])) #repr은 따옴표가 붙은 문자열을 생성"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen' ---- Index ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbmsf23Bymwe"
   },
   "source": [
    "### 예측 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wssHQ1oGymwe"
   },
   "source": [
    "주어진 문자나 문자 시퀀스가 주어졌을 때, 다음 문자로 가장 가능성 있는 문자는 무엇일까?\n",
    "\n",
    "* 이는 모델을 훈련하여 수행할 작업이다.\n",
    "* 모델의 입력은 문자열 시퀀스가 될 것이고, 모델을 훈련시켜 출력을 예측한다.\n",
    "* 이 출력은 현재 타임 스텝(time step)의 다음 문자이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "### 훈련 샘플과 타깃 만들기\n",
    "\n",
    "* 다음으로 텍스트를 샘플 시퀀스로 나누자.\n",
    "\n",
    "* 각 입력 시퀀스에는 텍스트에서 나온 `seq_length`개의 문자가 포함된다.\n",
    "\n",
    "* 각 입력 시퀀스에서, 해당 타깃은 한 문자를 오른쪽으로 이동한 것을 제외하고는 동일한 길이의 텍스트를 포함한다.\n",
    "\n",
    "* 텍스트를`seq_length + 1`개의 청크(chunk)로 나누자\n",
    "    * 예를 들어, `seq_length`는 4이고 텍스트를 \"Hello\"이라고 가정해 봅시다. 입력 시퀀스는 \"Hell\"이고 타깃 시퀀스는 \"ello\"가 된다.\n",
    "\n",
    "* 이렇게 하기 위해 먼저 `tf.data.Dataset.from_tensor_slices` 함수를 사용해 텍스트 벡터를 문자 인덱스의 스트림으로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HjXwjMSQbNnD",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:33.967311Z",
     "start_time": "2024-01-05T01:33:33.909649Z"
    }
   },
   "source": [
    "# Hello 라는 단어를 만들고 싶습니다.\n",
    "# 학습 데이터를 아래와 같이 세팅을 해줘야합니다.\n",
    "# input : H -> e -> l -> l\n",
    "# output : e -> l -> l -> o"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0UHJDA39zf-O",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f5ece31a-3b51-4c91-9ca9-349ec56436b0",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:34.034887Z",
     "start_time": "2024-01-05T01:33:33.952634Z"
    }
   },
   "source": [
    "# 단일 입력에 대해 원하는 문장의 최대 길이\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# 훈련 샘플/타깃 만들기\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) # 알파벳을 하나씩 생성합니다.\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(i.numpy())\n",
    "    print(idx2char[i.numpy()]) #확인가능"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "18\n",
      "F\n",
      "47\n",
      "i\n",
      "56\n",
      "r\n",
      "57\n",
      "s\n",
      "58\n",
      "t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 10:33:33.917450: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-01-05 10:33:33.917917: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZSYAcQV8OGP"
   },
   "source": [
    "* `batch`를 이용해 몇개의 텍스트를 가져올 것인지 정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "l4hkDU3i7ozi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "df2be4f5-13d8-4978-a22a-2f36d858b2a4",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:34.036911Z",
     "start_time": "2024-01-05T01:33:34.001356Z"
    }
   },
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True) # 문장을 가져와합니다. drop_reainder -> 배치가 짧거나 갯수가 부족하면 버리는 옵션\n",
    "                               # 문장을 가져올 수 있도록 batch 를 구성해줍니다.\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()]))) # repr 개행문자 출력"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbLcIPBj_mWZ"
   },
   "source": [
    "각 시퀀스에서, `map` 메서드를 사용해 각 배치에 간단한 함수를 적용하고 입력 텍스트와 타깃 텍스트를 복사 및 이동\n",
    "* 모델이 Text를 생성할때, 한 텍스트 단위로 생성하기 때문에 그 다음 텍스트를 target으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9NGu-FkO_kYU",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:34.116097Z",
     "start_time": "2024-01-05T01:33:34.015177Z"
    }
   },
   "source": [
    "# Hello -> 불러온 데이터의 길이\n",
    "# input : Hell # input의 길이\n",
    "# output : ello # output의 길이\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiCopyGZymwi"
   },
   "source": [
    "첫 번째 샘플의 타깃 값을 출력해보자"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GNbw-iR0ymwj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ba9b4654-7654-42cc-fadb-4e7f6c42635e",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:34.118037Z",
     "start_time": "2024-01-05T01:33:34.040663Z"
    }
   },
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('입력 데이터: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('타깃 데이터: ', repr(''.join(idx2char[target_example.numpy()])))"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 데이터:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "타깃 데이터:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 10:33:34.049966: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_33OHL3b84i0"
   },
   "source": [
    "* 이 벡터의 각 인덱스는 하나의 타임 스텝(time step)으로 처리됩니다. 타임 스텝 0의 입력으로 모델은 \"F\"의 인덱스를 받고 다음 문자로 \"i\"의 인덱스를 예측한다.\n",
    "\n",
    "* 다음 타임 스텝에서도 같은 일을 하지만 RNN은 현재 입력 문자 외에 이전 타임 스텝의 컨텍스트**(context)**를 고려한다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0eBu9WZG84i0",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ce760df0-1a44-43ed-f1a2-6ebeb0616b7f",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:34.119480Z",
     "start_time": "2024-01-05T01:33:34.070330Z"
    }
   },
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"iter {:4d}\".format(i))\n",
    "    print(\"  inputs: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  generated text: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter    0\n",
      "  inputs: 18 ('F')\n",
      "  generated text: 47 ('i')\n",
      "iter    1\n",
      "  inputs: 47 ('i')\n",
      "  generated text: 56 ('r')\n",
      "iter    2\n",
      "  inputs: 56 ('r')\n",
      "  generated text: 57 ('s')\n",
      "iter    3\n",
      "  inputs: 57 ('s')\n",
      "  generated text: 58 ('t')\n",
      "iter    4\n",
      "  inputs: 58 ('t')\n",
      "  generated text: 1 (' ')\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### 훈련 배치 생성\n",
    "\n",
    "* 텍스트를 다루기 쉬운 시퀀스로 분리하기 위해 `tf.data`를 사용\n",
    "* 이 데이터를 모델에 넣기 전에 데이터를 섞은 후 배치를 만들어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p2pGotuNzf-S",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "93d449a0-ba3c-4890-e552-a0d44b212a60",
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:34.127551Z",
     "start_time": "2024-01-05T01:33:34.074947Z"
    }
   },
   "source": [
    "# 배치 크기\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "dataset = dataset.shuffle(10000).batch(BATCH_SIZE, drop_remainder=True)\n",
    "# 알파벳을 기준으로 했던 데이터에서 문장단위로 데이터를 불러오는 dataset을 구성할 수 있습니다."
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[43 56 57 ...  1 53 44]\n",
      " [10  0 25 ... 43 56 57]\n",
      " [43  1 61 ... 31  1 27]\n",
      " ...\n",
      " [50 39 51 ... 51 53 56]\n",
      " [58 63  0 ... 53  1 41]\n",
      " [ 1 42 47 ... 57  1 51]], shape=(32, 100), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[56 57  5 ... 53 44 58]\n",
      " [ 0 25 63 ... 56 57  1]\n",
      " [ 1 61 43 ...  1 27 18]\n",
      " ...\n",
      " [39 51 43 ... 53 56 52]\n",
      " [63  0 47 ...  1 41 53]\n",
      " [42 47 57 ...  1 51 39]], shape=(32, 100), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for t,l in dataset:\n",
    "    print(t)\n",
    "    print(l)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T01:33:34.475013Z",
     "start_time": "2024-01-05T01:33:34.083875Z"
    }
   },
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## 모델 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8gPwEjRzf-Z"
   },
   "source": [
    "모델을 정의하려면 `tf.keras.Sequential`을 사용한다.\n",
    "\n",
    "이 예제에서는 3개의 층을 사용하여 모델을 정의한다:\n",
    "\n",
    "* `tf.keras.layers.Embedding` : 입력층. `embedding_dim` 차원 벡터에 각 문자의 정수 코드를 매핑하는 훈련 가능한 검색 테이블.\n",
    "* `tf.keras.layers.GRU` : 크기가 `units = rnn_units`인 RNN의 유형(여기서 LSTM층을 사용할 수도 있다.)\n",
    "* `tf.keras.layers.Dense` : 크기가 `vocab_size`인 출력을 생성하는 출력층.\n",
    "\n",
    "각 문자에 대해 모델은 임베딩을 검색하고, 임베딩을 입력으로 하여 GRU를 1개의 타임 스텝으로 실행하고, FC layers를 적용하여 다음 문자의 로그 가능도(log-likelihood)를 예측하는 로짓을 생성한다:"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 함수형태로 모델을 구성할 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zHT8cLh7EAsg",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "outputId": "21419c98-974a-4649-a649-d82b918d17f1",
    "ExecuteTime": {
     "end_time": "2024-01-05T02:16:50.316711Z",
     "start_time": "2024-01-05T02:16:50.294873Z"
    }
   },
   "source": [
    "# 문자로 된 어휘 사전의 크기\n",
    "vocab_size = 65\n",
    "\n",
    "# 임베딩 차원\n",
    "embedding_dim = 256 #  임의의 값 임베딩 차원 작은 모델에서는 1000 이하\n",
    "\n",
    "# RNN 유닛(unit) 개수\n",
    "rnn_units = 1024 #  임의의 값 RNN 유닛 개수는 1000 이상"
   ],
   "execution_count": 192,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wwsrpOik5zhv",
    "ExecuteTime": {
     "end_time": "2024-01-05T02:16:50.459506Z",
     "start_time": "2024-01-05T02:16:50.447183Z"
    }
   },
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim= vocab_size, output_dim=embedding_dim,\n",
    "                                batch_input_shape=[batch_size, None]), # 임베딩 레이어 배치만 잡음\n",
    "        tf.keras.layers.LSTM(units=rnn_units, \n",
    "                            return_sequences=True, # 단어가 들어오면 바로 다음단어 예측 \n",
    "                            stateful=True, # 이전배치의 마지막 상태가 다음 배치의 초기상태로 전달 학습이 잘되게 하기 위해서 \n",
    "                             # 개별적 문장일때는 False 서로 연관있는 문장일때는 True\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "\n",
    "    return model"
   ],
   "execution_count": 193,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xgzQLegT2H_J",
    "ExecuteTime": {
     "end_time": "2024-01-05T02:16:51.134759Z",
     "start_time": "2024-01-05T02:16:50.612543Z"
    }
   },
   "source": [
    "model = build_model(\n",
    "    vocab_size = vocab_size,\n",
    "    embedding_dim= embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ],
   "execution_count": 194,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pbNnqjvhI9lk",
    "ExecuteTime": {
     "end_time": "2024-01-05T02:16:51.135784Z",
     "start_time": "2024-01-05T02:16:51.117257Z"
    }
   },
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.output_shape)\n",
    "    \n",
    "    # 마지막은 시퀀스 "
   ],
   "execution_count": 195,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, None, 256)\n",
      "(32, None, 1024)\n",
      "(32, None, 65)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "## 모델 사용\n",
    "\n",
    "이제 모델을 실행하여 원하는대로 동작하는지 확인해보자.\n",
    "\n",
    "먼저 출력의 형태를 확인하자."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C-_70kKAPrPU",
    "ExecuteTime": {
     "end_time": "2024-01-05T02:16:52.011708Z",
     "start_time": "2024-01-05T02:16:51.120051Z"
    }
   },
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (배치 크기, 시퀀스 길이, 어휘 사전 크기)\")"
   ],
   "execution_count": 196,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 65) # (배치 크기, 시퀀스 길이, 어휘 사전 크기)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6NzLBi4VM4o"
   },
   "source": [
    "위 예제에서 입력의 시퀀스 길이는 100이지만 모델은 임의 시퀀스 길이의 입력도 사용 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vPGmAAXmVLGC",
    "ExecuteTime": {
     "end_time": "2024-01-05T02:16:52.015138Z",
     "start_time": "2024-01-05T02:16:52.010436Z"
    }
   },
   "source": [
    "model.summary()"
   ],
   "execution_count": 197,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_17 (Embedding)    (32, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm_15 (LSTM)              (32, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense_15 (Dense)            (32, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCbHQHiaa4Ic"
   },
   "source": [
    "이 문제는 표준 분류 문제로 취급될 수 있습니다. 이전 RNN 상태와 이번 타임 스텝(time step)의 입력으로 다음 문자의 클래스를 예측한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trpqTWyvk0nr"
   },
   "source": [
    "### Optimizer, Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAjbjY03eiQ4"
   },
   "source": [
    "`tf.keras.losses.sparse_softmax_crossentropy` 를 사용해 label을 벡터로 바꾸지 않고 loss를 계산한다.\n",
    "\n",
    "이 모델은 로짓을 반환하기 때문에 `from_logits` 플래그를 설정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4HrXTACTdzY-",
    "ExecuteTime": {
     "end_time": "2024-01-05T02:16:53.892493Z",
     "start_time": "2024-01-05T02:16:53.857224Z"
    }
   },
   "source": [
    "def loss(labels, logits):\n",
    "    # labels = tf.expand_dims(labels, axis=-1)  # 라벨 확장\n",
    "    # logits = tf.squeeze(logits, axis=-1)  # 로그의 마지막 차원 삭제\n",
    "    return tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(labels, logits)\n",
    "    # example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "    # print(example_batch_loss)"
   ],
   "execution_count": 198,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeOXriLcymww"
   },
   "source": [
    "### 학습준비\n",
    "* `tf.keras.Model.compile` 메서드를 사용하여 훈련 절차를 설정\n",
    "* 기본 매개변수의 `tf.keras.optimizers.Adam`과 손실 함수를 사용"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DDl1_Een6rL0",
    "ExecuteTime": {
     "end_time": "2024-01-05T02:16:54.886646Z",
     "start_time": "2024-01-05T02:16:54.863031Z"
    }
   },
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam( learning_rate=1e-3),\n",
    "              loss=loss)"
   ],
   "execution_count": 199,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieSJdchZggUj"
   },
   "source": [
    "### 체크포인트 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6XBUUavgF56"
   },
   "source": [
    "`tf.keras.callbacks.ModelCheckpoint`를 사용하여 훈련 중 체크포인트(checkpoint)가 저장되도록 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "W6fWTriUZP-n",
    "ExecuteTime": {
     "end_time": "2024-01-05T02:16:56.438939Z",
     "start_time": "2024-01-05T02:16:56.402643Z"
    }
   },
   "source": [
    "# the save point\n",
    "if use_colab:\n",
    "    checkpoint_dir ='./drive/My Drive/train_ckpt/text_gen/exp1'\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "else:\n",
    "    checkpoint_dir = 'text_gen/exp1'\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 monitor='loss',\n",
    "                                                 mode='auto',\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)"
   ],
   "execution_count": 200,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "### 훈련 실행"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7yGBE2zxMMHs",
    "ExecuteTime": {
     "end_time": "2024-01-05T02:16:57.384593Z",
     "start_time": "2024-01-05T02:16:57.374746Z"
    }
   },
   "source": [
    "EPOCHS=5"
   ],
   "execution_count": 201,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UK-hmKjYVoll",
    "ExecuteTime": {
     "end_time": "2024-01-05T02:21:47.189316Z",
     "start_time": "2024-01-05T02:16:57.842004Z"
    }
   },
   "source": [
    "history = model.fit(dataset,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[cp_callback])"
   ],
   "execution_count": 202,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 11:16:58.374040: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-01-05 11:16:58.856466: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-01-05 11:16:59.493854: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345/345 [==============================] - ETA: 0s - loss: 2.3035\n",
      "Epoch 1: loss improved from inf to 2.30351, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
      "345/345 [==============================] - 60s 164ms/step - loss: 2.3035\n",
      "Epoch 2/5\n",
      "345/345 [==============================] - ETA: 0s - loss: 1.6585\n",
      "Epoch 2: loss improved from 2.30351 to 1.65854, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
      "345/345 [==============================] - 58s 168ms/step - loss: 1.6585\n",
      "Epoch 3/5\n",
      "345/345 [==============================] - ETA: 0s - loss: 1.4799\n",
      "Epoch 3: loss improved from 1.65854 to 1.47988, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
      "345/345 [==============================] - 56s 162ms/step - loss: 1.4799\n",
      "Epoch 4/5\n",
      "345/345 [==============================] - ETA: 0s - loss: 1.3935\n",
      "Epoch 4: loss improved from 1.47988 to 1.39352, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
      "345/345 [==============================] - 55s 159ms/step - loss: 1.3935\n",
      "Epoch 5/5\n",
      "345/345 [==============================] - ETA: 0s - loss: 1.3355\n",
      "Epoch 5: loss improved from 1.39352 to 1.33551, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
      "345/345 [==============================] - 59s 169ms/step - loss: 1.3355\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## 텍스트 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIPcXllKjkdr"
   },
   "source": [
    "### 최근 체크포인트 복원"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyeYRiuVjodY"
   },
   "source": [
    "이 예측 단계에선 Batch size 1을 사용한다.\n",
    "\n",
    "* RNN 상태가 타임 스텝에서 타임 스텝으로 전달되는 방식이기 때문에 모델은 한 번 빌드된 고정 배치 크기만 허용\n",
    "* 다른 배치 크기로 모델을 실행하려면 모델을 다시 빌드하고 체크포인트에서 가중치를 복원해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LycQ-ot_jjyu",
    "ExecuteTime": {
     "end_time": "2024-01-05T02:29:55.046840Z",
     "start_time": "2024-01-05T02:29:54.807626Z"
    }
   },
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(checkpoint_dir)\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ],
   "execution_count": 217,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "71xa6jnYVrAN",
    "ExecuteTime": {
     "end_time": "2024-01-05T02:30:09.041870Z",
     "start_time": "2024-01-05T02:30:09.030356Z"
    }
   },
   "source": [
    "model.summary()"
   ],
   "execution_count": 218,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_22 (Embedding)    (1, None, 256)            16640     \n",
      "                                                                 \n",
      " lstm_20 (LSTM)              (1, None, 1024)           5246976   \n",
      "                                                                 \n",
      " dense_20 (Dense)            (1, None, 65)             66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "### 예측 루프\n",
    "\n",
    "텍스트 생성:\n",
    "\n",
    "* 시작 문자열 선택과 순환 신경망 상태를 초기화하고 생성할 문자 수를 설정\n",
    "\n",
    "* 시작 문자열과 순환 신경망 상태를 사용하여 다음 문자의 예측 배열을 가져온다.\n",
    "\n",
    "* 다음, 범주형 배열을 사용하여 예측된 문자의 인덱스를 계산\n",
    "\n",
    "* 이 예측된 문자를 모델의 다음 입력으로 활용\n",
    "\n",
    "* 모델에 의해 리턴된 RNN 상태는 모델로 피드백되어 이제는 하나의 단어가 아닌 더 많은 컨텍스트를 갖추게 된다.\n",
    "\n",
    "* 다음 단어를 예측한 후 수정된 RNN 상태가 다시 모델로 피드백되어 이전에 예측된 단어에서 더 많은 컨텍스트를 얻으면서 학습하는 방식\n",
    "\n",
    "![텍스트를 생성하기 위해 모델의 출력이 입력으로 피드백](https://tensorflow.org/tutorials/text/images/text_generation_sampling.png)\n",
    "\n",
    "* 생성된 텍스트를 보면 모델이 언제 대문자로 나타나고, 절을 만들고 셰익스피어와 유사한 어휘를 가져오는지 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WvuwZBX5Ogfd",
    "ExecuteTime": {
     "end_time": "2024-01-05T02:30:10.317555Z",
     "start_time": "2024-01-05T02:30:10.312971Z"
    }
   },
   "source": [
    "def generate_text(model, start_string):\n",
    "  # 평가 단계 (학습된 모델을 사용하여 텍스트 생성)\n",
    "\n",
    "  # 생성할 문자의 수\n",
    "  num_generate = 1000 # 토큰설정\n",
    "\n",
    "  # 시작 문자열을 숫자로 변환(벡터화)\n",
    "  input_eval = [char2idx[s] for s in start_string]  \n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # 결과를 저장할 빈 문자열\n",
    "  text_generated = []\n",
    "\n",
    "  # 온도가 낮으면 더 예측 가능한 텍스트 생성\n",
    "  # 온도가 높으면 더 의외의 텍스트 생성 (불확실성)\n",
    "  # 최적의 세팅을 찾기 위한 실험\n",
    "  temperature = 1.0 \n",
    "\n",
    "  # 여기에서 배치 크기 == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # 배치 차원 제거\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # 범주형 분포를 사용하여 모델에서 리턴한 단어 예측\n",
    "      predictions = predictions / temperature  # 에측을 변화시켜 불확실성을 조절\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy() # 랜덤하게 샘플링\n",
    "    #고유문자수의 idx를 랜덤하게 뽑아서 예측값으로 사용\n",
    "      # 예측된 단어를 다음 입력으로 모델에 전달\n",
    "      # 이전 은닉 상태와 함께\n",
    "      input_eval = tf.expand_dims([predicted_id], 0) # 예측된 단어를 다음 입력으로 모델에 전달\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ],
   "execution_count": 219,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ktovv0RFhrkn",
    "ExecuteTime": {
     "end_time": "2024-01-05T02:30:18.287630Z",
     "start_time": "2024-01-05T02:30:10.861058Z"
    }
   },
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ],
   "execution_count": 220,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: OCHzXQjZxxxXzzXC$XxjXKxxJxXXxXXxKJQxxxxJXXQxXxxXXXxxxx$ZXxjX&XKzXxzXXXxZXVxjx&$XXXXXxXxjX$A3XX$XZjXXx3xxxZXxxxx&xZx$Zxxx$xZxz$JZXX$qqjX$XjVEjxxxXxZVxJVxJ$J$XXxVxxxXzK&JZx&xZxZxxxzZxXxXXXQ&X&xxQxxXX$jXxPxXxxJJx&XXxJ3xMXXQJX3XjxxxXXxX&KxjXx&FXXz&x$XxxKDzXZxXXx3XxxQXqXXqJXxCQx&xxxxXXXJQBXXZXE$Qx$XJ&ZxjXjxQYXxxXx$XXxxVXxQQJZQXxDXxxxXjX&xJ3VzxxjQx&XX&ZJxMxzBxXGSxXZQXxQq&&x$&SX$jXxJ3xXXXCZxxxZxxXxZxxJjxDx$&$XxxXQJZXjJQqxZ$J3X&QXxxxxxJXxxQx&&3x$XXXZxZx&XxXqjxxxxxXxxXXj&VDXXxVxjXX$3XZxQDxX$XxJXXxXxXXXxVxX&xxX3XxZjxX3DxxXxXQXxx&XxxqzZ&xZXxP&XXXGxXxxjXxqXXXxJZXxjXxXV$XxjjZXZ$JQxXZXXXx&$x&XjQXXxXxx$x&zxZxxjxZXXXVZ$xXXxXjJJZxxxQZxJxxxxxxxXJj&X$BQxXZxVxXX3Mx&xxxxxx$Xxxxx3jKxxxjjXXZxJ$kXxxJXZxxQXxZZJQZzjjzxQJMX$XxVYXMXxxxXjXJx&&xZxQxjqXx$ZQJ&KxZXG3ZXVxXQxZ&Zz$JxXjqjYxxXXxxXxzxXXxx&XxXKX&$XxXSCZXxxXXxKD3XXxxxXxxxjXxxxXXXxxkxxXxxxxx&XJxXJZjxx$XPx&zXYxjXx$XXjVZ3&KxxxXXxJSjxVqXJ$QXX$Px&XxG&$xjxJxxCzZ&XX$$XU$XLXx$&X&DXxx$qxXQj3ZxXXXzjXXYJXXXxjZ&XXQXXXQxJxJJ$ZZxjjKxHkZZXQXXDBxxZXZGJJQZxVJxxjxxx$$KXZxPKJXq\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S5NJ0DrPzJsz",
    "ExecuteTime": {
     "start_time": "2024-01-05T01:33:34.496213Z"
    }
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}

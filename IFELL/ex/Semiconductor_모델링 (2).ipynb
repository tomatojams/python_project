{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THNjjG50BYJY"
   },
   "source": [
    "# 반도체 박막 두께 분석\n",
    "\n",
    "### 배경\n",
    "\n",
    "최근 고사양 반도체 수요가 많아지면서 반도체를 수직으로 적층하는 3차원 공정이 많이 연구되고 있습니다. 반도체 박막을 수십 ~ 수백 층 쌓아 올리는 공정에서는 박막의 결함으로 인한 두께와 균일도가 저하되는 문제가 있습니다. 이는 소자 구조의 변형을 야기하며 성능 하락의 주요 요인이 됩니다. 이를 사전에 방지하기 위해서는 박막의 두께를 빠르면서도 정확히 측정하는 것이 중요합니다.\n",
    "\n",
    "박막의 두께를 측정하기 위해 광스펙트럼 분석이 널리 사용되고 있습니다. 하지만 광 스펙트럼을 분석하기 위해서는 관련 지식을 많이 가진 전문가가 필요하며 분석과정에 많은 컴퓨팅자원이 필요합니다. 빅데이터 분석을 통해 이를 해결하고자 반도체 소자의 두께 분석 알고리즘 경진대회를 개최합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XN-kkqUGNko"
   },
   "source": [
    "### 배경 자료\n",
    "\n",
    "반도체 박막은 얇은 반도체 막으로 박막의 종류와 두께는 반도체 소자의 특성을 결정짓는 중요한 요소 중 하나입니다. 박막의 두께를 측정하는 방법으로 반사율 측정이 널리 사용되며 반사율은 입사광 세기에 대한 반사광 세기의 비율로 정해집니다. (반사율 = 반사광/입사광) 반사율은 빛의 파장에 따라 변하며 파장에 따른 반사율의 분포를 반사율 스펙트럼이라고 합니다.\n",
    "\n",
    "\n",
    "\n",
    "### 구조 설명\n",
    "\n",
    "이번 대회에서 분석할 소자는 질화규소(layer_1)/이산화규소(layer_2)/질화규소(layer_3)/이산화규소(layer_4)/규소(기판) 총 5층 구조로 되어 있습니다. 대회의 목적은 기판인 규소를 제외한 layer_1 ~ layer_4의 두께를 예측하는 것으로 학습 데이터 파일에는 각 층의 두께와 반사율 스펙트럼이 포함되어 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "### 데이터 설명\n",
    "\n",
    "train.csv 파일에는 4층 박막의 두께와 파장에 따른 반사율 스펙트럼이 주어집니다. 헤더의 이름에 따라 layer_1 ~ 4는 해당 박막의 두께,\n",
    "\n",
    "데이터값인 0 ~ 225은 빛의 파장에 해당하는 반사율이 됩니다. 헤더 이름인 0~225은 파장을 뜻하며 비식별화 처리가 되어있어 실제 값과는 다릅니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VHebfq47CZ-V",
    "ExecuteTime": {
     "end_time": "2024-01-09T14:02:47.310786800Z",
     "start_time": "2024-01-09T14:02:43.601398700Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, Conv2D, SeparableConv2D, Flatten, Dropout, MaxPooling2D, \\\n",
    "    BatchNormalization, ReLU, GlobalAveragePooling2D\n",
    "from tensorflow.keras import regularizers"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ATOrWJKuLlBT",
    "ExecuteTime": {
     "end_time": "2024-01-09T14:02:47.338995100Z",
     "start_time": "2024-01-09T14:02:47.311785700Z"
    }
   },
   "source": [
    "use_colab = True\n",
    "assert use_colab in [True, False]"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lsFNOIeSn_Z9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bebbdf4c-bf60-4630-eab6-4dfe0dad0ede",
    "ExecuteTime": {
     "end_time": "2024-01-09T14:02:47.354128400Z",
     "start_time": "2024-01-09T14:02:47.326888700Z"
    }
   },
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qoXy0bqBqKt"
   },
   "source": [
    "### 데이터 로드\n",
    "* 새로운 버전의 Colab파일에선 왼쪽의 폴더 tree에서 직접 드라이브 마운트를 진행해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kwp9HSrrD4XW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2425665e-6b38-4784-99a2-d8575790eb34",
    "ExecuteTime": {
     "end_time": "2024-01-09T14:02:51.265905400Z",
     "start_time": "2024-01-09T14:02:50.842816500Z"
    }
   },
   "source": [
    "train_d = np.load(\"./data/semicon_train_data.npy\")\n",
    "test_d = np.load(\"./data/semicon_test_data.npy\")\n",
    "\n",
    "# train data path\n",
    "# train_d = np.load(\"./semicon_train_data.npy\")\n",
    "\n",
    "# test data path\n",
    "# train_d = np.load(\"./semicon_test_data.npy\")\n",
    "print(train_d.shape, test_d.shape)"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(729000, 230) (81000, 230)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EHTnjvhZi4x9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cf4480b3-3ed3-4361-edb0-eba26682b9ac",
    "ExecuteTime": {
     "end_time": "2024-01-09T14:02:52.467944700Z",
     "start_time": "2024-01-09T14:02:52.444954200Z"
    }
   },
   "source": [
    "print(train_d[1].shape)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OsF_kurljuwU",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9a0217b4-c68c-4e4a-f675-160a0a027f78",
    "ExecuteTime": {
     "end_time": "2024-01-09T14:02:56.210769800Z",
     "start_time": "2024-01-09T14:02:56.177297900Z"
    }
   },
   "source": [
    "print(test_d[0].shape)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OdXgDw8oyTQf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9863b335-026d-4906-df87-ac237f28ec35",
    "ExecuteTime": {
     "end_time": "2024-01-09T14:02:56.847185700Z",
     "start_time": "2024-01-09T14:02:56.807333700Z"
    }
   },
   "source": [
    "# Raw Data 확인\n",
    "print(train_d[1000])\n",
    "# print(test_d[0])"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.00000000e+01 4.00000000e+01 1.50000000e+02 1.10000000e+02\n",
      " 4.02068880e-01 4.05173400e-01 4.22107550e-01 4.35197980e-01\n",
      " 4.66261860e-01 4.67785980e-01 4.76300870e-01 5.07203700e-01\n",
      " 5.23296600e-01 5.31541170e-01 5.23319200e-01 5.32340650e-01\n",
      " 5.38513840e-01 5.62478700e-01 5.74338900e-01 5.62083540e-01\n",
      " 5.83467800e-01 5.88462350e-01 5.89606940e-01 5.93186560e-01\n",
      " 5.93241930e-01 5.98241150e-01 5.87171000e-01 5.92055500e-01\n",
      " 6.06527860e-01 5.87087870e-01 5.94492100e-01 5.73233500e-01\n",
      " 5.94926300e-01 5.76777600e-01 5.70286300e-01 5.76913000e-01\n",
      " 5.52754400e-01 5.61735150e-01 5.52500000e-01 5.23800200e-01\n",
      " 5.11056840e-01 5.11732600e-01 4.92525460e-01 4.73373200e-01\n",
      " 4.57444160e-01 4.53428800e-01 4.19270130e-01 3.87709920e-01\n",
      " 3.87221520e-01 3.49848450e-01 3.16229370e-01 3.13858700e-01\n",
      " 2.80085700e-01 2.61263000e-01 2.30250820e-01 1.98457850e-01\n",
      " 1.53500440e-01 1.36026740e-01 1.08726785e-01 8.95344000e-02\n",
      " 6.58975300e-02 5.09539300e-02 3.69604570e-02 2.88708470e-02\n",
      " 1.54857090e-02 1.99594010e-02 2.33170990e-02 3.26487760e-02\n",
      " 5.89796340e-02 6.33707600e-02 9.20634640e-02 1.24182590e-01\n",
      " 1.63985540e-01 1.81501220e-01 2.16830660e-01 2.57668080e-01\n",
      " 2.86428720e-01 3.21613600e-01 3.65077380e-01 3.85984180e-01\n",
      " 4.33254400e-01 4.36655340e-01 4.60312520e-01 4.99771740e-01\n",
      " 5.03849100e-01 5.48682200e-01 5.58029060e-01 5.62947630e-01\n",
      " 5.99234340e-01 5.96399550e-01 6.04513000e-01 6.19093400e-01\n",
      " 6.38402700e-01 6.64816900e-01 6.56996000e-01 6.66098600e-01\n",
      " 6.69130500e-01 6.70661600e-01 6.80202360e-01 6.92821600e-01\n",
      " 6.96852400e-01 7.02015160e-01 7.00507460e-01 7.00250800e-01\n",
      " 7.11669300e-01 6.96838260e-01 7.08974360e-01 7.19114100e-01\n",
      " 7.18034700e-01 7.00589540e-01 7.05873250e-01 6.92893500e-01\n",
      " 7.14911160e-01 6.85250640e-01 6.82136830e-01 6.81311400e-01\n",
      " 6.78019200e-01 6.67259900e-01 6.78185700e-01 6.52875660e-01\n",
      " 6.34932940e-01 6.36031400e-01 6.16502100e-01 5.91461400e-01\n",
      " 5.74673350e-01 5.57246200e-01 5.42328200e-01 5.19422200e-01\n",
      " 5.03134850e-01 4.81939820e-01 4.45872100e-01 4.17206170e-01\n",
      " 4.05306970e-01 3.55970500e-01 3.27808440e-01 2.89738060e-01\n",
      " 2.44837760e-01 2.16494500e-01 1.69062380e-01 1.23260050e-01\n",
      " 1.03297760e-01 6.77379500e-02 3.89807700e-02 2.52380610e-02\n",
      " 2.59922780e-02 1.38827340e-02 1.90851740e-02 6.68724550e-02\n",
      " 7.21453900e-02 1.14929430e-01 1.77696590e-01 2.11153720e-01\n",
      " 2.55038900e-01 2.95599580e-01 3.21243700e-01 3.74151170e-01\n",
      " 3.91486200e-01 4.27665320e-01 4.61769700e-01 4.75576340e-01\n",
      " 4.78055980e-01 5.16503040e-01 5.30934450e-01 5.23902500e-01\n",
      " 5.51259300e-01 5.65235300e-01 5.67701600e-01 5.70858800e-01\n",
      " 5.76418300e-01 5.75021860e-01 6.01967200e-01 5.79546750e-01\n",
      " 5.81157900e-01 5.93576100e-01 5.84070600e-01 6.04394850e-01\n",
      " 5.87139800e-01 5.93731460e-01 5.91678200e-01 5.63344300e-01\n",
      " 5.69886800e-01 5.46404960e-01 5.43537260e-01 5.42394760e-01\n",
      " 5.42762940e-01 5.26218060e-01 4.89471520e-01 4.72608000e-01\n",
      " 4.59845570e-01 4.39476280e-01 4.24489440e-01 4.16756120e-01\n",
      " 4.11724180e-01 3.81418440e-01 3.60050380e-01 3.37973600e-01\n",
      " 3.20442830e-01 2.97488420e-01 2.85940900e-01 2.82688900e-01\n",
      " 2.78886770e-01 2.60735480e-01 2.64526520e-01 2.38374640e-01\n",
      " 2.46523140e-01 2.54422630e-01 2.63443470e-01 2.77559100e-01\n",
      " 3.05792480e-01 3.04755750e-01 3.47745950e-01 3.42683080e-01\n",
      " 3.59969050e-01 3.90264000e-01 4.21746500e-01 4.33448700e-01\n",
      " 4.50429620e-01 4.94950400e-01 4.90516330e-01 5.15429500e-01\n",
      " 5.46570300e-01 5.64438460e-01 5.68110800e-01 5.71491600e-01\n",
      " 5.81447660e-01 5.84012000e-01 6.05881900e-01 5.98874870e-01\n",
      " 5.97313170e-01 5.95610300e-01]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJTUCIIAt01D"
   },
   "source": [
    "* 데이터를 분석하여 모델에 학습할 수 있는 형태로 정리합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "awmkQxq8sWsG",
    "ExecuteTime": {
     "end_time": "2024-01-09T14:02:58.387515300Z",
     "start_time": "2024-01-09T14:02:58.370061700Z"
    }
   },
   "source": [
    "train = train_d[:,4:] # 729000, 230 4번째 데이터부터 가져온다.\n",
    "label = train_d[:,:4]\n",
    "\n",
    "t_train = test_d[:,4:] # 81000, 230개 4번째 까지 데이터를 가져온다.\n",
    "t_label = test_d[:,:4]"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u0Cb4BQPKqVs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b46a3a45-cf2a-4bc6-eb50-33a39875941d",
    "ExecuteTime": {
     "end_time": "2024-01-09T14:03:03.345492900Z",
     "start_time": "2024-01-09T14:02:59.414665800Z"
    }
   },
   "source": [
    "batch_size = 256\n",
    "\n",
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train, label))\n",
    "train_dataset = train_dataset.shuffle(10000).repeat().batch(batch_size=batch_size)\n",
    "print(train_dataset)\n",
    "\n",
    "# for test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((t_train, t_label))\n",
    "test_dataset = test_dataset.batch(batch_size=batch_size)\n",
    "print(test_dataset)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 226), dtype=tf.float64, name=None), TensorSpec(shape=(None, 4), dtype=tf.float64, name=None))>\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 226), dtype=tf.float64, name=None), TensorSpec(shape=(None, 4), dtype=tf.float64, name=None))>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsJ2qMl3Bryb"
   },
   "source": [
    "### 모델 구성\n",
    "* 현재 가장 간단한 모델로 구성되어 있습니다.\n",
    "* 데이터셋 자체를 가장 잘 학습할 수 있는 모델을 구현해 학습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras import layers, regularizers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# 클래스버젼 - Seperable Conv2D\n",
    "class ResBlock(tf.keras.Model):\n",
    "    def __init__(self, num_filter, stride=1, kernel_size=3, l2_reg=1e-4, dim = None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.num_filter = num_filter\n",
    "        self.dim = dim\n",
    "        # print(self.dim,self.num_filter)\n",
    "\n",
    "        self.conv1 = layers.Conv2D(num_filter, kernel_size, strides=stride, padding='same',\n",
    "                                   kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                                   kernel_regularizer=regularizers.l2(0.001))\n",
    "        self.se_conv1 = layers.SeparableConv2D(num_filter, kernel_size, padding='same',\n",
    "                                               depthwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               pointwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               depthwise_regularizer=regularizers.l2(0.001),\n",
    "                                               pointwise_regularizer=regularizers.l2(0.001))\n",
    "        self.se_sonv2 = layers.SeparableConv2D(num_filter, kernel_size, padding='same',\n",
    "                                               depthwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               pointwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               depthwise_regularizer=regularizers.l2(0.001),\n",
    "                                               pointwise_regularizer=regularizers.l2(0.001))\n",
    "        self.con_short = layers.Conv2D(num_filter, 1, strides=2, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.bn_short = layers.BatchNormalization()\n",
    "        self.LSTM = tf.keras.layers.LSTM(dim*dim,return_sequences=True,recurrent_initializer='glorot_uniform')\n",
    "        self.LSTM2 = tf.keras.layers.LSTM(dim*dim,return_sequences=True,recurrent_initializer='glorot_uniform')\n",
    "        self.LSTM3 = tf.keras.layers.LSTM(dim*dim)\n",
    "        # self.con1d_1 = layers.Conv2D()\n",
    "\n",
    "    def call(self, input, stride=1, dim=None):\n",
    "        self.dim = dim\n",
    "        shortcut = input\n",
    "        x = self.conv1(input)\n",
    "\n",
    "        x = self.bn1(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "\n",
    "\n",
    "        print(\"변환하기전:\",x.shape, dim, self.num_filter,stride)\n",
    "        RNN_x = layers.Reshape(target_shape=(-1, dim * dim * self.num_filter))(x) # 컨볼루션을 쓰기위한 변환 (64,4) -> CNN1D또는 RNN 사용가능 256,1도 가능\n",
    "        # print(\"변환후:\",RNN_x.shape)\n",
    "\n",
    "        RNN_x = self.LSTM(RNN_x)\n",
    "        RNN_x = self.LSTM2(RNN_x)\n",
    "        RNN_x = self.LSTM3(RNN_x)\n",
    "        # print(\"LSTM후:\",RNN_x.shape)\n",
    "        RNN_x = layers.Reshape(target_shape=(dim,dim,1))(RNN_x)  # RNN_x 리쉐이프\n",
    "        # print(\"재변환후:\",RNN_x.shape)\n",
    "        C1D_x = RNN_x\n",
    "\n",
    "\n",
    "        x = self.se_conv1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.se_sonv2(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        # 입력과 출력의 맞도록 숏컷을 조절\n",
    "        if shortcut.shape[-1] != self.num_filter or stride != 1:\n",
    "            shortcut = self.con_short(shortcut)\n",
    "            shortcut = self.bn_short(shortcut)\n",
    "\n",
    "        x = layers.Add()([x, shortcut])\n",
    "        # print(\"add후:\",x.shape)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet28(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet28, self).__init__()\n",
    "        self.dim = None  # dim 초기화\n",
    "\n",
    "        self.conv1 = layers.Conv2D(64, 7, strides=2,\n",
    "                                   kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                                   padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "        self.relu2 = layers.ReLU()\n",
    "\n",
    "        self.blocks = [\n",
    "            ResBlock(64, stride=1, dim = 8),\n",
    "            ResBlock(128, stride=2, dim = 4),\n",
    "            ResBlock(256, stride=2, dim = 2),\n",
    "            ResBlock(256, stride=1, dim = 2),\n",
    "            ResBlock(512, stride=2, dim = 1)\n",
    "        ]\n",
    "\n",
    "        self.avg_pool = layers.GlobalAveragePooling2D()\n",
    "        self.dropout = layers.Dropout(0.5)\n",
    "        self.out = layers.Dense(4)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = layers.Dense(16 * 16 * 1)(inputs)\n",
    "        x = layers.Reshape(target_shape=(16, 16, 1))(x)  # 컨볼루션을 쓰기위한 변환 (64,4) -> CNN1D또는 RNN 사용가능 256,1도 가능\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)  # 8x8*64\n",
    "\n",
    "\n",
    "        x = self.blocks[0](x,dim =8)\n",
    "        x = self.blocks[1](x,dim =4)\n",
    "        x = self.blocks[2](x,dim =2)\n",
    "        x = self.blocks[3](x,dim =2)\n",
    "        x = self.blocks[4](x,dim =1)\n",
    "        x = self.avg_pool(x)\n",
    "        x = layers.Dense(1000)(x)\n",
    "        # x =  x = layers.Reshape(target_shape=(1))(x)\n",
    "        # x = layers.LSTM(64)(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Specify input shape and number of classes\n",
    "input_tensor = layers.Input(shape=(226,))  # Example input shape for image classification\n",
    "\n",
    "# Build ResNet model\n",
    "Res = ResNet28()\n",
    "output_tensor = Res.call(input_tensor)\n",
    "model = tf.keras.Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j5z-Mwys0HJK",
    "outputId": "3a6f15a4-5d22-464b-dbfa-795debe5c615",
    "ExecuteTime": {
     "end_time": "2024-01-09T13:55:23.725941800Z",
     "start_time": "2024-01-09T13:55:23.254230400Z"
    }
   },
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 226)]             0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               58112     \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 16, 16, 1)         0         \n",
      "                                                                 \n",
      " conv2d_44 (Conv2D)          (None, 8, 8, 64)          3200      \n",
      "                                                                 \n",
      " batch_normalization_92 (Bat  (None, 8, 8, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_8 (ReLU)              (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " res_block_20 (ResBlock)     (None, 8, 8, 64)          47168     \n",
      "                                                                 \n",
      " res_block_21 (ResBlock)     (None, 4, 4, 128)         119552    \n",
      "                                                                 \n",
      " res_block_22 (ResBlock)     (None, 2, 2, 256)         468480    \n",
      "                                                                 \n",
      " res_block_23 (ResBlock)     (None, 2, 2, 256)         729344    \n",
      "                                                                 \n",
      " res_block_24 (ResBlock)     (None, 1, 1, 512)         1854464   \n",
      "                                                                 \n",
      " global_average_pooling2d_4   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1000)              513000    \n",
      "                                                                 \n",
      " batch_normalization_93 (Bat  (None, 1000)             4000      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_9 (ReLU)              (None, 1000)              0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 4)                 4004      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,801,580\n",
      "Trainable params: 3,790,364\n",
      "Non-trainable params: 11,216\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "FaxFpBbxyUzv",
    "ExecuteTime": {
     "end_time": "2024-01-09T13:55:23.742476200Z",
     "start_time": "2024-01-09T13:55:23.694763900Z"
    }
   },
   "execution_count": 76,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # 수직연산\n",
    "\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# input_tensor = layers.Input(shape=(226,))\n",
    "\n",
    "# # 목표 출력의 형태가 (batch_size, 4)인 경우에 맞춤\n",
    "# # target_shape = (16, 16, 4)\n",
    "\n",
    "# x = layers.Dense(8 * 8 * 4, activation='relu')(input_tensor)\n",
    "\n",
    "# x = layers.Reshape(target_shape=(16, 16, 1))(x)\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "\n",
    "# x = layers.Reshape(target_shape=(-1, 16 * 16 * 64))(x)\n",
    "# x = layers.LSTM(256)(x)\n",
    "# x = layers.Reshape(target_shape=(16, 16, 1))(x)\n",
    "\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "# output_tensor = layers.Dense(4)(x)  # 목표 데이터 형태에 맞게 조정\n",
    "\n",
    "# model = models.Model(input_tensor, output_tensor)\n",
    "# model.summary()\n"
   ],
   "metadata": {
    "id": "llyyseW9Pjtz",
    "ExecuteTime": {
     "end_time": "2024-01-09T13:55:23.749496200Z",
     "start_time": "2024-01-09T13:55:23.712315500Z"
    }
   },
   "execution_count": 77,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # 컨벌루션 하고 병렬연산\n",
    "\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# input_tensor = layers.Input(shape=(226,))\n",
    "\n",
    "# # 목표 출력의 형태가 (batch_size, 4)인 경우에 맞춤\n",
    "# # target_shape = (16, 16, 4)\n",
    "\n",
    "# x_input = layers.Dense(8 * 8 * 4, activation='relu')(input_tensor)\n",
    "\n",
    "# x = layers.Reshape(target_shape=(16, 16, 1))(x_input)\n",
    "# x_bi = layers.Conv2D(64, 3, padding='same',strides =2)(x)\n",
    "# # 8 * 8* 64 사이즈가 됨\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x_bi)\n",
    "\n",
    "\n",
    "# # 8* 8*64 사이즈를 계산해봄\n",
    "\n",
    "# x2 = layers.Reshape(target_shape=(-1, 8 * 8*64))(x_bi)\n",
    "# x2 = layers.LSTM(8*8*64)(x2)\n",
    "# x2 = layers.Reshape(target_shape=(8, 8, 64))(x2)\n",
    "\n",
    "# x = layers.add([x,x2])\n",
    "\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "# output_tensor = layers.Dense(4)(x)  # 목표 데이터 형태에 맞게 조정\n",
    "\n",
    "# model = models.Model(input_tensor, output_tensor)\n",
    "# model.summary()\n"
   ],
   "metadata": {
    "id": "mvbX3yprabj1",
    "ExecuteTime": {
     "end_time": "2024-01-09T13:55:23.760526800Z",
     "start_time": "2024-01-09T13:55:23.729457900Z"
    }
   },
   "execution_count": 78,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # 컨벌루션 하고 병렬연산  - 작동하지만 사이즈가 너무 커짐\n",
    "\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# input_tensor = layers.Input(shape=(226,))\n",
    "\n",
    "# # 목표 출력의 형태가 (batch_size, 4)인 경우에 맞춤\n",
    "# # target_shape = (16, 16, 4)\n",
    "\n",
    "# x_input = layers.Dense(8 * 8 * 4, activation='relu')(input_tensor)\n",
    "\n",
    "# x = layers.Reshape(target_shape=(16, 16, 1))(x_input)\n",
    "# x_bi = layers.Conv2D(64, 3, padding='same',strides =2)(x)\n",
    "# # 8 * 8* 64 사이즈가 됨\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x_bi)\n",
    "\n",
    "\n",
    "# # 8* 8*64 사이즈를 계산해봄\n",
    "\n",
    "# x2 = layers.Reshape(target_shape=(-1, 8 * 8*64))(x_bi)\n",
    "# x2 = layers.LSTM(8*8*64)(x2)\n",
    "# x2 = layers.Reshape(target_shape=(8, 8, 64))(x2)\n",
    "\n",
    "# x = layers.add([x,x2])\n",
    "\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "# output_tensor = layers.Dense(4)(x)  # 목표 데이터 형태에 맞게 조정\n",
    "\n",
    "# model = models.Model(input_tensor, output_tensor)\n",
    "# model.summary()\n"
   ],
   "metadata": {
    "id": "vqLh5Q0Ygpmi",
    "ExecuteTime": {
     "end_time": "2024-01-09T13:55:23.762525300Z",
     "start_time": "2024-01-09T13:55:23.743475300Z"
    }
   },
   "execution_count": 79,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # 컨벌루션 하고 병렬연산\n",
    "\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# input_tensor = layers.Input(shape=(226,))\n",
    "\n",
    "# # 목표 출력의 형태가 (batch_size, 4)인 경우에 맞춤\n",
    "# # target_shape = (16, 16, 4)\n",
    "\n",
    "# x_input = layers.Dense(8 * 8 * 4, activation='relu')(input_tensor)\n",
    "\n",
    "# x = layers.Reshape(target_shape=(16, 16, 1))(x_input)\n",
    "# x_bi = layers.Conv2D(64, 3, padding='same',strides =2)(x)\n",
    "# # 8 * 8* 64 사이즈가 됨\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x_bi)\n",
    "\n",
    "\n",
    "# # 8* 8*64 사이즈를 계산해봄\n",
    "\n",
    "# x2 = layers.Reshape(target_shape=(-1, 8 * 8*64))(x_bi)\n",
    "# x2 = layers.LSTM(8*8*64)(x2)\n",
    "# x2 = layers.Reshape(target_shape=(8, 8, 64))(x2)\n",
    "\n",
    "# x = layers.add([x,x2])\n",
    "\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "# output_tensor = layers.Dense(4)(x)  # 목표 데이터 형태에 맞게 조정\n",
    "\n",
    "# model = models.Model(input_tensor, output_tensor)\n",
    "# model.summary()\n"
   ],
   "metadata": {
    "id": "-IxAf9ROabuK",
    "ExecuteTime": {
     "end_time": "2024-01-09T13:55:23.780345500Z",
     "start_time": "2024-01-09T13:55:23.758511800Z"
    }
   },
   "execution_count": 80,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # 컨벌루션 하고 병렬연산  - 사이즈 조절해서 add\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# input_tensor = layers.Input(shape=(226,))\n",
    "\n",
    "# # 목표 출력의 형태가 (batch_size, 4)인 경우에 맞춤\n",
    "# # target_shape = (16, 16, 4)\n",
    "\n",
    "# x_input = layers.Dense(8 * 8 * 4, activation='relu')(input_tensor)\n",
    "\n",
    "# x = layers.Reshape(target_shape=(16, 16, 1))(x_input)\n",
    "# x_bi = layers.Conv2D(64, 3, padding='same',strides =2)(x)\n",
    "# # 8 * 8* 64 사이즈가 됨\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x_bi)\n",
    "\n",
    "\n",
    "# # 8* 8*64 사이즈를 계산해봄\n",
    "\n",
    "# x2 = layers.Reshape(target_shape=(-1, 8 * 8*64))(x_bi)\n",
    "# x2 = layers.LSTM(32, return_sequences=True)(x2)  # 여기서 사이즈를 줄이면 됨\n",
    "# x2 = layers.LSTM(64)(x2) # 여기서 사이즈를 줄이면 됨\n",
    "# x2 = layers.Reshape(target_shape=(8, 8, 1))(x2)\n",
    "\n",
    "# x = layers.add([x,x2])\n",
    "\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "# output_tensor = layers.Dense(4)(x)  # 목표 데이터 형태에 맞게 조정\n",
    "\n",
    "# model = models.Model(input_tensor, output_tensor)\n",
    "# model.summary()\n"
   ],
   "metadata": {
    "id": "KPGQ7dPlJrVf",
    "ExecuteTime": {
     "end_time": "2024-01-09T13:55:23.796394100Z",
     "start_time": "2024-01-09T13:55:23.773322700Z"
    }
   },
   "execution_count": 81,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zzo9y_5eryuq",
    "ExecuteTime": {
     "end_time": "2024-01-09T13:55:23.811107100Z",
     "start_time": "2024-01-09T13:55:23.788898400Z"
    }
   },
   "source": [
    "# input_tensor = layers.Input(shape=(226,))\n",
    "# # CNN1D, CNN2D, RNN 세가지를 사용해서 모델링\n",
    "\n",
    "# # 226에서 256으로 확장\n",
    "# x = layers.Dense(8*8*4, activation = 'relu')(input_tensor)\n",
    "\n",
    "# CNN_x = layers.Reshape(target_shape=(8,8,4))(x) # 컨볼루션을 쓰기위한 변환 (64,4) -> CNN1D또는 RNN 사용가능 256,1도 가능\n",
    "# RNN_x = layers.Reshape(target_shape=(256,1))(x)\n",
    "# Dense_x = RNN_x\n",
    "\n",
    "\n",
    "# x = tf.add([x, x_skip]) # batch, 128 + 226\n",
    "\n",
    "\n",
    "# x = layers.Dense(16,\n",
    "#                  kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Activation('relu')(x)\n",
    "\n",
    "# output_tensor = layers.Dense(4)(x)\n",
    "\n",
    "# model = tf.keras.Model(input_tensor, output_tensor)"
   ],
   "execution_count": 82,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0OODMx4Btb5"
   },
   "source": [
    "### 모델 학습\n",
    "* 4개층의 박막의 두께를 예측하는 모델을 학습해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YlhAzzfCLDQ0",
    "ExecuteTime": {
     "end_time": "2024-01-09T13:55:23.824643400Z",
     "start_time": "2024-01-09T13:55:23.804583400Z"
    }
   },
   "source": [
    "# the save point\n",
    "if use_colab:\n",
    "    checkpoint_dir ='./drive/My Drive/train_ckpt/semiconductor/exp1'\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "else:\n",
    "    checkpoint_dir = 'semiconductor/exp1'\n",
    "\n"
   ],
   "execution_count": 83,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 monitor='val_mae',\n",
    "                                                 mode='auto',\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=15,\n",
    "                                                     monitor='val_loss',\n",
    "                                                     restore_best_weights=True,\n",
    "\n",
    "                                                     verbose=1)\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',  # 관찰할 지표\n",
    "                              factor=0.2,  # 학습률을 줄이는 비율\n",
    "                              patience=5,  # 몇 번의 에포크 동안 감소하지 않아야 하는지\n",
    "                              min_lr=1e-9)"
   ],
   "metadata": {
    "id": "SHMZXYLxahcH",
    "ExecuteTime": {
     "end_time": "2024-01-09T13:55:23.839197Z",
     "start_time": "2024-01-09T13:55:23.820138100Z"
    }
   },
   "execution_count": 84,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XaTHvzqjuCQS",
    "ExecuteTime": {
     "end_time": "2024-01-09T13:55:23.875196200Z",
     "start_time": "2024-01-09T13:55:23.834669200Z"
    }
   },
   "source": [
    "model.compile(loss='mae', #mse\n",
    "              optimizer='adam',\n",
    "              metrics=['mae']) #mse"
   ],
   "execution_count": 85,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QUUO_VcoLJEg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "936958de-3595-4b89-96ab-3bcd60f22ef2",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-09T13:55:23.851382100Z"
    }
   },
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    steps_per_epoch=len(train) / batch_size, # train data의 길이 // batch 길이\n",
    "                    epochs=100,\n",
    "                    callbacks=[cp_callback,early_stopping_cb,reduce_lr],\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=len(t_train) / batch_size)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2848/2847 [==============================] - ETA: 0s - loss: 45.6339 - mae: 43.5353\n",
      "Epoch 1: val_mae improved from inf to 22.41954, saving model to ./drive/My Drive/train_ckpt/semiconductor\\exp1\n",
      "2847/2847 [==============================] - 55s 18ms/step - loss: 45.6339 - mae: 43.5353 - val_loss: 24.4505 - val_mae: 22.4195 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "2848/2847 [==============================] - ETA: 0s - loss: 19.6398 - mae: 17.4769\n",
      "Epoch 2: val_mae improved from 22.41954 to 12.19423, saving model to ./drive/My Drive/train_ckpt/semiconductor\\exp1\n",
      "2847/2847 [==============================] - 52s 18ms/step - loss: 19.6398 - mae: 17.4769 - val_loss: 14.4305 - val_mae: 12.1942 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "2846/2847 [============================>.] - ETA: 0s - loss: 16.0945 - mae: 13.8629\n",
      "Epoch 3: val_mae improved from 12.19423 to 10.25252, saving model to ./drive/My Drive/train_ckpt/semiconductor\\exp1\n",
      "2847/2847 [==============================] - 54s 19ms/step - loss: 16.0940 - mae: 13.8624 - val_loss: 12.4853 - val_mae: 10.2525 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "2848/2847 [==============================] - ETA: 0s - loss: 14.9394 - mae: 12.7121\n",
      "Epoch 4: val_mae improved from 10.25252 to 8.88231, saving model to ./drive/My Drive/train_ckpt/semiconductor\\exp1\n",
      "2847/2847 [==============================] - 53s 19ms/step - loss: 14.9394 - mae: 12.7121 - val_loss: 11.0949 - val_mae: 8.8823 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "2848/2847 [==============================] - ETA: 0s - loss: 14.2690 - mae: 12.0724\n",
      "Epoch 5: val_mae did not improve from 8.88231\n",
      "2847/2847 [==============================] - 54s 19ms/step - loss: 14.2690 - mae: 12.0724 - val_loss: 11.4102 - val_mae: 9.2413 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "2848/2847 [==============================] - ETA: 0s - loss: 13.7704 - mae: 11.6243\n",
      "Epoch 6: val_mae improved from 8.88231 to 7.15299, saving model to ./drive/My Drive/train_ckpt/semiconductor\\exp1\n",
      "2847/2847 [==============================] - 52s 18ms/step - loss: 13.7704 - mae: 11.6243 - val_loss: 9.2696 - val_mae: 7.1530 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "2848/2847 [==============================] - ETA: 0s - loss: 13.4557 - mae: 11.3560\n",
      "Epoch 7: val_mae did not improve from 7.15299\n",
      "2847/2847 [==============================] - 53s 19ms/step - loss: 13.4557 - mae: 11.3560 - val_loss: 10.6924 - val_mae: 8.6102 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "1707/2847 [================>.............] - ETA: 19s - loss: 13.2446 - mae: 11.1715"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Hcx7cdQgLWau",
    "is_executing": true
   },
   "source": [
    "# np.save(\"file/path.npy\", history.numpy())\n",
    "# history = np.load(\"histoy.npy\", allow_pickle=True)\n",
    "\n",
    "loss=history.history['mae'] # mse\n",
    "val_loss=history.history['val_mae'] # val_mse\n",
    "\n",
    "epochs_range = range(len(loss))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(epochs_range, loss, label='Training MAE') # MSE\n",
    "plt.plot(epochs_range, val_loss, label='Validation MAE') # MSE\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation MAE') # MSE\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIPmAVP1u9nX"
   },
   "source": [
    "### 모델 평가\n",
    "* Mean absolute error 를 이용해 모델이 정확히 예측하는지를 확인"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.load_weights(checkpoint_dir)"
   ],
   "metadata": {
    "id": "th1Wg9JEayQ5",
    "is_executing": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VqsdquFJuKUv",
    "is_executing": true
   },
   "source": [
    "# model.load_weights(checkpoint_dir)\n",
    "results = model.evaluate(test_dataset)\n",
    "print(\"MAE :\", results[0]) # MAE"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1mWFZphNupHX",
    "is_executing": true
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}

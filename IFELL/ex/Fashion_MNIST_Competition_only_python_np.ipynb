{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1WahOek584i"
   },
   "source": [
    "#Step 0 - Install Cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "g09AZuzY58D7",
    "outputId": "cf8e0927-fcc1-44a4-91f4-cea80d7a00d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libcusparse8.0 is already the newest version (8.0.61-1).\n",
      "libnvrtc8.0 is already the newest version (8.0.61-1).\n",
      "libnvtoolsext1 is already the newest version (8.0.61-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
      "Requirement already satisfied: cupy-cuda80 in /usr/local/lib/python3.6/dist-packages (4.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda80) (1.14.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda80) (1.11.0)\n",
      "Requirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda80) (0.4)\n"
     ]
    }
   ],
   "source": [
    "!apt -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1\n",
    "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
    "!pip install cupy-cuda80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HefS85hHS0rW"
   },
   "source": [
    "# Step 1 - import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uXQlGlNFN9Q-"
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pickle\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "#import keras\n",
    "\n",
    "#print(tf.__version__)\n",
    "#print(keras.__version__)\n",
    "\n",
    "cp.random.seed(10)\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oqMlQKdmS5If"
   },
   "source": [
    "# Step 2 - define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qZ4jjk_8Os3y"
   },
   "outputs": [],
   "source": [
    "cp.random.seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "# define def\n",
    "def identify_function(x):\n",
    "    return x\n",
    "\n",
    "def step_function(x):\n",
    "    return cp.array(x > 0, dtype=cp.int)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + cp.exp(-x))\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "\n",
    "def relu(x):\n",
    "    return cp.maximum(0, x)\n",
    "\n",
    "def relu_grad(x):\n",
    "    grad = cp.zeros(x)\n",
    "    grad[x>=0] = 1\n",
    "    return grad\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - cp.max(x, axis=0)\n",
    "        y = cp.exp(x) / cp.sum(cp.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - cp.max(x) # 오버플로 대책\n",
    "    return cp.exp(x) / cp.sum(cp.exp(x))\n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * cp.sum((y-t)**2)\n",
    "\n",
    "# def cross_entropy_error(y, t):\n",
    "#     delta = 1e-7\n",
    "#     return -1 * cp.sum(t * cp.log(y + delta))\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -cp.sum(cp.log(y[cp.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "    \n",
    "def softmax_loss(x,t):\n",
    "    y = softmax(x)\n",
    "    return cross_entropy_error(y, t)\n",
    "\n",
    "def im2col(icput_data, filter_h, filter_w, stride=1, pad=0):\n",
    "\n",
    "    N, C, H, W = icput_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = cp.pad(icput_data, [(0,0), (0,0), (pad, pad), (pad,pad)], 'constant')\n",
    "    col = cp.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "def col2im(col, icput_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    N, C, H, W = icput_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride+1\n",
    "    out_w = (W + 2*pad - filter_w)//stride+1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0,3,4,5,1,2)\n",
    "\n",
    "    img = cp.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride -1))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] = col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = cp.zeros_like(x)\n",
    "\n",
    "    it = cp.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "\n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lRu1pxJWS8V7"
   },
   "source": [
    "# Step 3 - define optimizer\n",
    "adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1CtgecYGO1_V"
   },
   "outputs": [],
   "source": [
    "cp.random.seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "# coding: utf-8\n",
    "import numpy as np\n",
    "\n",
    "class SGD:\n",
    "\n",
    "    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key] \n",
    "\n",
    "\n",
    "class Momentum:\n",
    "\n",
    "    \"\"\"모멘텀 SGD\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = cp.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]\n",
    "\n",
    "\n",
    "class Nesterov:\n",
    "\n",
    "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
    "    # NAG는 모멘텀에서 한 단계 발전한 방법이다. (http://newsight.tistory.com/224)\n",
    "    \n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = cp.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.v[key] *= self.momentum\n",
    "            self.v[key] -= self.lr * grads[key]\n",
    "            params[key] += self.momentum * self.momentum * self.v[key]\n",
    "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "\n",
    "    \"\"\"AdaGrad\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = cp.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (cp.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "\n",
    "    \"\"\"RMSprop\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = cp.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (cp.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = cp.zeros_like(val)\n",
    "                self.v[key] = cp.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * cp.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (cp.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (cp.sqrt(unbisa_b) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gg35Xeq_Thpj"
   },
   "source": [
    "# Step 4 - define trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovFL_nOlO6Z9"
   },
   "outputs": [],
   "source": [
    "cp.random.seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"신경망 훈련을 대신 해주는 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
    "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
    "        self.network = network\n",
    "        self.verbose = verbose\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = mini_batch_size\n",
    "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
    "\n",
    "        # optimzer\n",
    "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
    "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
    "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
    "        \n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
    "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "        \n",
    "    def train_step(self):\n",
    "        batch_mask = cp.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "        \n",
    "        grads = self.network.gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.network.params, grads)\n",
    "        \n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "#         if self.verbose: print(\"train loss:\" + str(loss))\n",
    "        \n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
    "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
    "            if not self.evaluate_sample_num_per_epoch is None:\n",
    "                t = self.evaluate_sample_num_per_epoch\n",
    "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
    "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
    "                \n",
    "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
    "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "\n",
    "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "an2VvBQLTmsv"
   },
   "source": [
    "# step 5 - define layer\n",
    "relu, convolution, pooling, affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rESey8s6PCJC"
   },
   "outputs": [],
   "source": [
    "cp.random.seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "# define layer\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.original_x_shape = x.shape # 텐서 대책이란?\n",
    "        x = x.reshape(x.shape[0], -1) # reshape에서 -1의 의미는?\n",
    "        self.x = x\n",
    "\n",
    "        out = cp.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = cp.dot(dout, self.W.T)\n",
    "        self.dW = cp.dot(self.x.T, dout)\n",
    "        self.db = cp.sum(dout, axis=0)\n",
    "\n",
    "        dx = dx.reshape(*self.original_x_shape) #입력 데이터 모양 변경 이유? 텐서 대응 how?\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size:\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[cp.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        # for backward\n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = cp.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = cp.sum(dout, axis=0)\n",
    "        self.dW = cp.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = cp.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = cp.argmax(col, axis=1)\n",
    "        out = cp.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "\n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = cp.zeros((dout.size, pool_size))\n",
    "        dmax[cp.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
    "\n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "      \n",
    "class Dropout:\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1207.0580\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = cp.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "      \n",
    "\n",
    "\n",
    "class BatchNormalization:\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1502.03167\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원  \n",
    "\n",
    "        # 시험할 때 사용할 평균과 분산\n",
    "        self.running_mean = running_mean\n",
    "        self.running_var = running_var  \n",
    "        \n",
    "        # backward 시에 사용할 중간 데이터\n",
    "        self.batch_size = None\n",
    "        self.xc = None\n",
    "        self.std = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        self.input_shape = x.shape\n",
    "        if x.ndim != 2:\n",
    "            N, C, H, W = x.shape\n",
    "            x = x.reshape(N, -1)\n",
    "\n",
    "        out = self.__forward(x, train_flg)\n",
    "        \n",
    "        return out.reshape(*self.input_shape)\n",
    "            \n",
    "    def __forward(self, x, train_flg):\n",
    "        if self.running_mean is None:\n",
    "            N, D = x.shape\n",
    "            self.running_mean = cp.zeros(D)\n",
    "            self.running_var = cp.zeros(D)\n",
    "                        \n",
    "        if train_flg:\n",
    "            mu = x.mean(axis=0)\n",
    "            xc = x - mu\n",
    "            var = cp.mean(xc**2, axis=0)\n",
    "            std = cp.sqrt(var + 10e-7)\n",
    "            xn = xc / std\n",
    "            \n",
    "            self.batch_size = x.shape[0]\n",
    "            self.xc = xc\n",
    "            self.xn = xn\n",
    "            self.std = std\n",
    "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
    "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
    "        else:\n",
    "            xc = x - self.running_mean\n",
    "            xn = xc / ((cp.sqrt(self.running_var + 10e-7)))\n",
    "            \n",
    "        out = self.gamma * xn + self.beta \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if dout.ndim != 2:\n",
    "            N, C, H, W = dout.shape\n",
    "            dout = dout.reshape(N, -1)\n",
    "\n",
    "        dx = self.__backward(dout)\n",
    "\n",
    "        dx = dx.reshape(*self.input_shape)\n",
    "        return dx\n",
    "\n",
    "    def __backward(self, dout):\n",
    "        dbeta = dout.sum(axis=0)\n",
    "        dgamma = cp.sum(self.xn * dout, axis=0)\n",
    "        dxn = self.gamma * dout\n",
    "        dxc = dxn / self.std\n",
    "        dstd = -cp.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
    "        dvar = 0.5 * dstd / self.std\n",
    "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
    "        dmu = cp.sum(dxc, axis=0)\n",
    "        dx = dxc - dmu / self.batch_size\n",
    "        \n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rt_8gRBcS_hF"
   },
   "source": [
    "# Step 6 - define Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_8tQbHbwPHfI"
   },
   "outputs": [],
   "source": [
    "cp.random.seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\"정확도 99% 이상의 고정밀 합성곱 신경망\n",
    "\n",
    "    네트워크 구성은 아래와 같음\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':128, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':128, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':256, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':256, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':512, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=128, output_size=10):\n",
    "        # 가중치 초기화===========\n",
    "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\n",
    "        pre_node_nums = cp.array([1*3*3, 64*3*3, 128*3*3, 128*3*3, 256*3*3, 256*3*3, 512*3*3, hidden_size])\n",
    "        wight_init_scales = cp.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초깃값\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * cp.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = cp.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * cp.random.randn(512*3*3, hidden_size)\n",
    "        self.params['b7'] = cp.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * cp.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = cp.zeros(output_size)\n",
    "        \n",
    "        # 계층 생성===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad'])) #0\n",
    "        self.layers.append(BatchNormalization(1.0, 0.0))\n",
    "        self.layers.append(Relu())        \n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad'])) #3\n",
    "        self.layers.append(BatchNormalization(1.0, 0.0))\n",
    "        self.layers.append(Relu())        \n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad'])) #7\n",
    "        self.layers.append(BatchNormalization(1.0, 0.0))\n",
    "        self.layers.append(Relu())        \n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad'])) #10\n",
    "        self.layers.append(BatchNormalization(1.0, 0.0))\n",
    "        self.layers.append(Relu())    \n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        \n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad'])) #14\n",
    "        self.layers.append(BatchNormalization(1.0, 0.0))\n",
    "        self.layers.append(Relu())      \n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad'])) #17\n",
    "        self.layers.append(BatchNormalization(1.0, 0.0))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "                \n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7'])) #21\n",
    "        self.layers.append(BatchNormalization(1.0, 0.0))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.4))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8'])) #25\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        # 0, 3, 7, 10, 14, 17, 21, 25\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = cp.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = cp.argmax(y, axis=1)\n",
    "            acc += cp.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 3, 7, 10, 14, 17, 21, 25)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 3, 7, 10, 14, 17, 21, 25)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8T4cJZqXTKsv"
   },
   "source": [
    "# Step 7 - execute code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2219
    },
    "colab_type": "code",
    "id": "4lMbW7VjgHsL",
    "outputId": "76a5f9a9-abb2-45ab-8562-50e9fa9b8328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== epoch:1, train acc:0.137, test acc:0.156 ===\n",
      "=== epoch:2, train acc:0.857, test acc:0.839 ===\n",
      "=== epoch:3, train acc:0.896, test acc:0.86 ===\n",
      "=== epoch:4, train acc:0.895, test acc:0.869 ===\n",
      "=== epoch:5, train acc:0.909, test acc:0.885 ===\n",
      "=== epoch:6, train acc:0.906, test acc:0.888 ===\n",
      "=== epoch:7, train acc:0.922, test acc:0.889 ===\n",
      "=== epoch:8, train acc:0.921, test acc:0.89 ===\n",
      "=== epoch:9, train acc:0.927, test acc:0.892 ===\n",
      "=== epoch:10, train acc:0.929, test acc:0.905 ===\n",
      "=== epoch:11, train acc:0.942, test acc:0.898 ===\n",
      "=== epoch:12, train acc:0.935, test acc:0.898 ===\n",
      "=== epoch:13, train acc:0.945, test acc:0.895 ===\n",
      "=== epoch:14, train acc:0.938, test acc:0.894 ===\n",
      "=== epoch:15, train acc:0.949, test acc:0.907 ===\n",
      "=== epoch:16, train acc:0.956, test acc:0.908 ===\n",
      "=== epoch:17, train acc:0.952, test acc:0.907 ===\n",
      "=== epoch:18, train acc:0.954, test acc:0.907 ===\n",
      "=== epoch:19, train acc:0.953, test acc:0.908 ===\n",
      "=== epoch:20, train acc:0.958, test acc:0.917 ===\n",
      "=== epoch:21, train acc:0.965, test acc:0.905 ===\n",
      "=== epoch:22, train acc:0.971, test acc:0.922 ===\n",
      "=== epoch:23, train acc:0.968, test acc:0.921 ===\n",
      "=== epoch:24, train acc:0.97, test acc:0.91 ===\n",
      "=== epoch:25, train acc:0.968, test acc:0.913 ===\n",
      "=== epoch:26, train acc:0.981, test acc:0.918 ===\n",
      "=== epoch:27, train acc:0.97, test acc:0.917 ===\n",
      "=== epoch:28, train acc:0.979, test acc:0.916 ===\n",
      "=== epoch:29, train acc:0.983, test acc:0.907 ===\n",
      "=== epoch:30, train acc:0.982, test acc:0.91 ===\n",
      "=== epoch:31, train acc:0.976, test acc:0.919 ===\n",
      "=== epoch:32, train acc:0.987, test acc:0.907 ===\n",
      "=== epoch:33, train acc:0.976, test acc:0.907 ===\n",
      "=== epoch:34, train acc:0.979, test acc:0.915 ===\n",
      "=== epoch:35, train acc:0.98, test acc:0.906 ===\n",
      "=== epoch:36, train acc:0.983, test acc:0.904 ===\n",
      "=== epoch:37, train acc:0.98, test acc:0.915 ===\n",
      "=== epoch:38, train acc:0.981, test acc:0.91 ===\n",
      "=== epoch:39, train acc:0.983, test acc:0.907 ===\n",
      "=== epoch:40, train acc:0.984, test acc:0.922 ===\n",
      "=== epoch:41, train acc:0.988, test acc:0.914 ===\n",
      "=== epoch:42, train acc:0.993, test acc:0.92 ===\n",
      "=== epoch:43, train acc:0.989, test acc:0.907 ===\n",
      "=== epoch:44, train acc:0.993, test acc:0.92 ===\n",
      "=== epoch:45, train acc:0.994, test acc:0.912 ===\n",
      "=== epoch:46, train acc:0.993, test acc:0.916 ===\n",
      "=== epoch:47, train acc:0.994, test acc:0.913 ===\n",
      "=== epoch:48, train acc:0.993, test acc:0.916 ===\n",
      "=== epoch:49, train acc:0.99, test acc:0.922 ===\n",
      "=== epoch:50, train acc:0.99, test acc:0.917 ===\n",
      "=== epoch:51, train acc:0.989, test acc:0.911 ===\n",
      "=== epoch:52, train acc:0.99, test acc:0.915 ===\n",
      "=== epoch:53, train acc:0.989, test acc:0.917 ===\n",
      "=== epoch:54, train acc:0.991, test acc:0.919 ===\n",
      "=== epoch:55, train acc:0.991, test acc:0.907 ===\n",
      "=== epoch:56, train acc:0.994, test acc:0.905 ===\n",
      "=== epoch:57, train acc:0.992, test acc:0.915 ===\n",
      "=== epoch:58, train acc:0.996, test acc:0.918 ===\n",
      "=== epoch:59, train acc:0.992, test acc:0.909 ===\n",
      "=== epoch:60, train acc:0.993, test acc:0.915 ===\n",
      "=== epoch:61, train acc:0.998, test acc:0.912 ===\n",
      "=== epoch:62, train acc:0.992, test acc:0.921 ===\n",
      "=== epoch:63, train acc:0.994, test acc:0.907 ===\n",
      "=== epoch:64, train acc:0.997, test acc:0.913 ===\n",
      "=== epoch:65, train acc:0.995, test acc:0.915 ===\n",
      "=== epoch:66, train acc:0.995, test acc:0.91 ===\n",
      "=== epoch:67, train acc:0.992, test acc:0.91 ===\n",
      "=== epoch:68, train acc:0.997, test acc:0.916 ===\n",
      "=== epoch:69, train acc:0.998, test acc:0.904 ===\n",
      "=== epoch:70, train acc:1.0, test acc:0.911 ===\n",
      "=== epoch:71, train acc:0.998, test acc:0.914 ===\n",
      "=== epoch:72, train acc:0.997, test acc:0.909 ===\n",
      "=== epoch:73, train acc:0.992, test acc:0.897 ===\n",
      "=== epoch:74, train acc:0.995, test acc:0.911 ===\n",
      "=== epoch:75, train acc:0.99, test acc:0.907 ===\n",
      "=== epoch:76, train acc:0.989, test acc:0.91 ===\n",
      "=== epoch:77, train acc:0.997, test acc:0.905 ===\n",
      "=== epoch:78, train acc:0.995, test acc:0.914 ===\n",
      "=== epoch:79, train acc:0.998, test acc:0.918 ===\n",
      "=== epoch:80, train acc:0.999, test acc:0.92 ===\n",
      "=== epoch:81, train acc:0.996, test acc:0.92 ===\n",
      "=== epoch:82, train acc:0.995, test acc:0.921 ===\n",
      "=== epoch:83, train acc:0.994, test acc:0.917 ===\n",
      "=== epoch:84, train acc:0.996, test acc:0.919 ===\n",
      "=== epoch:85, train acc:0.998, test acc:0.923 ===\n",
      "=== epoch:86, train acc:0.993, test acc:0.929 ===\n",
      "=== epoch:87, train acc:0.994, test acc:0.922 ===\n",
      "=== epoch:88, train acc:0.996, test acc:0.916 ===\n",
      "=== epoch:89, train acc:0.997, test acc:0.921 ===\n",
      "=== epoch:90, train acc:0.998, test acc:0.911 ===\n",
      "=== epoch:91, train acc:0.995, test acc:0.921 ===\n",
      "=== epoch:92, train acc:0.992, test acc:0.923 ===\n",
      "=== epoch:93, train acc:0.99, test acc:0.903 ===\n",
      "=== epoch:94, train acc:0.995, test acc:0.912 ===\n",
      "=== epoch:95, train acc:0.996, test acc:0.906 ===\n",
      "=== epoch:96, train acc:0.993, test acc:0.911 ===\n",
      "=== epoch:97, train acc:0.997, test acc:0.905 ===\n",
      "=== epoch:98, train acc:0.998, test acc:0.91 ===\n",
      "=== epoch:99, train acc:0.998, test acc:0.917 ===\n",
      "=== epoch:100, train acc:0.997, test acc:0.924 ===\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.908625\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFcCAYAAADh1zYWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3268dd1a20>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cp.random.seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "x_train=cp.expand_dims(x_train,axis=1)\n",
    "x_test=cp.expand_dims(x_test,axis=1)\n",
    "\n",
    "N = 30000\n",
    "\n",
    "x_test = np.concatenate((x_test, x_train[N:]), axis=0)\n",
    "t_test = np.concatenate((t_test, t_train[N:]), axis=0)\n",
    "\n",
    "x_train = x_train[:N]\n",
    "t_train = t_train[:N]\n",
    "\n",
    "x_train.astype('float32') / 255\n",
    "x_test.astype('float32') / 255\n",
    "\n",
    "x_train.astype('float32') - 0.5\n",
    "x_test.astype('float32') - 0.5\n",
    "\n",
    "x_train.astype('float32') * 2\n",
    "x_test.astype('float32') * 2\n",
    "\n",
    "# cp 변환\n",
    "x_train = cp.array(x_train)\n",
    "x_test = cp.array(x_test)\n",
    "t_train = cp.array(t_train)\n",
    "t_test = cp.array(t_test)\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.0001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보관\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Jlp54Vb4nun"
   },
   "source": [
    "#test acc:0.908625\n",
    "\n",
    "## Tensorflow 1.10.1\n",
    "## Keras 2.1.6 Using TensorFlow backend.\n",
    "\n",
    "###Random seed 10\n",
    "###100 epoch\n",
    "\n",
    "#Accuracy Graph\n",
    "\n",
    "![Result](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAe8AAAFcCAYAAADh1zYWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Fashion_MNIST_Competition_Junseop So.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

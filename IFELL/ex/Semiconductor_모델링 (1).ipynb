{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THNjjG50BYJY"
   },
   "source": [
    "# 반도체 박막 두께 분석\n",
    "\n",
    "### 배경\n",
    "\n",
    "최근 고사양 반도체 수요가 많아지면서 반도체를 수직으로 적층하는 3차원 공정이 많이 연구되고 있습니다. 반도체 박막을 수십 ~ 수백 층 쌓아 올리는 공정에서는 박막의 결함으로 인한 두께와 균일도가 저하되는 문제가 있습니다. 이는 소자 구조의 변형을 야기하며 성능 하락의 주요 요인이 됩니다. 이를 사전에 방지하기 위해서는 박막의 두께를 빠르면서도 정확히 측정하는 것이 중요합니다.\n",
    "\n",
    "박막의 두께를 측정하기 위해 광스펙트럼 분석이 널리 사용되고 있습니다. 하지만 광 스펙트럼을 분석하기 위해서는 관련 지식을 많이 가진 전문가가 필요하며 분석과정에 많은 컴퓨팅자원이 필요합니다. 빅데이터 분석을 통해 이를 해결하고자 반도체 소자의 두께 분석 알고리즘 경진대회를 개최합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XN-kkqUGNko"
   },
   "source": [
    "### 배경 자료\n",
    "\n",
    "반도체 박막은 얇은 반도체 막으로 박막의 종류와 두께는 반도체 소자의 특성을 결정짓는 중요한 요소 중 하나입니다. 박막의 두께를 측정하는 방법으로 반사율 측정이 널리 사용되며 반사율은 입사광 세기에 대한 반사광 세기의 비율로 정해집니다. (반사율 = 반사광/입사광) 반사율은 빛의 파장에 따라 변하며 파장에 따른 반사율의 분포를 반사율 스펙트럼이라고 합니다.\n",
    "\n",
    "\n",
    "\n",
    "### 구조 설명\n",
    "\n",
    "이번 대회에서 분석할 소자는 질화규소(layer_1)/이산화규소(layer_2)/질화규소(layer_3)/이산화규소(layer_4)/규소(기판) 총 5층 구조로 되어 있습니다. 대회의 목적은 기판인 규소를 제외한 layer_1 ~ layer_4의 두께를 예측하는 것으로 학습 데이터 파일에는 각 층의 두께와 반사율 스펙트럼이 포함되어 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "### 데이터 설명\n",
    "\n",
    "train.csv 파일에는 4층 박막의 두께와 파장에 따른 반사율 스펙트럼이 주어집니다. 헤더의 이름에 따라 layer_1 ~ 4는 해당 박막의 두께,\n",
    "\n",
    "데이터값인 0 ~ 225은 빛의 파장에 해당하는 반사율이 됩니다. 헤더 이름인 0~225은 파장을 뜻하며 비식별화 처리가 되어있어 실제 값과는 다릅니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VHebfq47CZ-V",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:50:56.472551300Z",
     "start_time": "2024-01-09T21:50:53.115433200Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.layers import Dense, Conv2D, SeparableConv2D, Flatten, Dropout, MaxPooling2D, \\\n",
    "    BatchNormalization, ReLU, GlobalAveragePooling2D\n",
    "from tensorflow.keras import regularizers"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ATOrWJKuLlBT",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:50:56.484599200Z",
     "start_time": "2024-01-09T21:50:56.465550600Z"
    }
   },
   "source": [
    "use_colab = True\n",
    "assert use_colab in [True, False]"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lsFNOIeSn_Z9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "bebbdf4c-bf60-4630-eab6-4dfe0dad0ede",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:50:56.501120700Z",
     "start_time": "2024-01-09T21:50:56.480598900Z"
    }
   },
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qoXy0bqBqKt"
   },
   "source": [
    "### 데이터 로드\n",
    "* 새로운 버전의 Colab파일에선 왼쪽의 폴더 tree에서 직접 드라이브 마운트를 진행해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kwp9HSrrD4XW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2425665e-6b38-4784-99a2-d8575790eb34",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:50:56.860949900Z",
     "start_time": "2024-01-09T21:50:56.498121300Z"
    }
   },
   "source": [
    "train_d = np.load(\"./data/semicon_train_data.npy\")\n",
    "test_d = np.load(\"./data/semicon_test_data.npy\")\n",
    "\n",
    "# train data path\n",
    "# train_d = np.load(\"./semicon_train_data.npy\")\n",
    "\n",
    "# test data path\n",
    "# train_d = np.load(\"./semicon_test_data.npy\")\n",
    "print(train_d.shape, test_d.shape)"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(729000, 230) (81000, 230)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EHTnjvhZi4x9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cf4480b3-3ed3-4361-edb0-eba26682b9ac",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:50:56.880948200Z",
     "start_time": "2024-01-09T21:50:56.862948300Z"
    }
   },
   "source": [
    "print(train_d[1].shape)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OsF_kurljuwU",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9a0217b4-c68c-4e4a-f675-160a0a027f78",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:50:56.929323700Z",
     "start_time": "2024-01-09T21:50:56.878949300Z"
    }
   },
   "source": [
    "print(test_d[0].shape)"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OdXgDw8oyTQf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9863b335-026d-4906-df87-ac237f28ec35",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:50:56.948329400Z",
     "start_time": "2024-01-09T21:50:56.895949100Z"
    }
   },
   "source": [
    "# Raw Data 확인\n",
    "print(train_d[1000])\n",
    "# print(test_d[0])"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.00000000e+01 4.00000000e+01 1.50000000e+02 1.10000000e+02\n",
      " 4.02068880e-01 4.05173400e-01 4.22107550e-01 4.35197980e-01\n",
      " 4.66261860e-01 4.67785980e-01 4.76300870e-01 5.07203700e-01\n",
      " 5.23296600e-01 5.31541170e-01 5.23319200e-01 5.32340650e-01\n",
      " 5.38513840e-01 5.62478700e-01 5.74338900e-01 5.62083540e-01\n",
      " 5.83467800e-01 5.88462350e-01 5.89606940e-01 5.93186560e-01\n",
      " 5.93241930e-01 5.98241150e-01 5.87171000e-01 5.92055500e-01\n",
      " 6.06527860e-01 5.87087870e-01 5.94492100e-01 5.73233500e-01\n",
      " 5.94926300e-01 5.76777600e-01 5.70286300e-01 5.76913000e-01\n",
      " 5.52754400e-01 5.61735150e-01 5.52500000e-01 5.23800200e-01\n",
      " 5.11056840e-01 5.11732600e-01 4.92525460e-01 4.73373200e-01\n",
      " 4.57444160e-01 4.53428800e-01 4.19270130e-01 3.87709920e-01\n",
      " 3.87221520e-01 3.49848450e-01 3.16229370e-01 3.13858700e-01\n",
      " 2.80085700e-01 2.61263000e-01 2.30250820e-01 1.98457850e-01\n",
      " 1.53500440e-01 1.36026740e-01 1.08726785e-01 8.95344000e-02\n",
      " 6.58975300e-02 5.09539300e-02 3.69604570e-02 2.88708470e-02\n",
      " 1.54857090e-02 1.99594010e-02 2.33170990e-02 3.26487760e-02\n",
      " 5.89796340e-02 6.33707600e-02 9.20634640e-02 1.24182590e-01\n",
      " 1.63985540e-01 1.81501220e-01 2.16830660e-01 2.57668080e-01\n",
      " 2.86428720e-01 3.21613600e-01 3.65077380e-01 3.85984180e-01\n",
      " 4.33254400e-01 4.36655340e-01 4.60312520e-01 4.99771740e-01\n",
      " 5.03849100e-01 5.48682200e-01 5.58029060e-01 5.62947630e-01\n",
      " 5.99234340e-01 5.96399550e-01 6.04513000e-01 6.19093400e-01\n",
      " 6.38402700e-01 6.64816900e-01 6.56996000e-01 6.66098600e-01\n",
      " 6.69130500e-01 6.70661600e-01 6.80202360e-01 6.92821600e-01\n",
      " 6.96852400e-01 7.02015160e-01 7.00507460e-01 7.00250800e-01\n",
      " 7.11669300e-01 6.96838260e-01 7.08974360e-01 7.19114100e-01\n",
      " 7.18034700e-01 7.00589540e-01 7.05873250e-01 6.92893500e-01\n",
      " 7.14911160e-01 6.85250640e-01 6.82136830e-01 6.81311400e-01\n",
      " 6.78019200e-01 6.67259900e-01 6.78185700e-01 6.52875660e-01\n",
      " 6.34932940e-01 6.36031400e-01 6.16502100e-01 5.91461400e-01\n",
      " 5.74673350e-01 5.57246200e-01 5.42328200e-01 5.19422200e-01\n",
      " 5.03134850e-01 4.81939820e-01 4.45872100e-01 4.17206170e-01\n",
      " 4.05306970e-01 3.55970500e-01 3.27808440e-01 2.89738060e-01\n",
      " 2.44837760e-01 2.16494500e-01 1.69062380e-01 1.23260050e-01\n",
      " 1.03297760e-01 6.77379500e-02 3.89807700e-02 2.52380610e-02\n",
      " 2.59922780e-02 1.38827340e-02 1.90851740e-02 6.68724550e-02\n",
      " 7.21453900e-02 1.14929430e-01 1.77696590e-01 2.11153720e-01\n",
      " 2.55038900e-01 2.95599580e-01 3.21243700e-01 3.74151170e-01\n",
      " 3.91486200e-01 4.27665320e-01 4.61769700e-01 4.75576340e-01\n",
      " 4.78055980e-01 5.16503040e-01 5.30934450e-01 5.23902500e-01\n",
      " 5.51259300e-01 5.65235300e-01 5.67701600e-01 5.70858800e-01\n",
      " 5.76418300e-01 5.75021860e-01 6.01967200e-01 5.79546750e-01\n",
      " 5.81157900e-01 5.93576100e-01 5.84070600e-01 6.04394850e-01\n",
      " 5.87139800e-01 5.93731460e-01 5.91678200e-01 5.63344300e-01\n",
      " 5.69886800e-01 5.46404960e-01 5.43537260e-01 5.42394760e-01\n",
      " 5.42762940e-01 5.26218060e-01 4.89471520e-01 4.72608000e-01\n",
      " 4.59845570e-01 4.39476280e-01 4.24489440e-01 4.16756120e-01\n",
      " 4.11724180e-01 3.81418440e-01 3.60050380e-01 3.37973600e-01\n",
      " 3.20442830e-01 2.97488420e-01 2.85940900e-01 2.82688900e-01\n",
      " 2.78886770e-01 2.60735480e-01 2.64526520e-01 2.38374640e-01\n",
      " 2.46523140e-01 2.54422630e-01 2.63443470e-01 2.77559100e-01\n",
      " 3.05792480e-01 3.04755750e-01 3.47745950e-01 3.42683080e-01\n",
      " 3.59969050e-01 3.90264000e-01 4.21746500e-01 4.33448700e-01\n",
      " 4.50429620e-01 4.94950400e-01 4.90516330e-01 5.15429500e-01\n",
      " 5.46570300e-01 5.64438460e-01 5.68110800e-01 5.71491600e-01\n",
      " 5.81447660e-01 5.84012000e-01 6.05881900e-01 5.98874870e-01\n",
      " 5.97313170e-01 5.95610300e-01]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJTUCIIAt01D"
   },
   "source": [
    "* 데이터를 분석하여 모델에 학습할 수 있는 형태로 정리합니다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "awmkQxq8sWsG",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:50:56.990080300Z",
     "start_time": "2024-01-09T21:50:56.911953100Z"
    }
   },
   "source": [
    "train = train_d[:,4:] # 729000, 230 4번째 데이터부터 가져온다.\n",
    "label = train_d[:,:4]\n",
    "\n",
    "t_train = test_d[:,4:] # 81000, 230개 4번째 까지 데이터를 가져온다.\n",
    "t_label = test_d[:,:4]"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u0Cb4BQPKqVs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b46a3a45-cf2a-4bc6-eb50-33a39875941d",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:51:00.063066Z",
     "start_time": "2024-01-09T21:50:59.050110900Z"
    }
   },
   "source": [
    "batch_size = 256\n",
    "\n",
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train, label))\n",
    "train_dataset = train_dataset.shuffle(10000).repeat().batch(batch_size=batch_size)\n",
    "print(train_dataset)\n",
    "\n",
    "# for test\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((t_train, t_label))\n",
    "test_dataset = test_dataset.batch(batch_size=batch_size)\n",
    "print(test_dataset)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 226), dtype=tf.float64, name=None), TensorSpec(shape=(None, 4), dtype=tf.float64, name=None))>\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, 226), dtype=tf.float64, name=None), TensorSpec(shape=(None, 4), dtype=tf.float64, name=None))>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsJ2qMl3Bryb"
   },
   "source": [
    "### 모델 구성\n",
    "* 현재 가장 간단한 모델로 구성되어 있습니다.\n",
    "* 데이터셋 자체를 가장 잘 학습할 수 있는 모델을 구현해 학습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.keras import layers, regularizers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# 클래스버젼 - Seperable Conv2D\n",
    "class ResBlock_without_RNN(tf.keras.Model):\n",
    "    def __init__(self, num_filter, stride=1, kernel_size=3, l2_reg=1e-4, dim = None):\n",
    "        super(ResBlock_without_RNN, self).__init__()\n",
    "        self.num_filter = num_filter\n",
    "        self.dim = dim\n",
    "        # print(self.dim,self.num_filter)\n",
    "\n",
    "        self.conv1 = layers.Conv2D(num_filter, kernel_size, strides=stride, padding='same',\n",
    "                                   kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                                   kernel_regularizer=regularizers.l2(0.001))\n",
    "        self.se_conv1 = layers.SeparableConv2D(num_filter, kernel_size, padding='same',\n",
    "                                               depthwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               pointwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               depthwise_regularizer=regularizers.l2(0.001),\n",
    "                                               pointwise_regularizer=regularizers.l2(0.001))\n",
    "        self.se_sonv2 = layers.SeparableConv2D(num_filter, kernel_size, padding='same',\n",
    "                                               depthwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               pointwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               depthwise_regularizer=regularizers.l2(0.001),\n",
    "                                               pointwise_regularizer=regularizers.l2(0.001))\n",
    "        self.se_conv1d_1 = layers.SeparableConv1D(num_filter, kernel_size, padding='same',\n",
    "                                                  depthwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                                  pointwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                                  depthwise_regularizer=regularizers.l2(0.001),\n",
    "                                                  pointwise_regularizer=regularizers.l2(0.001))\n",
    "        self.se_sonv1d_2 = layers.SeparableConv1D(num_filter, kernel_size, padding='same',\n",
    "                                              depthwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                              pointwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                              depthwise_regularizer=regularizers.l2(0.001),\n",
    "                                              pointwise_regularizer=regularizers.l2(0.001))\n",
    "        self.con_short = layers.Conv2D(num_filter, 1, strides=2, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.bn_short = layers.BatchNormalization()\n",
    "        # self.LSTM = tf.keras.layers.LSTM(dim*dim,return_sequences=True,recurrent_initializer='glorot_uniform')\n",
    "        # self.LSTM2 = tf.keras.layers.LSTM(dim*dim,return_sequences=True,recurrent_initializer='glorot_uniform')\n",
    "        # self.LSTM3 = tf.keras.layers.LSTM(dim*dim)\n",
    "        # # self.con1d_1 = layers.Conv2D()\n",
    "\n",
    "    def call(self, input, stride=1, dim=None):\n",
    "        self.dim = dim\n",
    "        shortcut = input\n",
    "        x = self.conv1(input)\n",
    "\n",
    "        x = self.bn1(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "\n",
    "\n",
    "        # print(\"변환하기전:\",x.shape, dim, self.num_filter,stride)\n",
    "        # RNN_x = layers.Reshape(target_shape=(-1, dim * dim * self.num_filter))(x) # 컨볼루션을 쓰기위한 변환 (64,4) -> CNN1D또는 RNN 사용가능 256,1도 가능\n",
    "        # # print(\"변환후:\",RNN_x.shape)\n",
    "        # \n",
    "        # RNN_x = self.LSTM(RNN_x)\n",
    "        # RNN_x = self.LSTM2(RNN_x)\n",
    "        # RNN_x = self.LSTM3(RNN_x)\n",
    "        # # print(\"LSTM후:\",RNN_x.shape)\n",
    "        # RNN_x = layers.Reshape(target_shape=(dim,dim,1))(RNN_x)  # RNN_x 리쉐이프\n",
    "        # # print(\"재변환후:\",RNN_x.shape)\n",
    "        # C1D_x = RNN_x\n",
    "\n",
    "\n",
    "        x = self.se_conv1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.se_sonv2(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        # 입력과 출력의 맞도록 숏컷을 조절\n",
    "        if shortcut.shape[-1] != self.num_filter or stride != 1:\n",
    "            shortcut = self.con_short(shortcut)\n",
    "            shortcut = self.bn_short(shortcut)\n",
    "\n",
    "        x = layers.Add()([x, shortcut])\n",
    "        # print(\"add후:\",x.shape)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "class ResBlock(tf.keras.Model):\n",
    "    def __init__(self, num_filter, stride=1, kernel_size=3, l2_reg=1e-4, dim = None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.num_filter = num_filter\n",
    "        self.dim = dim\n",
    "        # print(self.dim,self.num_filter)\n",
    "\n",
    "        self.conv1 = layers.Conv2D(num_filter, kernel_size, strides=stride, padding='same',\n",
    "                                   kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                                   kernel_regularizer=regularizers.l2(0.001))\n",
    "        self.se_conv1 = layers.SeparableConv2D(num_filter, kernel_size, padding='same',\n",
    "                                               depthwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               pointwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               depthwise_regularizer=regularizers.l2(0.001),\n",
    "                                               pointwise_regularizer=regularizers.l2(0.001))\n",
    "        self.se_sonv2 = layers.SeparableConv2D(num_filter, kernel_size, padding='same',\n",
    "                                               depthwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               pointwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               depthwise_regularizer=regularizers.l2(0.001),\n",
    "                                               pointwise_regularizer=regularizers.l2(0.001))\n",
    "        self.se_conv1d_1 = layers.SeparableConv1D(num_filter, kernel_size, padding='same',\n",
    "                                               depthwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               pointwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                               depthwise_regularizer=regularizers.l2(0.001),\n",
    "                                               pointwise_regularizer=regularizers.l2(0.001))\n",
    "        self.se_sonv1d_2 = layers.SeparableConv1D(num_filter, kernel_size, padding='same',\n",
    "                                           depthwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                           pointwise_initializer=tf.keras.initializers.he_normal(),\n",
    "                                           depthwise_regularizer=regularizers.l2(0.001),\n",
    "                                           pointwise_regularizer=regularizers.l2(0.001))\n",
    "        self.con_short = layers.Conv2D(num_filter, 1, strides=2, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.bn_short = layers.BatchNormalization()\n",
    "        self.LSTM = tf.keras.layers.LSTM(dim*dim,return_sequences=True,recurrent_initializer='glorot_uniform')\n",
    "        self.LSTM2 = tf.keras.layers.LSTM(dim*dim,return_sequences=True,recurrent_initializer='glorot_uniform')\n",
    "        self.LSTM3 = tf.keras.layers.LSTM(dim*dim)\n",
    "        # self.con1d_1 = layers.Conv2D()\n",
    "\n",
    "    def call(self, input, stride=1, dim=None):\n",
    "        self.dim = dim\n",
    "        shortcut = input\n",
    "        x = self.conv1(input)\n",
    "\n",
    "        x = self.bn1(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "\n",
    "\n",
    "        # print(\"변환하기전:\",x.shape, dim, self.num_filter,stride)\n",
    "        RNN_x = layers.Reshape(target_shape=(-1, dim * dim * self.num_filter))(x) # 컨볼루션을 쓰기위한 변환 (64,4) -> CNN1D또는 RNN 사용가능 256,1도 가능\n",
    "        # print(\"변환후:\",RNN_x.shape)\n",
    "\n",
    "        RNN_x = self.LSTM(RNN_x)\n",
    "        RNN_x = self.LSTM2(RNN_x)\n",
    "        RNN_x = self.LSTM3(RNN_x)\n",
    "        # print(\"LSTM후:\",RNN_x.shape)\n",
    "        RNN_x = layers.Reshape(target_shape=(dim,dim,1))(RNN_x)  # RNN_x 리쉐이프\n",
    "        # print(\"재변환후:\",RNN_x.shape)\n",
    "        # C1D_x = RNN_x\n",
    "\n",
    "\n",
    "        x = self.se_conv1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.se_sonv2(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        # 입력과 출력의 맞도록 숏컷을 조절\n",
    "        if shortcut.shape[-1] != self.num_filter or stride != 1:\n",
    "            shortcut = self.con_short(shortcut)\n",
    "            shortcut = self.bn_short(shortcut)\n",
    "\n",
    "        x = layers.Add()([x, shortcut,RNN_x])\n",
    "        # print(\"add후:\",x.shape)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        return x\n",
    "\n",
    "class ResNet28(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet28, self).__init__()\n",
    "        self.dim = None  # dim 초기화\n",
    "\n",
    "        self.conv1 = layers.Conv2D(64, 7, strides=2,\n",
    "                                   kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                                   padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.relu1 = layers.ReLU()\n",
    "        self.relu2 = layers.ReLU()\n",
    "\n",
    "        self.blocks = [\n",
    "            ResBlock_without_RNN(64, stride=1, dim = 8),\n",
    "            ResBlock(128, stride=2, dim = 4),\n",
    "            ResBlock(256, stride=2, dim = 2),\n",
    "            ResBlock(256, stride=1, dim = 2),\n",
    "            ResBlock(512, stride=2, dim = 1)\n",
    "        ]\n",
    "\n",
    "        self.avg_pool = layers.GlobalAveragePooling2D()\n",
    "        self.dropout = layers.Dropout(0.5)\n",
    "        self.out = layers.Dense(4)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = layers.Dense(16 * 16 * 1)(inputs)\n",
    "        x = layers.Reshape(target_shape=(16, 16, 1))(x)  # 컨볼루션을 쓰기위한 변환 (64,4) -> CNN1D또는 RNN 사용가능 256,1도 가능\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)  # 8x8*64\n",
    "\n",
    "\n",
    "        x = self.blocks[0](x,dim =8)\n",
    "        x = self.blocks[1](x,dim =4)\n",
    "        x = self.blocks[2](x,dim =2)\n",
    "        x = self.blocks[3](x,dim =2)\n",
    "        x = self.blocks[4](x,dim =1)\n",
    "        x = self.avg_pool(x)\n",
    "        x = layers.Dense(1000)(x)\n",
    "        # x =  x = layers.Reshape(target_shape=(1))(x)\n",
    "        # x = layers.LSTM(64)(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Specify input shape and number of classes\n",
    "input_tensor = layers.Input(shape=(226,))  # Example input shape for image classification\n",
    "\n",
    "# Build ResNet model\n",
    "Res = ResNet28()\n",
    "output_tensor = Res.call(input_tensor)\n",
    "model = tf.keras.Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "# Display model summary\n",
    "model.summary()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j5z-Mwys0HJK",
    "outputId": "3a6f15a4-5d22-464b-dbfa-795debe5c615",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:59:14.405639800Z",
     "start_time": "2024-01-09T21:59:11.577013900Z"
    }
   },
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 226)]             0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 256)               58112     \n",
      "                                                                 \n",
      " reshape_8 (Reshape)         (None, 16, 16, 1)         0         \n",
      "                                                                 \n",
      " conv2d_89 (Conv2D)          (None, 8, 8, 64)          3200      \n",
      "                                                                 \n",
      " batch_normalization_187 (Ba  (None, 8, 8, 64)         256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " re_lu_18 (ReLU)             (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " res_block_without_rnn_5 (Re  (None, 8, 8, 64)         47168     \n",
      " sBlock_without_RNN)                                             \n",
      "                                                                 \n",
      " res_block_35 (ResBlock)     (None, 4, 4, 128)         255936    \n",
      "                                                                 \n",
      " res_block_36 (ResBlock)     (None, 2, 2, 256)         485232    \n",
      "                                                                 \n",
      " res_block_37 (ResBlock)     (None, 2, 2, 256)         746096    \n",
      "                                                                 \n",
      " res_block_38 (ResBlock)     (None, 1, 1, 512)         1856544   \n",
      "                                                                 \n",
      " global_average_pooling2d_8   (None, 512)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 1000)              513000    \n",
      "                                                                 \n",
      " batch_normalization_188 (Ba  (None, 1000)             4000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " re_lu_19 (ReLU)             (None, 1000)              0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 4)                 4004      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,973,548\n",
      "Trainable params: 3,962,332\n",
      "Non-trainable params: 11,216\n",
      "_________________________________________________________________\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "FaxFpBbxyUzv",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:59:14.412640400Z",
     "start_time": "2024-01-09T21:59:14.380912400Z"
    }
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # 수직연산\n",
    "\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# input_tensor = layers.Input(shape=(226,))\n",
    "\n",
    "# # 목표 출력의 형태가 (batch_size, 4)인 경우에 맞춤\n",
    "# # target_shape = (16, 16, 4)\n",
    "\n",
    "# x = layers.Dense(8 * 8 * 4, activation='relu')(input_tensor)\n",
    "\n",
    "# x = layers.Reshape(target_shape=(16, 16, 1))(x)\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "\n",
    "# x = layers.Reshape(target_shape=(-1, 16 * 16 * 64))(x)\n",
    "# x = layers.LSTM(256)(x)\n",
    "# x = layers.Reshape(target_shape=(16, 16, 1))(x)\n",
    "\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "# output_tensor = layers.Dense(4)(x)  # 목표 데이터 형태에 맞게 조정\n",
    "\n",
    "# model = models.Model(input_tensor, output_tensor)\n",
    "# model.summary()\n"
   ],
   "metadata": {
    "id": "llyyseW9Pjtz",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:59:14.447240400Z",
     "start_time": "2024-01-09T21:59:14.397635900Z"
    }
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # 컨벌루션 하고 병렬연산\n",
    "\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# input_tensor = layers.Input(shape=(226,))\n",
    "\n",
    "# # 목표 출력의 형태가 (batch_size, 4)인 경우에 맞춤\n",
    "# # target_shape = (16, 16, 4)\n",
    "\n",
    "# x_input = layers.Dense(8 * 8 * 4, activation='relu')(input_tensor)\n",
    "\n",
    "# x = layers.Reshape(target_shape=(16, 16, 1))(x_input)\n",
    "# x_bi = layers.Conv2D(64, 3, padding='same',strides =2)(x)\n",
    "# # 8 * 8* 64 사이즈가 됨\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x_bi)\n",
    "\n",
    "\n",
    "# # 8* 8*64 사이즈를 계산해봄\n",
    "\n",
    "# x2 = layers.Reshape(target_shape=(-1, 8 * 8*64))(x_bi)\n",
    "# x2 = layers.LSTM(8*8*64)(x2)\n",
    "# x2 = layers.Reshape(target_shape=(8, 8, 64))(x2)\n",
    "\n",
    "# x = layers.add([x,x2])\n",
    "\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "# output_tensor = layers.Dense(4)(x)  # 목표 데이터 형태에 맞게 조정\n",
    "\n",
    "# model = models.Model(input_tensor, output_tensor)\n",
    "# model.summary()\n"
   ],
   "metadata": {
    "id": "mvbX3yprabj1",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:59:14.448239700Z",
     "start_time": "2024-01-09T21:59:14.413638900Z"
    }
   },
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # 컨벌루션 하고 병렬연산  - 작동하지만 사이즈가 너무 커짐\n",
    "\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# input_tensor = layers.Input(shape=(226,))\n",
    "\n",
    "# # 목표 출력의 형태가 (batch_size, 4)인 경우에 맞춤\n",
    "# # target_shape = (16, 16, 4)\n",
    "\n",
    "# x_input = layers.Dense(8 * 8 * 4, activation='relu')(input_tensor)\n",
    "\n",
    "# x = layers.Reshape(target_shape=(16, 16, 1))(x_input)\n",
    "# x_bi = layers.Conv2D(64, 3, padding='same',strides =2)(x)\n",
    "# # 8 * 8* 64 사이즈가 됨\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x_bi)\n",
    "\n",
    "\n",
    "# # 8* 8*64 사이즈를 계산해봄\n",
    "\n",
    "# x2 = layers.Reshape(target_shape=(-1, 8 * 8*64))(x_bi)\n",
    "# x2 = layers.LSTM(8*8*64)(x2)\n",
    "# x2 = layers.Reshape(target_shape=(8, 8, 64))(x2)\n",
    "\n",
    "# x = layers.add([x,x2])\n",
    "\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "# output_tensor = layers.Dense(4)(x)  # 목표 데이터 형태에 맞게 조정\n",
    "\n",
    "# model = models.Model(input_tensor, output_tensor)\n",
    "# model.summary()\n"
   ],
   "metadata": {
    "id": "vqLh5Q0Ygpmi",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:59:14.450240800Z",
     "start_time": "2024-01-09T21:59:14.427178100Z"
    }
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # 컨벌루션 하고 병렬연산\n",
    "\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# input_tensor = layers.Input(shape=(226,))\n",
    "\n",
    "# # 목표 출력의 형태가 (batch_size, 4)인 경우에 맞춤\n",
    "# # target_shape = (16, 16, 4)\n",
    "\n",
    "# x_input = layers.Dense(8 * 8 * 4, activation='relu')(input_tensor)\n",
    "\n",
    "# x = layers.Reshape(target_shape=(16, 16, 1))(x_input)\n",
    "# x_bi = layers.Conv2D(64, 3, padding='same',strides =2)(x)\n",
    "# # 8 * 8* 64 사이즈가 됨\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x_bi)\n",
    "\n",
    "\n",
    "# # 8* 8*64 사이즈를 계산해봄\n",
    "\n",
    "# x2 = layers.Reshape(target_shape=(-1, 8 * 8*64))(x_bi)\n",
    "# x2 = layers.LSTM(8*8*64)(x2)\n",
    "# x2 = layers.Reshape(target_shape=(8, 8, 64))(x2)\n",
    "\n",
    "# x = layers.add([x,x2])\n",
    "\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "# output_tensor = layers.Dense(4)(x)  # 목표 데이터 형태에 맞게 조정\n",
    "\n",
    "# model = models.Model(input_tensor, output_tensor)\n",
    "# model.summary()\n"
   ],
   "metadata": {
    "id": "-IxAf9ROabuK",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:59:14.489362600Z",
     "start_time": "2024-01-09T21:59:14.446241400Z"
    }
   },
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # 컨벌루션 하고 병렬연산  - 사이즈 조절해서 add\n",
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# input_tensor = layers.Input(shape=(226,))\n",
    "\n",
    "# # 목표 출력의 형태가 (batch_size, 4)인 경우에 맞춤\n",
    "# # target_shape = (16, 16, 4)\n",
    "\n",
    "# x_input = layers.Dense(8 * 8 * 4, activation='relu')(input_tensor)\n",
    "\n",
    "# x = layers.Reshape(target_shape=(16, 16, 1))(x_input)\n",
    "# x_bi = layers.Conv2D(64, 3, padding='same',strides =2)(x)\n",
    "# # 8 * 8* 64 사이즈가 됨\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x_bi)\n",
    "\n",
    "\n",
    "# # 8* 8*64 사이즈를 계산해봄\n",
    "\n",
    "# x2 = layers.Reshape(target_shape=(-1, 8 * 8*64))(x_bi)\n",
    "# x2 = layers.LSTM(32, return_sequences=True)(x2)  # 여기서 사이즈를 줄이면 됨\n",
    "# x2 = layers.LSTM(64)(x2) # 여기서 사이즈를 줄이면 됨\n",
    "# x2 = layers.Reshape(target_shape=(8, 8, 1))(x2)\n",
    "\n",
    "# x = layers.add([x,x2])\n",
    "\n",
    "\n",
    "# x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(64, activation='relu')(x)\n",
    "\n",
    "# output_tensor = layers.Dense(4)(x)  # 목표 데이터 형태에 맞게 조정\n",
    "\n",
    "# model = models.Model(input_tensor, output_tensor)\n",
    "# model.summary()\n"
   ],
   "metadata": {
    "id": "KPGQ7dPlJrVf",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:59:14.504361500Z",
     "start_time": "2024-01-09T21:59:14.462240500Z"
    }
   },
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zzo9y_5eryuq",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:59:14.506765300Z",
     "start_time": "2024-01-09T21:59:14.477848900Z"
    }
   },
   "source": [
    "# input_tensor = layers.Input(shape=(226,))\n",
    "# # CNN1D, CNN2D, RNN 세가지를 사용해서 모델링\n",
    "\n",
    "# # 226에서 256으로 확장\n",
    "# x = layers.Dense(8*8*4, activation = 'relu')(input_tensor)\n",
    "\n",
    "# CNN_x = layers.Reshape(target_shape=(8,8,4))(x) # 컨볼루션을 쓰기위한 변환 (64,4) -> CNN1D또는 RNN 사용가능 256,1도 가능\n",
    "# RNN_x = layers.Reshape(target_shape=(256,1))(x)\n",
    "# Dense_x = RNN_x\n",
    "\n",
    "\n",
    "# x = tf.add([x, x_skip]) # batch, 128 + 226\n",
    "\n",
    "\n",
    "# x = layers.Dense(16,\n",
    "#                  kernel_initializer=tf.keras.initializers.HeUniform())(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "# x = layers.Activation('relu')(x)\n",
    "\n",
    "# output_tensor = layers.Dense(4)(x)\n",
    "\n",
    "# model = tf.keras.Model(input_tensor, output_tensor)"
   ],
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0OODMx4Btb5"
   },
   "source": [
    "### 모델 학습\n",
    "* 4개층의 박막의 두께를 예측하는 모델을 학습해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YlhAzzfCLDQ0",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:59:14.858382900Z",
     "start_time": "2024-01-09T21:59:14.843385800Z"
    }
   },
   "source": [
    "# the save point\n",
    "if use_colab:\n",
    "    checkpoint_dir ='./drive/My Drive/train_ckpt/semiconductor/exp1'\n",
    "    if not os.path.isdir(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "else:\n",
    "    checkpoint_dir = 'semiconductor/exp1'\n",
    "\n"
   ],
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 monitor='val_mae',\n",
    "                                                 mode='auto',\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=15,\n",
    "                                                     monitor='val_loss',\n",
    "                                                     restore_best_weights=True,\n",
    "\n",
    "                                                     verbose=1)\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',  # 관찰할 지표\n",
    "                              factor=0.2,  # 학습률을 줄이는 비율\n",
    "                              patience=5,  # 몇 번의 에포크 동안 감소하지 않아야 하는지\n",
    "                              min_lr=1e-9)"
   ],
   "metadata": {
    "id": "SHMZXYLxahcH",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:59:15.087295500Z",
     "start_time": "2024-01-09T21:59:15.054641800Z"
    }
   },
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XaTHvzqjuCQS",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:59:15.486796500Z",
     "start_time": "2024-01-09T21:59:15.450142500Z"
    }
   },
   "source": [
    "model.compile(loss='mae', #mse\n",
    "              optimizer='adam',\n",
    "              metrics=['mae']) #mse"
   ],
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QUUO_VcoLJEg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "936958de-3595-4b89-96ab-3bcd60f22ef2",
    "ExecuteTime": {
     "end_time": "2024-01-09T22:07:52.759452500Z",
     "start_time": "2024-01-09T21:59:16.054578200Z"
    }
   },
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    steps_per_epoch=len(train) / batch_size, # train data의 길이 // batch 길이\n",
    "                    epochs=10,\n",
    "                    callbacks=[cp_callback,early_stopping_cb,reduce_lr],\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=len(t_train) / batch_size)"
   ],
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2847/2847 [============================>.] - ETA: 0s - loss: 45.5541 - mae: 43.4169\n",
      "Epoch 1: val_mae improved from inf to 22.46510, saving model to ./drive/My Drive/train_ckpt/semiconductor\\exp1\n",
      "2847/2847 [==============================] - 111s 34ms/step - loss: 45.5468 - mae: 43.4096 - val_loss: 24.6465 - val_mae: 22.4651 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "2848/2847 [==============================] - ETA: 0s - loss: 19.4296 - mae: 17.1164\n",
      "Epoch 2: val_mae improved from 22.46510 to 11.83725, saving model to ./drive/My Drive/train_ckpt/semiconductor\\exp1\n",
      "2847/2847 [==============================] - 98s 34ms/step - loss: 19.4296 - mae: 17.1164 - val_loss: 14.2244 - val_mae: 11.8373 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "2847/2847 [============================>.] - ETA: 0s - loss: 16.0553 - mae: 13.6689\n",
      "Epoch 3: val_mae improved from 11.83725 to 10.43833, saving model to ./drive/My Drive/train_ckpt/semiconductor\\exp1\n",
      "2847/2847 [==============================] - 99s 35ms/step - loss: 16.0552 - mae: 13.6688 - val_loss: 12.8131 - val_mae: 10.4383 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "2848/2847 [==============================] - ETA: 0s - loss: 14.9567 - mae: 12.5984\n",
      "Epoch 4: val_mae improved from 10.43833 to 8.69294, saving model to ./drive/My Drive/train_ckpt/semiconductor\\exp1\n",
      "2847/2847 [==============================] - 94s 33ms/step - loss: 14.9567 - mae: 12.5984 - val_loss: 11.0372 - val_mae: 8.6929 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "2847/2847 [============================>.] - ETA: 0s - loss: 14.2218 - mae: 11.9129\n",
      "Epoch 5: val_mae improved from 8.69294 to 7.85288, saving model to ./drive/My Drive/train_ckpt/semiconductor\\exp1\n",
      "2847/2847 [==============================] - 96s 34ms/step - loss: 14.2214 - mae: 11.9125 - val_loss: 10.1289 - val_mae: 7.8529 - lr: 0.0010\n",
      "Epoch 6/10\n",
      " 595/2847 [=====>........................] - ETA: 1:10 - loss: 14.0047 - mae: 11.7282"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[53], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m                    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# train data의 길이 // batch 길이\u001B[39;49;00m\n\u001B[0;32m      3\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mcp_callback\u001B[49m\u001B[43m,\u001B[49m\u001B[43mearly_stopping_cb\u001B[49m\u001B[43m,\u001B[49m\u001B[43mreduce_lr\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mt_train\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\keras\\engine\\training.py:1564\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1556\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[0;32m   1557\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1558\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1561\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   1562\u001B[0m ):\n\u001B[0;32m   1563\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[1;32m-> 1564\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1565\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[0;32m   1566\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[1;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[0;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[0;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[1;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateless_fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[0;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[0;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[0;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2493\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m   2494\u001B[0m   (graph_function,\n\u001B[0;32m   2495\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2496\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2497\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1858\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[0;32m   1859\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[0;32m   1860\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[0;32m   1861\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[1;32m-> 1862\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1863\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m   1864\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[0;32m   1865\u001B[0m     args,\n\u001B[0;32m   1866\u001B[0m     possible_gradient_type,\n\u001B[0;32m   1867\u001B[0m     executing_eagerly)\n\u001B[0;32m   1868\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    498\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 499\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    505\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    506\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[0;32m    507\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[0;32m    508\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    511\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[0;32m    512\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Hcx7cdQgLWau",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:51:40.705292700Z",
     "start_time": "2024-01-09T21:51:40.703294400Z"
    }
   },
   "source": [
    "# np.save(\"file/path.npy\", history.numpy())\n",
    "# history = np.load(\"histoy.npy\", allow_pickle=True)\n",
    "\n",
    "loss=history.history['mae'] # mse\n",
    "val_loss=history.history['val_mae'] # val_mse\n",
    "\n",
    "epochs_range = range(len(loss))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(epochs_range, loss, label='Training MAE') # MSE\n",
    "plt.plot(epochs_range, val_loss, label='Validation MAE') # MSE\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation MAE') # MSE\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIPmAVP1u9nX"
   },
   "source": [
    "### 모델 평가\n",
    "* Mean absolute error 를 이용해 모델이 정확히 예측하는지를 확인"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.load_weights(checkpoint_dir)"
   ],
   "metadata": {
    "id": "th1Wg9JEayQ5",
    "ExecuteTime": {
     "start_time": "2024-01-09T21:51:40.704293100Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VqsdquFJuKUv",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:51:40.706291900Z",
     "start_time": "2024-01-09T21:51:40.706291900Z"
    }
   },
   "source": [
    "# model.load_weights(checkpoint_dir)\n",
    "results = model.evaluate(test_dataset)\n",
    "print(\"MAE :\", results[0]) # MAE"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1mWFZphNupHX",
    "ExecuteTime": {
     "end_time": "2024-01-09T21:51:40.709296300Z",
     "start_time": "2024-01-09T21:51:40.707296100Z"
    }
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-09T21:51:40.708295600Z"
    }
   }
  }
 ]
}

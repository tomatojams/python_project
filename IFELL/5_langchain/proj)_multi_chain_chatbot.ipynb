{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 구글 드라이브 연결"
   ],
   "metadata": {
    "id": "AvGJfn3fKnIh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "dhBepVQ7kaYa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1dc42c0b-48ba-4222-9b7a-bfe423d09133"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 필수 설치 라이브러리"
   ],
   "metadata": {
    "id": "69rCi1LbKlkz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U langchain openai"
   ],
   "metadata": {
    "id": "E5cSH-P7kDDf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0dd705a2-7f1f-4fee-feab-a5c13ba1db36"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.1.4-py3-none-any.whl (803 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m803.6/803.6 kB\u001B[0m \u001B[31m9.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting openai\n",
      "  Downloading openai-1.10.0-py3-none-any.whl (225 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m225.1/225.1 kB\u001B[0m \u001B[31m13.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.14 (from langchain)\n",
      "  Downloading langchain_community-0.0.16-py3-none-any.whl (1.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m21.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting langchain-core<0.2,>=0.1.16 (from langchain)\n",
      "  Downloading langchain_core-0.1.17-py3-none-any.whl (235 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m235.9/235.9 kB\u001B[0m \u001B[31m17.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting langsmith<0.1,>=0.0.83 (from langchain)\n",
      "  Downloading langsmith-0.0.85-py3-none-any.whl (54 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m54.0/54.0 kB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.14)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m75.9/75.9 kB\u001B[0m \u001B[31m8.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
      "Collecting typing-extensions<5,>=4.7 (from openai)\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.4/49.4 kB\u001B[0m \u001B[31m5.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m76.9/76.9 kB\u001B[0m \u001B[31m9.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m58.3/58.3 kB\u001B[0m \u001B[31m6.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: typing-extensions, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, openai, langchain-core, langchain-community, langchain\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llmx 0.0.15a0 requires cohere, which is not installed.\n",
      "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
      "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed dataclasses-json-0.6.3 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.4 langchain-community-0.0.16 langchain-core-0.1.17 langsmith-0.0.85 marshmallow-3.20.2 mypy-extensions-1.0.0 openai-1.10.0 typing-extensions-4.9.0 typing-inspect-0.9.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "from langchain.chains import ConversationChain, LLMChain, LLMRouterChain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from pydantic import BaseModel"
   ],
   "metadata": {
    "id": "k9S8nGKhkF5O"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### API 키 입력"
   ],
   "metadata": {
    "id": "X4k4IQXLKqJd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import getpass\n",
    "import os\n",
    "#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\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ],
   "metadata": {
    "id": "IF5T5HPykHoL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fdea00c0-bca2-4d4c-9465-9c0febcbd237"
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "··········\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LLM 파트 구현\n",
    "* 게임룰에 대한 정보들을 얻는 방법을 프롬프트 체인을 이용해 구성했습니다.\n",
    "* 부루마블이라는 보드게임을 진행하기위한 기본적인 rule과 건물을 지을 수 있는 규칙이 들어간 데이터를 이용해서 문답을 진행합니다."
   ],
   "metadata": {
    "id": "Q-JehMyJK4OB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "PATH = \"./drive/MyDrive/Colab Notebooks/IFELL/chain_prompts\"\n",
    "RULE_1 = os.path.join(\n",
    "    PATH, \"game_basic.txt\"\n",
    ")\n",
    "RULE_2 = os.path.join(\n",
    "    PATH, \"game_building.txt\"\n",
    ")\n",
    "PHIL_1 = os.path.join(\n",
    "    PATH, \"Philosophy_1.txt\"\n",
    ")\n",
    "PHIL_2 = os.path.join(\n",
    "    PATH, \"Philosophy_2.txt\"\n",
    ")\n",
    "PHIL_3 = os.path.join(\n",
    "    PATH, \"Philosophy_3.txt\"\n",
    ")\n",
    "\n",
    "\n",
    "def read_prompt_template(file_path: str) -> str:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        prompt_template = f.read()\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def create_chain(llm, template_path, output_key):\n",
    "    return LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=ChatPromptTemplate.from_template(\n",
    "            template=read_prompt_template(template_path)\n",
    "        ),\n",
    "        output_key=output_key,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1, max_tokens=200, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "rule_1 = create_chain(\n",
    "    llm=llm,\n",
    "    template_path=RULE_1,\n",
    "    output_key=\"text\",\n",
    ")\n",
    "rule_2 = create_chain(\n",
    "    llm=llm,\n",
    "    template_path=RULE_2,\n",
    "    output_key=\"text\",\n",
    ")\n",
    "phil_1 = create_chain(\n",
    "    llm=llm,\n",
    "    template_path=PHIL_1,\n",
    "    output_key=\"text\",\n",
    ")\n",
    "phil_2 = create_chain(\n",
    "    llm=llm,\n",
    "    template_path=PHIL_2,\n",
    "    output_key=\"text\",\n",
    ")\n",
    "phil_3 = create_chain(\n",
    "    llm=llm,\n",
    "    template_path=PHIL_3,\n",
    "    output_key=\"text\",\n",
    ")\n",
    "\n",
    "\n",
    "destinations = [\n",
    "    \"basic: This page describes the basic rules used to play the board game Burumble.\",\n",
    "    \"building: This is where you'll find the rules for buildings as you play the board game.\",\n",
    "    \"philosophy_kant: This is where you'll find the philosophy of kant.\",\n",
    "    \"philosophy_descartes: This is where you'll find the philosophy of descartes.\",\n",
    "    \"philosophy_Confucius: This is where you'll find the philosophy of Confucius.\",\n",
    "]\n",
    "destinations = \"\\n\".join(destinations)\n",
    "router_prompt_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations)\n",
    "router_prompt = PromptTemplate.from_template(\n",
    "    template=router_prompt_template, output_parser=RouterOutputParser()\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm=llm, prompt=router_prompt, verbose=True)\n",
    "\n",
    "multi_prompt_chain = MultiPromptChain( # 멀티프롬프트체인 라우터체인과 데이터네이션 체인 그리고 디펄트체의 합체\n",
    "    router_chain=router_chain, # 라우터를 쓰면서 텍스트양이 반으로 줄어듬 정확도도 올라감\n",
    "\n",
    "    destination_chains={\n",
    "    \"basic\": rule_1,\n",
    "    \"building\": rule_2,\n",
    "    \"philosophy_kant\": phil_1,\n",
    "    \"philosophy_descartes\": phil_2,\n",
    "    \"philosophy_Confucius\": phil_3,\n",
    "},\n",
    "\n",
    "    default_chain=ConversationChain(llm=llm, output_key=\"text\"),\n",
    ")\n",
    "\n",
    "\n",
    "class UserRequest(BaseModel):\n",
    "    user_message: str\n",
    "\n",
    "\n",
    "def gernerate_answer(req: UserRequest) -> Dict[str, str]:\n",
    "    context = req.dict()\n",
    "    context[\"input\"] = context[\"user_message\"]\n",
    "    answer = multi_prompt_chain.run(context)\n",
    "\n",
    "    return {\"answer\": answer}"
   ],
   "metadata": {
    "id": "5fnHwrb3myFZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### User 데이터 입력\n",
    "* 유저 데이터 입력 후 결과를 확인 합니다."
   ],
   "metadata": {
    "id": "1uFhqhFLLdA_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "user_data = {\n",
    "    \"user_message\": \"칸트 철학을 요약해줘\"\n",
    "}"
   ],
   "metadata": {
    "id": "BmhMK-FlDVgO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "request_instance = UserRequest(**user_data)"
   ],
   "metadata": {
    "id": "EThoAGelDMeJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "gernerate_answer(request_instance)"
   ],
   "metadata": {
    "id": "hhqWB9SxDPQM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b5db86d6-cdb7-458f-986f-f3ff981c3b4f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMRouterChain chain...\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mHuman: 칸트 \n",
      "\n",
      "칸트의 사상 체계는 흔히 크게 세 갈래로 나눠진다:\n",
      "\t•\t인식론: \"나는 무엇을 알 수 있는가?\"\n",
      "\t•\t윤리학: \"나는 무엇을 해야 하는가?\"\n",
      "\t•\t종교철학: \"나는 무엇을 희망해도 좋은가?\"\n",
      "\n",
      "이 중 첫째 질문인 \"나는 무엇을 알 수 있는가?\"는 『순수 이성 비판』에서 주로 다루어진다. 첫째 질문은 자신이 문제삼는 것에서 분명하게 드러나듯, 인간 이성이 인식할 수 있는 범위와 한계가 어디까지인지에 대한 물음이다. 이는 다시 말해, 우리는 어디까지 알 수 있으며, 또한 어떤 것은 알 수 없는지를 논의하겠다는 것을 의미한다. 이러한 점에서 『순수 이성 비판』은 첫째 질문을 본격적으로 논의면서, 인간 이성의 능력이 지닌 범위와 한계가 어디까지인지를 적극적으로 검토한다. 이 점에서 『순수 이성 비판』은 인간의 인식과 앎에 대한 논의를 첫째 질문을 통해 수행하고 있음을 알 수 있다. 칸트에 따르면, 이성주의(합리주의) 철학의 전통에서 이성은 그 능력이 검증되지 않은 채, 신, 영혼불멸, 자유와 같은 무제약자들을 함부로 추구했다는 문제가 있다. 그러나 이러한 신과 같은 무제약자들은 인간 이성이 인식할 수 있는 한계를 아득히 넘어서는 초감성적인 대상들로, 유한한 인간 이성은 자신의 한계로 인해 결코 인식할 수 없다. 따라서 인간 이성이 인식할 수 있는 범위는 오직 자연 인과성의 지배를 받는 감성적인 경험적 대상들밖에 없으며, 이러한 범위를 넘어서서 초감성적인 대상들을 인식하려는 모든 시도들은 독단적인dogmatisch 것일 수밖에 없다.\n",
      "\n",
      "둘째 질문인 \"나는 무엇을 해야만 하는가?\"는 도덕적 물음으로, 『도덕형이상학 정초』와 『실천 이성 비판』에서 다루어진다. 둘째 물음에 대한 답변, 즉 우리가 해야만 하는 일은 결국 도덕적 행동이다. 우리는 도덕적으로 행동해야만 한다. 그렇다면 도덕적 행동은 무엇인가? 바로 도덕법칙에 따르고자 하는 행위 원리를 받아들여 행동하는 것이다. 이처럼 도덕법칙에 따라 행동하는 것의 의미가 무엇인지에 대해 칸트는 『도덕형이상학 정초』와 『실천 이성 비판』 전반부 「순수 실천이성의 분석론」에서 비판적으로 검토하고 있다.\n",
      "\n",
      "마지막으로 셋째 물음인 \"나는 무엇을 희망해도 좋은가?\"는 칸트의 여러 저서에서 복합적으로 논의되고 있다. 칸트의 저서에서 이 셋째 물음이 최초로 등장한 저서는 『순수 이성 비판』으로, 칸트는 『순수 이성 비판』의 후반부인 「방법론」 중에서도 「순수 실천이성의 규준」 장에서 이 물음의 의미를 본격적으로 제시하고 있다.[79] 『순수 이성 비판』에 따르면, 우리가 둘째 물음에 따라 도덕성을 확보하는 일은 우리가 \"행복해도 좋을 자격\"을 갖추는 일이다. 따라서 우리가 충분히 도덕적 행위 원리를 받아들여 행복해도 좋을 자격을 가지게 된다면, 비로소 우리는 우리의 도덕성에 상응하는 정도만큼 행복해질 수 있을 것이라고 희망할 수 있게 된다는 것이다. 다만 이 행복이 우리에게 실제로 주어진다고 보지 않고, 희망의 대상이라고 말한다는 점에서 우리는 칸트는 전통적인 행복주의 윤리학을 적극적으로 거부하였음을 알 수 있는데, 왜냐하면 전통적인 행복주의 윤리학은 도덕적으로 행동하게 될 경우 현세의 삶에서 도덕적 행복을 얻을 수 있다고 주장하기 때문이다. 즉 행복주의와는 달리 칸트는 우리가 아무리 도덕적으로 살아간다고 하더라도, 자연법칙이 지배하는 감성계에서는 결코 도덕성에 상응하는 행복이 도출될 수 없으며, 따라서 우리가 도덕적으로 산다 해도 얻을 수 있는 것은 단지 행복해도 좋을 자격, 그리고 그 자격에 행복이 따라오리라는 희망 뿐이라고 주장한다. 또한 칸트에 따르면, 도덕성에 상응하는 행복은 결코 우리 인간의 능력으로는 실현 불가능한 것이지만, 행복의 분배에 개입하는 전능한 신을 가정한다면 각자의 도덕성에 상응하는 정도만큼 그 행복을 희망할 수 있게 되므로, 이러한 도덕에 상응하는 행복이 보장될 수 있게 하기 위해서는 신의 현존을 요청해야만 한다. 이 점에서 \"나는 무엇을 희망해도 좋은가?\"라는 물음은 신의 현존을 요청하는 요청이론Postulatslehre으로 이행하게 되며, 이는 칸트의 고유한 종교철학 이론인 도덕신학Moralstheologie을 구성하는 주요한 체계가 된다. 그러므로 셋째 물음은 종교적 물음과 필연적으로 연관될 수밖에 없다.\n",
      "\n",
      "이처럼 『순수 이성 비판』에서 제시된 \"나는 무엇을 희망해도 좋은가?\"라는 물음은 결국 행복할 자격과 행복이 필연적으로 연결되는 최고선이라는 도덕적 이상의 문제로 연결된다. 그러므로 이 세 번째 물음은 최고선의 문제를 다룬 여러 저서들에서 간접적으로, 또 반복적으로 등장한다고 이해할 수 있다. 이는 『실천 이성 비판』의 후반부인 「변증론」에서는 영혼불멸과 신의 현존의 요청을 통해 최고선의 실현에 대한 희망으로 나타나며, 『이성의 한계 안에서의 종교』에서는 근본악의 문제와 그 원인인 악의 원리에 대항하는 윤리적 공동체구현 내지는 최고선에 대한 희망으로, 또 『판단력비판』에서는 자연과 자유의 통일을 통해 이 지상에서 실현될 수 있을 자연의 궁극목적이자 목적들의 나라인 최고선에 대한 희망으로 나타나기도 한다.\n",
      "\n",
      "[message]\n",
      "칸트 철학을 요약해줘\n",
      "\n",
      "Answer:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'answer': '칸트의 사상 체계는 인식론, 윤리학, 종교철학으로 크게 세 가지로 나눌 수 있다. 첫째 질문은 \"나는 무엇을 알 수 있는가?\"로, 이는 인간 이성이 인식할 수 있는 범위와 한계에 대한 물음이다. 둘째 질문은 \"나는 무엇을 해야 하는가?\"로, 도덕적 행동에 대한 물음이다. 셋째 질문은 \"나는 무엇을 희망해도 좋은가?\"로, 최고선에 대한 희망과 종교적 물음이다.\\n\\n칸트는 인식'}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ZTq--JbgOCDc"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}

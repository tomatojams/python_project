{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 구글 드라이브 연결"
   ],
   "metadata": {
    "id": "AvGJfn3fKnIh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "dhBepVQ7kaYa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1dc42c0b-48ba-4222-9b7a-bfe423d09133",
    "ExecuteTime": {
     "end_time": "2024-02-08T13:56:26.042940200Z",
     "start_time": "2024-02-08T13:56:26.028755800Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 필수 설치 라이브러리"
   ],
   "metadata": {
    "id": "69rCi1LbKlkz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install -U langchain openai"
   ],
   "metadata": {
    "id": "E5cSH-P7kDDf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0dd705a2-7f1f-4fee-feab-a5c13ba1db36",
    "ExecuteTime": {
     "end_time": "2024-02-08T13:56:27.370096600Z",
     "start_time": "2024-02-08T13:56:27.356175700Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "from langchain.chains import ConversationChain, LLMChain, LLMRouterChain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from pydantic import BaseModel\n",
    "from langchain.chains import create_extraction_chain\n",
    "from typing import Optional\n",
    "\n",
    "from langchain.chains import create_extraction_chain_pydantic\n",
    "from langchain.pydantic_v1 import BaseModel\n"
   ],
   "metadata": {
    "id": "k9S8nGKhkF5O",
    "ExecuteTime": {
     "end_time": "2024-02-10T10:30:15.249247Z",
     "start_time": "2024-02-10T10:30:14.284530Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### API 키 입력"
   ],
   "metadata": {
    "id": "X4k4IQXLKqJd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import getpass\n",
    "import os\n",
    "# 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\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ],
   "metadata": {
    "id": "IF5T5HPykHoL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fdea00c0-bca2-4d4c-9465-9c0febcbd237",
    "ExecuteTime": {
     "end_time": "2024-02-10T10:30:28.621622Z",
     "start_time": "2024-02-10T10:30:26.423626Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new ConversationChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mThe following is a friendly conversation between a human and an AI. \n",
      "The AI is talkative and provides lots of specific details from its context. If the \n",
      "AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: What is GPT in 5 words?\n",
      "AI:\u001B[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create() got an unexpected keyword argument 'engine'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[241], line 42\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Example call\u001B[39;00m\n\u001B[1;32m     41\u001B[0m user_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat is GPT in 5 words?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 42\u001B[0m \u001B[43mchain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[43minput_key\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser_input\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m[output_key]\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:145\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    143\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    144\u001B[0m     emit_warning()\n\u001B[0;32m--> 145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/langchain/chains/base.py:363\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    331\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[1;32m    332\u001B[0m \n\u001B[1;32m    333\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[1;32m    355\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    356\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    357\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[1;32m    358\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[1;32m    359\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[1;32m    360\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[1;32m    361\u001B[0m }\n\u001B[0;32m--> 363\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    364\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    365\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    367\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    368\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/langchain/chains/base.py:162\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    161\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 162\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    163\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    164\u001B[0m final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    165\u001B[0m     inputs, outputs, return_only_outputs\n\u001B[1;32m    166\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/langchain/chains/base.py:156\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    149\u001B[0m run_manager \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_chain_start(\n\u001B[1;32m    150\u001B[0m     dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    151\u001B[0m     inputs,\n\u001B[1;32m    152\u001B[0m     name\u001B[38;5;241m=\u001B[39mrun_name,\n\u001B[1;32m    153\u001B[0m )\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 156\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    158\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    159\u001B[0m     )\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    161\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/langchain/chains/llm.py:103\u001B[0m, in \u001B[0;36mLLMChain._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call\u001B[39m(\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    100\u001B[0m     inputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[1;32m    101\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    102\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m--> 103\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_outputs(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/langchain/chains/llm.py:115\u001B[0m, in \u001B[0;36mLLMChain.generate\u001B[0;34m(self, input_list, run_manager)\u001B[0m\n\u001B[1;32m    113\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m run_manager\u001B[38;5;241m.\u001B[39mget_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm, BaseLanguageModel):\n\u001B[0;32m--> 115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    122\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mbind(stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs)\u001B[38;5;241m.\u001B[39mbatch(\n\u001B[1;32m    123\u001B[0m         cast(List, prompts), {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks}\n\u001B[1;32m    124\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:544\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    537\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    538\u001B[0m     prompts: List[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    541\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    542\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    543\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 544\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:408\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[1;32m    406\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[1;32m    407\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 408\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    409\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    410\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)\n\u001B[1;32m    411\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[1;32m    412\u001B[0m ]\n\u001B[1;32m    413\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:398\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[1;32m    395\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[1;32m    396\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    397\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 398\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    400\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    401\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    402\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    403\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    404\u001B[0m         )\n\u001B[1;32m    405\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    406\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:577\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    573\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    574\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    575\u001B[0m     )\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported:\n\u001B[0;32m--> 577\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    578\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    579\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    580\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    581\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/langchain_community/chat_models/openai.py:439\u001B[0m, in \u001B[0;36mChatOpenAI._generate\u001B[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001B[0m\n\u001B[1;32m    433\u001B[0m message_dicts, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_message_dicts(messages, stop)\n\u001B[1;32m    434\u001B[0m params \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    435\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[1;32m    436\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream} \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}),\n\u001B[1;32m    437\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    438\u001B[0m }\n\u001B[0;32m--> 439\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompletion_with_retry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    440\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmessages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessage_dicts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\n\u001B[1;32m    441\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    442\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(response)\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/langchain_community/chat_models/openai.py:356\u001B[0m, in \u001B[0;36mChatOpenAI.completion_with_retry\u001B[0;34m(self, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001B[39;00m\n\u001B[1;32m    355\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_openai_v1():\n\u001B[0;32m--> 356\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    358\u001B[0m retry_decorator \u001B[38;5;241m=\u001B[39m _create_retry_decorator(\u001B[38;5;28mself\u001B[39m, run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[1;32m    360\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/openai/_utils/_utils.py:271\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    269\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    270\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 271\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: create() got an unexpected keyword argument 'engine'"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Keys can be freely adjusted\n",
    "memory_key = \"foo\"\n",
    "input_key = \"bar\"\n",
    "output_key = \"baz\"\n",
    "\n",
    "# Initialize the context with a prompt template\n",
    "template = r\"\"\"The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context. If the \n",
    "AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{}\n",
    "Human: {}\n",
    "AI:\"\"\".format(\"{\" + memory_key + \"}\", \"{\" + input_key + \"}\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[memory_key, input_key], template=template\n",
    ")\n",
    "\n",
    "# Initialize memory to store conversation history\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=memory_key, input_key=input_key, output_key=output_key\n",
    ")\n",
    "\n",
    "# Initialize large language model\n",
    "model_kwargs = {\"engine\": \"gpt-35-turbo-0613\", \"headers\": {\"x-api-key\": os.environ[\"OPENAI_API_KEY\"]}}\n",
    "llm = ChatOpenAI(model_kwargs=model_kwargs, temperature=0.0)\n",
    "\n",
    "# Initialize and return conversation chain\n",
    "chain = ConversationChain(\n",
    "    llm=llm, memory=memory, prompt=prompt, verbose=True,\n",
    "    input_key=input_key, output_key=output_key\n",
    ")\n",
    "\n",
    "# Example call\n",
    "user_input = \"What is GPT in 5 words?\"\n",
    "chain({input_key: user_input})[output_key]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T12:40:44.461702Z",
     "start_time": "2024-02-10T12:40:43.664881Z"
    }
   },
   "execution_count": 241
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LLM 파트 구현\n",
    "* 게임룰에 대한 정보들을 얻는 방법을 프롬프트 체인을 이용해 구성했습니다.\n",
    "* 부루마블이라는 보드게임을 진행하기위한 기본적인 rule과 건물을 지을 수 있는 규칙이 들어간 데이터를 이용해서 문답을 진행합니다."
   ],
   "metadata": {
    "id": "Q-JehMyJK4OB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 분기 + 추출 테스트 + 시퀀셜\n",
    "from langchain.chains import SequentialChain\n",
    "PATH = \"./chain_prompts\"\n",
    "\n",
    "RULE_1 = os.path.join(\n",
    "    PATH, \"game_basic.txt\"\n",
    ")\n",
    "RULE_2 = os.path.join(\n",
    "    PATH, \"game_building.txt\"\n",
    ")\n",
    "INTRO = os.path.join(\n",
    "    PATH, \"intro.txt\"\n",
    ")\n",
    "\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"my_name\": {\"type\": \"string\"},\n",
    "        \"my_feeling\": {\"type\": \"string\"},\n",
    "        \"my_age\": {\"type\": \"string\"},\n",
    "        \"my_extra_info\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"이름\"],\n",
    "}\n",
    "\n",
    "def read_prompt_template(file_path: str) -> str:\n",
    "    with open(file_path, \"r\", encoding='UTF8') as f:\n",
    "        prompt_template = f.read()\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def create_chain(llm, template_path, output_key):\n",
    "    return LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=ChatPromptTemplate.from_template(\n",
    "            template=read_prompt_template(template_path)\n",
    "        ),\n",
    "        output_key=output_key,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1, max_tokens=200, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "rule_1 = create_chain(\n",
    "    llm=llm,\n",
    "    template_path=RULE_1,\n",
    "    output_key=\"text\",\n",
    ")\n",
    "rule_2 = create_chain(\n",
    "    llm=llm,\n",
    "    template_path=RULE_2,\n",
    "    output_key=\"aa\",\n",
    ")\n",
    "intro = create_chain(\n",
    "    llm=llm,\n",
    "    template_path=INTRO,\n",
    "    output_key=\"text\",\n",
    ")\n",
    "\n",
    "\n",
    "extract_chain = (\n",
    "    create_extraction_chain(schema, llm,))\n",
    "\n",
    "destinations = [\n",
    "    \"basic: This page describes the basic rules used to play the board game Burumble.\",\n",
    "    # \"building: This is where you'll find the rules for buildings as you play the board game.\",\n",
    "    \"intro_AI: This is where you'll find the introduction of myself.\",\n",
    "    # \"고객에대해: 고객이 자신에 대해서 말할때.\",\n",
    "]\n",
    "destinations = \"\\n\".join(destinations)\n",
    "router_prompt_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations)\n",
    "router_prompt = PromptTemplate.from_template(\n",
    "    template=router_prompt_template, output_parser=RouterOutputParser()\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm=llm, prompt=router_prompt, verbose=True, output_key=\"out_text\")\n",
    "\n",
    "multi_prompt_chain = MultiPromptChain( # 멀티프롬프트체인 라우터체인과 데이터네이션 체인 그리고 디펄트체의 합체\n",
    "    router_chain=router_chain, # 라우터를 쓰면서 텍스트양이 반으로 줄어듬 정확도도 올라감\n",
    "\n",
    "    destination_chains={\n",
    "    \"basic\": rule_1,\n",
    "    # \"building\": rule_2,\n",
    "    \"intro_AI\": intro, # 소개를 질문받으면 답변합니다.\n",
    "    # \"고객에대해\": extract_chain, # 고객정보를 추출합니다.\n",
    "},\n",
    "\n",
    "    default_chain=ConversationChain(llm=llm, output_key=\"text\"),\n",
    ")\n",
    "\n",
    "overall = SequentialChain(\n",
    "    chains=[\n",
    "        # multi_prompt_chain,\n",
    "        # rule_2,\n",
    "        extract_chain,\n",
    "    ],\n",
    "    input_variables=[\"input\"],\n",
    "    output_variables=[\"text\"],\n",
    ")\n"
   ],
   "metadata": {
    "id": "5fnHwrb3myFZ",
    "ExecuteTime": {
     "end_time": "2024-02-10T12:56:18.937953Z",
     "start_time": "2024-02-10T12:56:18.827478Z"
    }
   },
   "execution_count": 286,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class UserRequest(BaseModel):\n",
    "    user_message: str\n",
    "\n",
    "\n",
    "def gernerate_answer(req: UserRequest) -> Dict[str, str]:\n",
    "    context = req.dict()\n",
    "    context[\"input\"] = context[\"user_message\"]\n",
    "    answer = overall(context)\n",
    "    # answer = multi_prompt_chain.run(context)\n",
    "\n",
    "    return {\"answer\": answer}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-10T12:56:19.779338Z",
     "start_time": "2024-02-10T12:56:19.772206Z"
    }
   },
   "execution_count": 287
  },
  {
   "cell_type": "markdown",
   "source": [
    "### User 데이터 입력\n",
    "* 유저 데이터 입력 후 결과를 확인 합니다."
   ],
   "metadata": {
    "id": "1uFhqhFLLdA_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "user_data = {\n",
    "    \"user_message\": \"나는 김지한 나이는 30살?\"\n",
    "}"
   ],
   "metadata": {
    "id": "BmhMK-FlDVgO",
    "ExecuteTime": {
     "end_time": "2024-02-10T12:56:53.773001Z",
     "start_time": "2024-02-10T12:56:53.766237Z"
    }
   },
   "execution_count": 291,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "request_instance = UserRequest(**user_data)\n",
    "# extract_chain.run(user_data[\"user_message\"])"
   ],
   "metadata": {
    "id": "EThoAGelDMeJ",
    "ExecuteTime": {
     "end_time": "2024-02-10T12:56:54.093194Z",
     "start_time": "2024-02-10T12:56:54.089690Z"
    }
   },
   "execution_count": 292,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "gernerate_answer(request_instance)"
   ],
   "metadata": {
    "id": "hhqWB9SxDPQM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b5db86d6-cdb7-458f-986f-f3ff981c3b4f",
    "ExecuteTime": {
     "end_time": "2024-02-10T12:56:56.417580Z",
     "start_time": "2024-02-10T12:56:54.517016Z"
    }
   },
   "execution_count": 293,
   "outputs": [
    {
     "data": {
      "text/plain": "{'answer': {'user_message': '나는 김지한 나이는 30살?',\n  'input': '나는 김지한 나이는 30살?',\n  'text': [{'my_name': '김지한', 'my_age': '30살'}]}}"
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}

{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 구글 드라이브 연결"
   ],
   "metadata": {
    "id": "AvGJfn3fKnIh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "dhBepVQ7kaYa",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1dc42c0b-48ba-4222-9b7a-bfe423d09133",
    "ExecuteTime": {
     "end_time": "2024-02-08T13:56:26.042940200Z",
     "start_time": "2024-02-08T13:56:26.028755800Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 필수 설치 라이브러리"
   ],
   "metadata": {
    "id": "69rCi1LbKlkz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !pip install -U langchain openai"
   ],
   "metadata": {
    "id": "E5cSH-P7kDDf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0dd705a2-7f1f-4fee-feab-a5c13ba1db36",
    "ExecuteTime": {
     "end_time": "2024-02-08T13:56:27.370096600Z",
     "start_time": "2024-02-08T13:56:27.356175700Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "from langchain.chains import ConversationChain, LLMChain, LLMRouterChain\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from pydantic import BaseModel\n",
    "from langchain.chains import create_extraction_chain\n",
    "\n",
    "from typing import Any, List, Optional\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.prompts import BasePromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.openai_functions.utils import (\n",
    "    _convert_schema,\n",
    "    _resolve_schema_references,\n",
    "    get_llm_kwargs,\n",
    ")\n",
    "from langchain.output_parsers.openai_functions import (\n",
    "    JsonKeyOutputFunctionsParser,\n",
    "    PydanticAttrOutputFunctionsParser,\n",
    ")\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T03:39:28.562079300Z",
     "start_time": "2024-02-12T03:39:26.782793500Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### API 키 입력"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T03:39:30.686798800Z",
     "start_time": "2024-02-12T03:39:28.563653700Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#멀티프롬프트 체인의 키를 변경하기 위한 클래스\n",
    "class CustomPromptChain(MultiPromptChain):\n",
    "    \"\"\"A custom multi-route chain based on MultiPromptChain with a modified output key.\"\"\"\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return [\"out_text\"]\n",
    "# 추출체인의 output_key를 변경하기 위한 클래스\n",
    "llm = ChatOpenAI(temperature=0.1, max_tokens=400, model=\"gpt-3.5-turbo\")\n",
    "class CustomExtractionChain(LLMChain):\n",
    "    \"\"\"A custom extraction chain based on LLMChain with a modified output key.\"\"\"\n",
    "    output_key: str = \"extracted_data\"\n",
    "\n",
    "def _get_extraction_function(entity_schema: dict) -> dict:\n",
    "    return {\n",
    "        \"name\": \"information_extraction\",\n",
    "        \"description\": \"Extracts the relevant information from the passage.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"info\": {\"type\": \"array\", \"items\": _convert_schema(entity_schema)}\n",
    "            },\n",
    "            \"required\": [\"info\"],\n",
    "        },\n",
    "    }\n",
    "_EXTRACTION_TEMPLATE = \"\"\"Extract and save the relevant entities mentioned \\\n",
    "in the following passage together with their properties.\n",
    "\n",
    "Only extract the properties mentioned in the 'information_extraction' function.\n",
    "\n",
    "If a property is not present and is not required in the function parameters, do not include it in the output.\n",
    "\n",
    "Passage:\n",
    "{input}\n",
    "\"\"\"  # noqa: E501\n",
    "\n",
    "\n",
    "def create_custom_extraction_chain(\n",
    "    schema: dict,\n",
    "    llm: BaseLanguageModel,\n",
    "    prompt: Optional[BasePromptTemplate] = None,\n",
    "    tags: Optional[List[str]] = None,\n",
    "    verbose: bool = False,\n",
    ") -> Chain:\n",
    "\n",
    "    function = _get_extraction_function(schema)\n",
    "    extraction_prompt = prompt or ChatPromptTemplate.from_template(_EXTRACTION_TEMPLATE)\n",
    "    output_parser = JsonKeyOutputFunctionsParser(key_name=\"info\")\n",
    "    llm_kwargs = get_llm_kwargs(function)\n",
    "    custom_chain = CustomExtractionChain(\n",
    "        llm=llm,\n",
    "        prompt=extraction_prompt,\n",
    "        llm_kwargs=llm_kwargs,\n",
    "        output_parser=output_parser,\n",
    "        tags=tags,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    return custom_chain"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T03:39:32.591728Z",
     "start_time": "2024-02-12T03:39:32.132961Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "시퀀셜 체인내에 분기 + 추출 하는 체인을 만듭니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "memory_key = \"foo\"\n",
    "input_key = \"input\"\n",
    "output_key = \"out_text\"\n",
    "\n",
    "# Initialize the context with a prompt template\n",
    "memory_template = r\"\"\"The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context. If the \n",
    "AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{}\n",
    "Human: {}\n",
    "AI:\"\"\".format(\"{\" + memory_key + \"}\", \"{\" + input_key + \"}\")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[memory_key, input_key], template=memory_template\n",
    ")\n",
    "\n",
    "# Initialize memory to store conversation history\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=memory_key, input_key=input_key, output_key=output_key\n",
    ")\n",
    "\n",
    "# Initialize and return conversation chain\n",
    "con_chain = ConversationChain(\n",
    "    llm=llm, memory=memory, prompt=prompt, verbose=True,\n",
    "    input_key=input_key, output_key=output_key\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T03:44:40.617419600Z",
     "start_time": "2024-02-12T03:44:40.607902100Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 분기 + 추출 테스트 + 시퀀셜\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chains import create_tagging_chain, create_tagging_chain_pydantic\n",
    "PATH = \"./chain_prompts\"\n",
    "\n",
    "ABLE = os.path.join(\n",
    "    PATH, \"ability.txt\"\n",
    ")\n",
    "MENTOR = os.path.join(\n",
    "    PATH, \"mentor.txt\"\n",
    ")\n",
    "INTRO = os.path.join(\n",
    "    PATH, \"intro.txt\"\n",
    ")\n",
    "\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"cutomer_name\": {\"type\": \"string\"},\n",
    "        \"cutomer_feeling\": {\"type\": \"string\"},\n",
    "        \"cutomer_age\": {\"type\": \"string\"},\n",
    "        \"cutomer_extra_info\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"이름\"],\n",
    "}\n",
    "tag_schema  = {\n",
    "    \"properties\": {\n",
    "        \"sentiment\": {\"type\": \"string\", \"enum\": [\"행복\", \"중립\", \"슬픔\", \"분노\", \"불안\", \"기쁨\", \"불안\", \"놀람\", \"기대\", \"기타\"]},\n",
    "        \"problem\": {\"type\": \"string\", \"enum\": [\"가족\", \"애인\", \"직장\", \"학교\", \"기타\"]},\n",
    "    }\n",
    "}\n",
    "\n",
    "def read_prompt_template(file_path: str) -> str:\n",
    "    with open(file_path, \"r\", encoding='UTF8') as f:\n",
    "        prompt_template = f.read()\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "def create_chain(llm, template_path, output_key):\n",
    "    return LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=ChatPromptTemplate.from_template(\n",
    "            template=read_prompt_template(template_path)\n",
    "        ),\n",
    "        output_key=output_key,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "able = create_chain(\n",
    "    llm=llm,\n",
    "    template_path=ABLE,\n",
    "    output_key=\"out_text\",\n",
    ")\n",
    "mentor = create_chain(\n",
    "    llm=llm,\n",
    "    template_path=MENTOR,\n",
    "    output_key=\"out_text\",\n",
    ")\n",
    "intro = create_chain(\n",
    "    llm=llm,\n",
    "    template_path=INTRO,\n",
    "    output_key=\"out_text\",\n",
    ")\n",
    "\n",
    "tag_chain = create_tagging_chain(tag_schema, llm)\n",
    "extract_chain = create_custom_extraction_chain(schema, llm)\n",
    "\n",
    "destinations = [\n",
    "    # \"능력: 챗봇이 할수있는 것들을 알려줍니다\",\n",
    "    \"Mentor: 추천할 상담사 목록\",\n",
    "    \"building: This is where you'll find the rules for buildings as you play the board game.\",\n",
    "    \"intro_AI: This is where you'll find the introduction of myself and my abilities.\",\n",
    "    # \"고객에대해: 고객이 자신에 대해서 말할때.\",\n",
    "]\n",
    "destinations = \"\\n\".join(destinations)\n",
    "router_prompt_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations)\n",
    "router_prompt = PromptTemplate.from_template(\n",
    "    template=router_prompt_template, output_parser=RouterOutputParser()\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm=llm, prompt=router_prompt, verbose=True, output_key=\"out_text\")\n",
    "\n",
    "multi_prompt_chain = CustomPromptChain( # 멀티프롬프트체인 라우터체인과 데이터네이션 체인 그리고 디펄트체의 합체\n",
    "    router_chain=router_chain, # 라우터를 쓰면서 텍스트양이 반으로 줄어듬 정확도도 올라감\n",
    "\n",
    "    destination_chains={\n",
    "    \"Mentor\": mentor,\n",
    "    \"intro_AI\": intro, # 소개를 질문받으면 답변합니다.\n",
    "},\n",
    "    default_chain=con_chain # 디펄트 체인을 설정합니다.\n",
    ")\n",
    "\n",
    "overall = SequentialChain(\n",
    "    chains=[\n",
    "        multi_prompt_chain,\n",
    "        extract_chain,\n",
    "        tag_chain,\n",
    "    ],\n",
    "    input_variables=[\"input\"],\n",
    "    output_variables=[\"out_text\",\"extracted_data\", \"text\"],\n",
    ")\n",
    "\n",
    "class UserRequest(BaseModel):\n",
    "    user_message: str\n",
    "\n",
    "def gernerate_answer(req: UserRequest) -> Dict[str, str]:\n",
    "    context = req.dict()\n",
    "    context[\"input\"] = context[\"user_message\"]\n",
    "    answer = overall.invoke(context)\n",
    "    # answer = multi_prompt_chain.run(context)\n",
    "\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "# 문제가 발생하면 상담사 추천하는 로직을 구현"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T03:44:41.179201Z",
     "start_time": "2024-02-12T03:44:41.144539100Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "### User 데이터 입력\n",
    "* 유저 데이터 입력 후 결과를 확인 합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMRouterChain chain...\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain\\chains\\llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mHuman: [persona]\n",
      "1. 말을 거는 사람은 고객이며, A.I. 항상 고객의 감정을 위로해주거나 공감하는 말을 추천하기전에 우선 합니다.\n",
      "2. 20세 이상의 성인에게는 청소년상담사 자격증을 지닌 상담사 추천하지 않습니다.(이 내용을 출력하지 않습니다.)\n",
      "3. 20세 미만의 경우에는 청소년 상담사를 추천합니다.(이 내용을 출력하지 않습니다.)\n",
      "4. 애인과의 다툼등은 연애문제로 분류하고 연애문제가 주요분야인 상담사를 추천합니다.(이 내용을 출력하지 않습니다.)\n",
      "5. 진로상담은 학생에게만 추천합니다.(이 내용을 출력하지 않습니다.)\n",
      "\n",
      "[상담사목록]\n",
      "\n",
      "나미선\n",
      "별칭: 해피매직\n",
      "대표문구:행복의 기적을 찾아갑니다.\n",
      "자격증: 상담심리사1급\n",
      "연차: 15년\n",
      "주요분야: 부부문제, 이별문제, 자존감, 우울, 불안, 스트레스, 인생고민, 직장문제, 대인관계, 가족문제, 자녀교육, 진로상담,\n",
      "\n",
      "박진주\n",
      "별칭: 소울힐러\n",
      "대표문구: 마음의 H.P를 채워드립니다.\n",
      "자격증: 상담심리사2급\n",
      "연차 :3년\n",
      "주요분야: 연애문제, 이별문제, 재취업\n",
      "\n",
      "이다함\n",
      "별칭: 라이프 트레이너\n",
      "대표문구: 험란한 인생게임을 즐겁게\n",
      "자격증: 청소년 상담사2급\n",
      "연차: 5년\n",
      "주요문제: 진로상담, 우울, 불안, 스트레스,학생 재능발굴\n",
      "\n",
      "[message]\n",
      "저는 30살 이명지입니다. 가족문제로 마음이 슬퍼요. 상담을 받고싶습니다..\n",
      "\n",
      "Answer:\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'answer': {'user_message': '저는 30살 이명지입니다. 가족문제로 마음이 슬퍼요. 상담을 받고싶습니다..',\n  'input': '저는 30살 이명지입니다. 가족문제로 마음이 슬퍼요. 상담을 받고싶습니다..',\n  'out_text': '안녕하세요, 이명지님. 가족문제로 마음이 슬픈 것 같아요. 상담을 받아보시는 것이 좋을 것 같습니다. 저희 상담사 중에서는 부부문제, 이별문제, 자존감, 우울, 불안, 스트레스, 인생고민, 직장문제, 대인관계, 가족문제, 자녀교육에 대한 상담을 주요 분야로 하는 나미선 상담사를 추천해드릴 수 있어요. 어떠신가요?',\n  'extracted_data': [{'cutomer_name': '이명지',\n    'cutomer_feeling': '슬퍼요',\n    'cutomer_age': '30살'}],\n  'text': {'problem': '가족', 'sentiment': '슬픔'}}}"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data = {\n",
    "    # \"user_message\":\"저는 15살 이미려입니다. 마음이 슬퍼요. 상담을 받고싶어요\",\n",
    "    \"user_message\":\"저는 30살 이명지입니다. 가족문제로 마음이 슬퍼요. 상담을 받고싶습니다..\",\n",
    "    # \"user_message\":\"너무 좋은데? 너는 누구니?\",\n",
    "    # \"user_message\":\" 너는 누구니\"\n",
    "    # \"user_message\":\"여자친구와 싸웠어. 어떻게 해야할까?\",\n",
    "    # \"user_message\":\"아들의 학교생활이 걱정돼요. 어떻게 해야할까요?\",\n",
    "    # \"user_message\":\"내 이름은 김지한입니다. 30살이고요.\",\n",
    "    # \"user_message\":\"오늘날씨가 어때요? 제 이름 기억하시나요?\",\n",
    "}\n",
    "request_instance = UserRequest(**user_data)\n",
    "gernerate_answer(request_instance)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-12T04:51:51.940759800Z",
     "start_time": "2024-02-12T04:51:40.680221300Z"
    }
   },
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 현재 고쳐야 할것\n",
    "분기시 대화가 이어지지 않음\n",
    "모든 대화가 기억하고 이어져야함\n",
    "추출된 정보를 저장하고 이용해야함\n",
    "추출된 정보를 이용해 상담사 추천해야함\n",
    "추출된 정보를 이용해 대화를 이어나가야함"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 구현해야 하는 내용\n",
    "1.현재 대화내용 기억 \n",
    "2.과거 대화내용 저장 및 기억\n",
    "3.웹서비스 하에서 대화표현 (한글자씩 나오게 하기)\n",
    "4. 만약에 문제가 발견되면 문제에 따른 상담사 추천하는 로직을 구현\n",
    "5. 이모티콘 활용\n",
    "### 구현 된 내용\n",
    "- 멀티프롬프트 체인을 이용한 분기처리\n",
    "- 추출체인을 이용한 정보추출\n",
    "- 태깅체인을 이용한 태깅처리\n",
    "- 시퀀셜체인을 이용한 체인연결"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}

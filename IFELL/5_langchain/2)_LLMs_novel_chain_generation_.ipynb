{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 드라이브 마운트"
   ],
   "metadata": {
    "id": "Bq_50zTu-dgW"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 패키지 설치"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install openai langchain"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from typing import Dict, List\n",
    "\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from pydantic import BaseModel\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OpenAI API key"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "#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\n",
    "# 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\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prompt chain 준비\n",
    "* 서비스할 내용의 프롬프트 체인을 준비합니다.\n",
    "* 각 프롬프트 체인을 미리 준비해 놓고, 템플릿으로 사용합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TMP_PATH = \"./drive/MyDrive/Colab Notebooks/IFELL/multi_prompt\"\n",
    "P_PATH = \"./drive/MyDrive/Colab Notebooks/IFELL/multi_prompt\"\n",
    "IDEA_P = os.path.join(P_PATH, \"extract_idea.txt\")\n",
    "OUTLINE_P = os.path.join(P_PATH, \"write_outline.txt\")\n",
    "PLOT_P = os.path.join(P_PATH, \"write_plot.txt\")\n",
    "CHAPTER_P = os.path.join(P_PATH, \"write_chapter.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prompt chain 구현\n",
    "* `SequentialChain`을 이용해서 여러개의 chain을 연속적으로 구현할 수 있습니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class UserRequest(BaseModel):\n",
    "    genre: str\n",
    "    characters: List[Dict[str, str]]\n",
    "    text: str\n",
    "\n",
    "\n",
    "def read_prompt_template(file_path: str) -> str:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        prompt_template = f.read()\n",
    "\n",
    "    return prompt_template\n",
    "\n",
    "\n",
    "def create_chain(llm, template_path, output_key):\n",
    "    return LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=ChatPromptTemplate.from_template(\n",
    "            template=read_prompt_template(template_path),\n",
    "        ),\n",
    "        output_key=output_key,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_novel(req: UserRequest) -> Dict[str, str]:\n",
    "    writer_llm = ChatOpenAI(temperature=0.3, max_tokens=500, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    # 아이디어 뽑기 체인 생성\n",
    "    novel_idea_chain = create_chain(writer_llm, IDEA_P, \"novel_idea\")\n",
    "\n",
    "    # 아웃라인 작성 체인 생성\n",
    "    novel_outline_chain = create_chain(\n",
    "        writer_llm, OUTLINE_P, \"novel_outline\"\n",
    "    )\n",
    "    # 플롯 작성 체인 생성\n",
    "    novel_plot_chain = create_chain(writer_llm, PLOT_P, \"novel_plot\")\n",
    "\n",
    "    # 챕터 작성 체인 생성\n",
    "    novel_chapter_chain = create_chain(writer_llm, CHAPTER_P, \"output\")\n",
    "\n",
    "    preprocess_chain = SequentialChain(\n",
    "        chains=[novel_idea_chain,\n",
    "                novel_outline_chain,\n",
    "                novel_plot_chain\n",
    "\n",
    "        ],\n",
    "        input_variables=[\"genre\", \"characters\", \"text\"],\n",
    "        output_variables=[\"novel_idea\", \"novel_outline\", \"novel_plot\"],\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    context = req.dict()\n",
    "    context = preprocess_chain(context)\n",
    "\n",
    "    context[\"novel_chapter\"] = []\n",
    "    for chapter_number in range(1, 3):\n",
    "        context[\"chapter_number\"] = chapter_number\n",
    "        context = novel_chapter_chain(context)\n",
    "        context[\"novel_chapter\"].append(context[\"output\"])\n",
    "\n",
    "    contents = \"\\n\\n\".join(context[\"novel_chapter\"])\n",
    "    return {\"results\": contents}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### User prompt 작성\n",
    "* User가 직접 작성하는 프롬프트를 작성합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "user_data = {\n",
    "    \"genre\": \"판타지\",\n",
    "    \"characters\": [\n",
    "        {\n",
    "            \"name\": \"김철수\",\n",
    "            \"role\": \"주인공\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"이영희\",\n",
    "            \"role\": \"조연\"\n",
    "        }\n",
    "    ],\n",
    "    \"text\": \"날씨가 추워지고 있습니다.\"\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* User Prompt를 입력합니다."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "request_instance = UserRequest(**user_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Text Generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "generate_novel(request_instance)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}

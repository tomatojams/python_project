{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86fc5bb2-017f-434e-8cd6-53ab214a5604",
   "metadata": {
    "id": "86fc5bb2-017f-434e-8cd6-53ab214a5604"
   },
   "source": [
    "# Retrieval-augmented generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5151afed",
   "metadata": {
    "id": "5151afed"
   },
   "source": [
    "## Overview\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "RAG는 비공개 또는 실시간 데이터를 추가하여 학습자의 지식을 보강하는 기법입니다.\n",
    "\n",
    "인공신경망은 광범위한 주제에 대해 추론할 수 있지만, 그 지식은 학습된 특정 시점까지의 공개 데이터로 제한됩니다. 비공개 데이터나 모델의 마감일 이후에 도입된 데이터에 대해 추론할 수 있는 AI 애플리케이션을 구축하려면 모델에 필요한 특정 정보로 모델에 대한 지식을 보강해야 합니다. 적절한 정보를 가져와서 모델 프롬프트에 삽입하는 프로세스를 검색 증강 생성(RAG)이라고 합니다.\n",
    "\n",
    "### What's in this guide?\n",
    "\n",
    "LangChain에는 RAG 애플리케이션을 구축하는 데 도움이 되도록 특별히 설계된 여러 구성 요소가 있습니다. 이러한 구성 요소에 익숙해지기 위해 텍스트 데이터 소스에 대한 간단한 질문-답변 애플리케이션을 구축해 보겠습니다. 특히, 릴리안 웡의 블로그 게시물 [LLM 기반 자율 에이전트](https://lilianweng.github.io/posts/2023-06-23-agent/)를 통해 QA 봇을 구축해 보겠습니다. 그 과정에서 일반적인 QA 아키텍처를 살펴보고, 관련 LangChain 구성 요소에 대해 논의하며, 고급 QA 기술을 위한 추가 리소스를 강조할 것입니다. 또한, 애플리케이션을 추적하고 이해하는 데 LangSmith가 어떻게 도움이 되는지 살펴볼 것입니다. 애플리케이션의 복잡성이 증가함에 따라 LangSmith는 점점 더 유용해질 것입니다.\n",
    "\n",
    "**Note**\n",
    "여기서는 비정형 데이터를 위한 RAG에 초점을 맞춥니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f25cbbd-0938-4e3d-87e4-17a204a03ffb",
   "metadata": {
    "id": "2f25cbbd-0938-4e3d-87e4-17a204a03ffb"
   },
   "source": [
    "## Architecture\n",
    "\n",
    "일반적인 RAG 애플리케이션에는 두 가지 주요 구성 요소가 있습니다:\n",
    "\n",
    "**Indexing**: 소스에서 데이터를 수집하고 인덱싱하기 위한 파이프라인. *이 작업은 보통 오프라인에서 이루어집니다.\n",
    "\n",
    "**Retrieval and generation**: 런타임에 사용자 쿼리를 받아 인덱스에서 관련 데이터를 검색한 다음 이를 모델로 전달하는 실제 RAG 체인.\n",
    "\n",
    "Raw data에서 답변에 이르는 가장 일반적인 전체 시퀀스는 다음과 같습니다:\n",
    "\n",
    "#### Indexing\n",
    "1. **Load**: 먼저 데이터를 로드해야 합니다. 이를 위해 문서 로더를 사용하겠습니다.\n",
    "2. **Split**: 텍스트 분할기는 큰 '문서'를 작은 덩어리로 나눕니다. 큰 덩어리는 검색하기 어렵고 모델의 한정된 컨텍스트 창에서는 검색되지 않으므로 이 기능은 데이터를 색인하고 모델에 전달할 때 유용합니다.\n",
    "3. **Store**: 나중에 검색할 수 있도록 분할을 저장하고 색인할 곳이 필요합니다. 이 작업은 대개 벡터스토어와 임베딩 모델을 사용해 수행합니다.\n",
    "\n",
    "#### Retrieval and generation\n",
    "4. **Retrieve**: 사용자 입력이 주어지면 Retriever를 사용하여 스토리지에서 관련 스플릿을 검색합니다.\n",
    "5. **Generate**: ChatModel / LLM은 질문과 검색된 데이터가 포함된 프롬프트를 사용하여 답변을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487d8d79-5ee9-4aa4-9fdf-cd5f4303e099",
   "metadata": {
    "id": "487d8d79-5ee9-4aa4-9fdf-cd5f4303e099"
   },
   "source": [
    "## Setup\n",
    "\n",
    "### Dependencies\n",
    "\n",
    "이 단계별 안내에서는 OpenAI 채팅 모델과 임베딩, Chroma 벡터 스토어를 사용하지만, 여기에 표시된 모든 내용은 모든 ChatModel 또는 LLM에서 작동합니다, 임베딩, 벡터스토어 또는 retrievers를 사용합니다.\n",
    "\n",
    "필요 패키지 설치:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 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"
   ],
   "metadata": {
    "id": "fVFNBTBUTKet",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:12.518462Z",
     "start_time": "2024-01-30T01:00:12.507536Z"
    }
   },
   "id": "fVFNBTBUTKet",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28d272cd-4e31-40aa-bbb4-0be0a1f49a14",
   "metadata": {
    "id": "28d272cd-4e31-40aa-bbb4-0be0a1f49a14",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5d2f495d-1be6-4079-b879-2062d4aac717",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:17.462254Z",
     "start_time": "2024-01-30T01:00:12.523222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (0.1.4)\r\n",
      "Requirement already satisfied: openai in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (1.10.0)\r\n",
      "Requirement already satisfied: chromadb in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (0.4.22)\r\n",
      "Requirement already satisfied: langchainhub in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (0.1.14)\r\n",
      "Collecting bs4\r\n",
      "  Obtaining dependency information for bs4 from https://files.pythonhosted.org/packages/51/bb/bf7aab772a159614954d84aa832c129624ba6c32faa559dfb200a534e50b/bs4-0.0.2-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\r\n",
      "Requirement already satisfied: tiktoken in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (0.5.2)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (6.0)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (2.0.25)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (3.9.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (4.0.3)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (0.6.3)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (1.33)\r\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (0.0.16)\r\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (0.1.16)\r\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (0.0.83)\r\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (1.23.2)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (2.5.3)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (2.31.0)\r\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (8.2.3)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from openai) (3.5.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from openai) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from openai) (0.26.0)\r\n",
      "Requirement already satisfied: sniffio in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from openai) (1.2.0)\r\n",
      "Requirement already satisfied: tqdm>4 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from openai) (4.66.1)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from openai) (4.9.0)\r\n",
      "Requirement already satisfied: build>=1.0.3 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (1.0.3)\r\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (0.7.3)\r\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (0.109.0)\r\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (0.27.0)\r\n",
      "Requirement already satisfied: posthog>=2.4.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (3.3.3)\r\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (3.4.0)\r\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (1.16.3)\r\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (1.22.0)\r\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (1.22.0)\r\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (0.43b0)\r\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (1.22.0)\r\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (0.15.0)\r\n",
      "Requirement already satisfied: pypika>=0.48.9 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (0.48.9)\r\n",
      "Requirement already satisfied: overrides>=7.3.1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (7.7.0)\r\n",
      "Requirement already satisfied: importlib-resources in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (6.0.1)\r\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (1.60.0)\r\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (4.1.2)\r\n",
      "Requirement already satisfied: typer>=0.9.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (0.9.0)\r\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (29.0.0)\r\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from chromadb) (4.1.0)\r\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchainhub) (2.31.0.20240125)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from bs4) (4.12.2)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from tiktoken) (2023.10.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\r\n",
      "Requirement already satisfied: idna>=2.8 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\r\n",
      "Requirement already satisfied: packaging>=19.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from build>=1.0.3->chromadb) (23.2)\r\n",
      "Requirement already satisfied: pyproject_hooks in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from build>=1.0.3->chromadb) (1.0.0)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from build>=1.0.3->chromadb) (6.0.0)\r\n",
      "Requirement already satisfied: tomli>=1.1.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from build>=1.0.3->chromadb) (2.0.1)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\r\n",
      "Requirement already satisfied: starlette<0.36.0,>=0.35.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from fastapi>=0.95.2->chromadb) (0.35.1)\r\n",
      "Requirement already satisfied: certifi in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (1.15.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\r\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (2.22.0)\r\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (0.58.0)\r\n",
      "Requirement already satisfied: requests-oauthlib in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\r\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\r\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from kubernetes>=28.1.0->chromadb) (2.1.0)\r\n",
      "Requirement already satisfied: coloredlogs in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\r\n",
      "Requirement already satisfied: flatbuffers in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\r\n",
      "Requirement already satisfied: protobuf in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (3.19.6)\r\n",
      "Requirement already satisfied: sympy in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\r\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\r\n",
      "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\r\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\r\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\r\n",
      "Requirement already satisfied: opentelemetry-proto==1.22.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\r\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.43b0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\r\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.43b0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\r\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\r\n",
      "Requirement already satisfied: opentelemetry-util-http==0.43b0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\r\n",
      "Requirement already satisfied: setuptools>=16.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (68.0.0)\r\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.15.0)\r\n",
      "Requirement already satisfied: asgiref~=3.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.7.2)\r\n",
      "Requirement already satisfied: monotonic>=1.5 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.6)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.1.0)\r\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from tokenizers>=0.13.2->chromadb) (0.20.2)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\r\n",
      "Requirement already satisfied: httptools>=0.5.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\r\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\r\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\r\n",
      "Requirement already satisfied: watchfiles>=0.13 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\r\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from beautifulsoup4->bs4) (2.4)\r\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from importlib-resources->chromadb) (3.11.0)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.1)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\r\n",
      "INFO: pip is looking at multiple versions of google-auth to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\r\n",
      "  Obtaining dependency information for google-auth>=1.0.1 from https://files.pythonhosted.org/packages/82/41/7fb855444cead5b2213e053447ce3a0b7bf2c3529c443e0cf75b2f13b405/google_auth-2.27.0-py2.py3-none-any.whl.metadata\r\n",
      "  Downloading google_auth-2.27.0-py2.py3-none-any.whl.metadata (4.7 kB)\r\n",
      "Requirement already satisfied: filelock in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.10.0)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\r\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\r\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.0)\r\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\r\n",
      "Downloading google_auth-2.27.0-py2.py3-none-any.whl (186 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m186.8/186.8 kB\u001B[0m \u001B[31m777.1 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m0:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: google-auth, bs4\r\n",
      "  Attempting uninstall: google-auth\r\n",
      "    Found existing installation: google-auth 2.22.0\r\n",
      "    Uninstalling google-auth-2.22.0:\r\n",
      "      Successfully uninstalled google-auth-2.22.0\r\n",
      "Successfully installed bs4-0.0.2 google-auth-2.27.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain openai chromadb langchainhub bs4 tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef48de-70b6-4f43-8e0b-ab9b84c9c02a",
   "metadata": {
    "id": "51ef48de-70b6-4f43-8e0b-ab9b84c9c02a"
   },
   "source": [
    "`OPENAI_API_KEY` 입력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "143787ca-d8e6-4dc9-8281-4374f4d71720",
   "metadata": {
    "id": "143787ca-d8e6-4dc9-8281-4374f4d71720",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4132faaf-8998-4339-9b28-0a3563b3902d",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:20.346688Z",
     "start_time": "2024-01-30T01:00:17.461833Z"
    }
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6ba684-26cf-4860-904e-a4d51380c134",
   "metadata": {
    "id": "fa6ba684-26cf-4860-904e-a4d51380c134"
   },
   "source": [
    "## Quickstart\n",
    "\n",
    "릴리안 웡의 LLM 기반 자율 에이전트 블로그 게시물을 통해 QA 앱을 구축하고자 한다고 가정해 보겠습니다.\n",
    "\n",
    "이를 위한 간단한 파이프라인을 약 20줄의 코드로 만들 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a913b1-0eea-442a-8a64-ec73333f104b",
   "metadata": {
    "id": "d8a913b1-0eea-442a-8a64-ec73333f104b",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:21.346163Z",
     "start_time": "2024-01-30T01:00:20.299436Z"
    }
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub # prompt examples\n",
    "from langchain.chat_models import ChatOpenAI # LLM\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings # load -> embedding\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "820244ae-74b4-4593-b392-822979dd91b8",
   "metadata": {
    "id": "820244ae-74b4-4593-b392-822979dd91b8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "21d27098-4f31-41a2-bcab-c58638f70abe",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:27.959093Z",
     "start_time": "2024-01-30T01:00:21.348937Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\") # 어디를 크롤링 할건지\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings()) # 임베딩 데이타 ㅅ페이스\n",
    "retriever = vectorstore.as_retriever() # 학습\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\") # 웹프롬프트 만들어진것\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = ( # 리트리버가 매번 돌아감, 실시간으로 업데이트됨.\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "prompt"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCLGxNqkNXMA",
    "outputId": "e1324acd-f258-4ea0-b89c-ed7dae0c3cfb",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:27.977236Z",
     "start_time": "2024-01-30T01:00:27.961424Z"
    }
   },
   "id": "yCLGxNqkNXMA",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d3b0f36-7b56-49c0-8e40-a1aa9ebcbf24",
   "metadata": {
    "id": "0d3b0f36-7b56-49c0-8e40-a1aa9ebcbf24",
    "outputId": "89fc87f3-19f1-47ce-f496-5fbf36fadbb4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:31.347695Z",
     "start_time": "2024-01-30T01:00:27.967883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\"Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through various methods such as using prompting techniques, task-specific instructions, or human inputs. The goal is to make the task more manageable and facilitate the interpretation of the model's thinking process.\""
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cb344e0-c423-400c-a079-964c08e07e32",
   "metadata": {
    "id": "7cb344e0-c423-400c-a079-964c08e07e32",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:31.375736Z",
     "start_time": "2024-01-30T01:00:31.346755Z"
    }
   },
   "outputs": [],
   "source": [
    "# cleanup\n",
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842cf72d-abbc-468e-a2eb-022470347727",
   "metadata": {
    "id": "842cf72d-abbc-468e-a2eb-022470347727"
   },
   "source": [
    "## Detailed walkthrough\n",
    "\n",
    "위의 코드를 단계별로 살펴보고 무슨 일이 일어나고 있는지 실제로 이해해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5daed6",
   "metadata": {
    "id": "ba5daed6"
   },
   "source": [
    "## Step 1. Load\n",
    "\n",
    "먼저 블로그 게시물 콘텐츠를 로드해야 합니다. 이를 위해 소스에서 데이터를 `Documents`로 로드하는 객체인 `DocumentLoader`를 사용할 수 있습니다.  `Documents`는 `page_content`(문자열) 및 `metadata`(딕셔너리) 속성을 가진 객체입니다.\n",
    "\n",
    "이 경우 `urllib`와 `BeautifulSoup`을 사용하여 전달된 웹 URL을 로드하고 구문 분석하여 URL당 하나의 `Document`를 반환하는 `WebBaseLoader`를 사용하겠습니다. 우리는 `bs_kwargs`를 통해 `BeautifulSoup` 구문 분석기에 매개변수를 전달하여 html -> 텍스트 구문 분석을 사용자 정의할 수 있습니다([BeautifulSoup 문서](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup) 참조). 이 경우 클래스가 \"post-content\", \"post-title\" 또는 \"post-header\"인 HTML 태그만 관련이 있으므로 다른 태그는 모두 제거합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf4d5c72",
   "metadata": {
    "id": "cf4d5c72",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:31.739187Z",
     "start_time": "2024-01-30T01:00:31.364666Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    },\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "207f87a3-effa-4457-b013-6d233bc7a088",
   "metadata": {
    "id": "207f87a3-effa-4457-b013-6d233bc7a088",
    "outputId": "08d48209-1939-42c6-cb23-048f3cca8d2f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:31.744876Z",
     "start_time": "2024-01-30T01:00:31.740455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "42824"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[0].page_content) #글자수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52469796-5ce4-4c12-bd2a-a903872dac33",
   "metadata": {
    "id": "52469796-5ce4-4c12-bd2a-a903872dac33",
    "outputId": "36d47f9a-af14-453b-e21d-2e435ffc6cb6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:31.768061Z",
     "start_time": "2024-01-30T01:00:31.745474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      LLM Powered Autonomous Agents\n",
      "    \n",
      "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
      "Agent System Overview#\n",
      "In\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500]) # 500자 블로그"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "maZDGrx3U8W1",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:31.770288Z",
     "start_time": "2024-01-30T01:00:31.750417Z"
    }
   },
   "id": "maZDGrx3U8W1",
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fd2cc9a7",
   "metadata": {
    "id": "fd2cc9a7"
   },
   "source": [
    "## Step 2. Split\n",
    "\n",
    "로드된 문서의 길이가 42,000자가 넘습니다. 이는 많은 모델의 컨텍스트 창에 맞추기에는 너무 깁니다. 또한 컨텍스트 창에 전체 게시물을 넣을 수 있는 모델의 경우에도 경험적으로 모델은 매우 긴 프롬프트에서 관련 컨텍스트를 찾는 데 어려움을 겪습니다.\n",
    "\n",
    "그래서 우리는 'Document'를 임베딩과 벡터 저장을 위해 청크로 분할할 것입니다. 이렇게 하면 런타임에 블로그 게시물에서 가장 관련성이 높은 부분만 검색하는 데 도움이 됩니다.\n",
    "\n",
    "이 경우 문서를 1000자의 청크로 분할하고 청크 간에 200자의 겹침이 있습니다. 겹침은 문장과 관련된 중요한 문맥에서 문장이 분리될 가능성을 줄이는 데 도움이 됩니다. 각 청크가 적절한 크기가 될 때까지 공통 구분 기호(예: 줄 바꿈)를 사용하여 문서를 (재귀적으로) 분할하는 `RecursiveCharacterTextSplitter`를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b11c01d",
   "metadata": {
    "id": "4b11c01d",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:31.823910Z",
     "start_time": "2024-01-30T01:00:31.759320Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True # 잘리는데 겹치게 함\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3741eb67-9caf-40f2-a001-62f49349bff5",
   "metadata": {
    "id": "3741eb67-9caf-40f2-a001-62f49349bff5",
    "outputId": "c6998f22-78b9-402e-86b8-1103c8311fa4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:31.825013Z",
     "start_time": "2024-01-30T01:00:31.765037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "66"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits) # 66개의 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f868d0e5-5670-4d54-b562-f50265e907f4",
   "metadata": {
    "id": "f868d0e5-5670-4d54-b562-f50265e907f4",
    "outputId": "36929c88-4ce6-42cc-f35c-81103d3e5f79",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:31.855429Z",
     "start_time": "2024-01-30T01:00:31.769219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "969"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits[0].page_content) # 하나당 1000개 가량"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c9e5f27-c8e3-4ca7-8a8e-45c5de2901cc",
   "metadata": {
    "id": "5c9e5f27-c8e3-4ca7-8a8e-45c5de2901cc",
    "outputId": "d747fe0d-c841-4f99-a475-9fc23e6bd732",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:31.856602Z",
     "start_time": "2024-01-30T01:00:31.775813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n 'start_index': 7056}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[10].metadata # 어디에서 얼마나 가져왔는지"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46547031-2352-4321-9970-d6ea27285c2e",
   "metadata": {
    "id": "46547031-2352-4321-9970-d6ea27285c2e"
   },
   "source": [
    "## Step 3. Store\n",
    "\n",
    "이제 66개의 텍스트 청크가 메모리에 저장되었으므로 나중에 RAG 앱에서 검색할 수 있도록 이를 저장하고 색인을 생성해야 합니다. 이를 수행하는 가장 일반적인 방법은 각 문서 분할의 내용을 임베드하고 해당 임베딩을 벡터 스토어에 업로드하는 것입니다.\n",
    "\n",
    "그런 다음, 분할을 검색하고 싶을 때 검색 쿼리를 가져와서 임베딩하고 일종의 '유사성' 검색을 수행하여 쿼리 임베딩과 가장 유사한 임베딩을 가진 저장된 분할을 식별합니다. 가장 간단한 유사성 측정은 코사인 유사성으로, 각 임베딩 쌍 사이의 각도의 코사인(매우 높은 차원의 벡터)을 측정합니다.\n",
    "\n",
    "`Chroma` 벡터 저장소(vector store)와 `OpenAIEmbeddings` 모델을 사용하여 단일 명령으로 모든 문서 분할을 임베드하고 저장할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9c302c8",
   "metadata": {
    "id": "e9c302c8",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:33.767451Z",
     "start_time": "2024-01-30T01:00:31.782065Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d64d40-e475-43d9-b64c-925922bb5ef7",
   "metadata": {
    "id": "70d64d40-e475-43d9-b64c-925922bb5ef7"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# 코드로 형식 지정됨\n",
    "```\n",
    "\n",
    "## Step 4. Retrieve (검색)\n",
    "\n",
    "이제 실제 애플리케이션 로직을 작성해 보겠습니다. 사용자가 질문을 하고, 그 질문과 관련된 문서를 검색하고, 검색된 문서와 초기 질문을 모델에 전달하고, 마지막으로 답변을 반환하는 간단한 애플리케이션을 만들고자 합니다.\n",
    "\n",
    "LangChain은 문자열 쿼리가 주어지면 관련 문서를 반환할 수 있는 인덱스를 래핑하는 `Retriever` 인터페이스를 정의합니다. 모든 리트리버는 공통 메서드인 `get_relevant_documents()`(및 그 비동기 변형인 `aget_relevant_documents()`)를 구현합니다.\n",
    "\n",
    "`Retriever`의 가장 일반적인 유형은 벡터 저장소의 유사성 검색 기능을 사용해 검색을 용이하게 하는 `VectorStoreRetriever`입니다. 모든 `VectorStore`는 `Retriever`로 쉽게 변환할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4414df0d-5d43-46d0-85a9-5f47be0dd099",
   "metadata": {
    "id": "4414df0d-5d43-46d0-85a9-5f47be0dd099",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:33.770829Z",
     "start_time": "2024-01-30T01:00:33.767856Z"
    }
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2c26b7d",
   "metadata": {
    "id": "e2c26b7d",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:34.073021Z",
     "start_time": "2024-01-30T01:00:33.772811Z"
    }
   },
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(\n",
    "    \"What are the approaches to Task Decomposition?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8684291d-0f5e-453a-8d3e-ff9feea765d0",
   "metadata": {
    "id": "8684291d-0f5e-453a-8d3e-ff9feea765d0",
    "outputId": "cdb4a204-56d0-410c-cf44-ef2d3f525d92",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:34.087294Z",
     "start_time": "2024-01-30T01:00:34.078682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a5dc074-816d-409a-b005-ab4eddfd76af",
   "metadata": {
    "id": "9a5dc074-816d-409a-b005-ab4eddfd76af",
    "outputId": "784d02c0-4674-456f-ac2a-4231805449f1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:34.090062Z",
     "start_time": "2024-01-30T01:00:34.082492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d6824",
   "metadata": {
    "id": "415d6824"
   },
   "source": [
    "## Step 5. Generate\n",
    "\n",
    "질문을 받고, 관련 문서를 검색하고, 프롬프트를 구성하고, 이를 모델에 전달하고, 출력을 파싱하는 체인으로 이 모든 것을 합쳐 보겠습니다.\n",
    "\n",
    "여기서는 `gpt-3.5-turbo` OpenAI 채팅 모델을 사용하겠지만, 어떤 LangChain 'LLM' 또는 'ChatModel'로 대체할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d34d998c-9abf-4e01-a4ad-06dadfcf131c",
   "metadata": {
    "id": "d34d998c-9abf-4e01-a4ad-06dadfcf131c",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:34.166936Z",
     "start_time": "2024-01-30T01:00:34.088071Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc826723-36fc-45d1-a3ef-df8c2c8471a8",
   "metadata": {
    "id": "bc826723-36fc-45d1-a3ef-df8c2c8471a8"
   },
   "source": [
    "We'll use a prompt for RAG that is checked into the LangChain prompt hub ([here](https://smith.langchain.com/hub/rlm/rag-prompt))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bede955b-9aeb-4fd3-964d-8e43f214ce70",
   "metadata": {
    "id": "bede955b-9aeb-4fd3-964d-8e43f214ce70",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:36.466627Z",
     "start_time": "2024-01-30T01:00:34.123952Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "prompt"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ekfmz2k8PGGU",
    "outputId": "061713f7-22a6-43ad-83cd-7b543f56b523",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:36.468894Z",
     "start_time": "2024-01-30T01:00:36.459279Z"
    }
   },
   "id": "ekfmz2k8PGGU",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11c35354-f275-47ec-9f72-ebd5c23731eb",
   "metadata": {
    "id": "11c35354-f275-47ec-9f72-ebd5c23731eb",
    "outputId": "52caa682-7f26-4a9a-b66a-deed321b860e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:36.470904Z",
     "start_time": "2024-01-30T01:00:36.461289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print( # 프롬프트 형식 출력\n",
    "    prompt.invoke(\n",
    "        {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    "    ).to_string()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9a210-1eee-4054-99d7-9d9ddf7e3593",
   "metadata": {
    "id": "51f9a210-1eee-4054-99d7-9d9ddf7e3593"
   },
   "source": [
    "우리는 체인을 정의하기 위해 [LCEL Runnable](https://python.langchain.com/docs/expression_language/) 프로토콜을 사용하여 다음과 같이 할 수 있습니다.\n",
    "\n",
    "- 컴포넌트와 함수를 투명한 방식으로 파이프하고\n",
    "- 스트리밍, 비동기, 일괄 호출을 바로 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99fa1aec",
   "metadata": {
    "id": "99fa1aec",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:36.477823Z",
     "start_time": "2024-01-30T01:00:36.468404Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = ( # 질문할때마다 실시간으로 검색해서 답변을 찾아내는 특성이 있음\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser() # 답변의 덩어리에서  필요한 답변만 뽑아냄\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8655a152-d7cf-466f-b1bc-fbff9ae2b889",
   "metadata": {
    "id": "8655a152-d7cf-466f-b1bc-fbff9ae2b889",
    "outputId": "31aec819-69ba-4b0a-89e6-3f374332979b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:39.896116Z",
     "start_time": "2024-01-30T01:00:36.473165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for easier interpretation and execution by autonomous agents or models. Task decomposition can be done through various methods, such as using prompting techniques, task-specific instructions, or human inputs."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa82f437",
   "metadata": {
    "id": "fa82f437"
   },
   "source": [
    "#### Customizing the prompt\n",
    "\n",
    "위와 같이 프롬프트 허브에서 프롬프트(예: [이 RAG 프롬프트](https://smith.langchain.com/hub/rlm/rag-prompt))를 로드할 수 있습니다. 프롬프트는 쉽게 사용자 지정할 수도 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4fee704",
   "metadata": {
    "id": "e4fee704",
    "outputId": "80fd7bef-0c38-4266-f1fa-b3310c569d9e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:42.693007Z",
     "start_time": "2024-01-30T01:00:39.865378Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!'"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "# 프롬프트를 개선해서 추가해서 쓸수있음.\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "rag_prompt_custom = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt_custom\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f99b5-80b4-4178-bf30-c1c0a152638f",
   "metadata": {
    "id": "1c2f99b5-80b4-4178-bf30-c1c0a152638f"
   },
   "source": [
    "### Adding sources\n",
    "\n",
    "LCEL을 사용하면 검색된 문서 또는 문서에서 특정 소스 메타데이터를 쉽게 반환할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ded41680-b749-4e2a-9daa-b1165d74783b",
   "metadata": {
    "id": "ded41680-b749-4e2a-9daa-b1165d74783b",
    "outputId": "7fb8c6a7-1565-4279-df17-f6b5d468a145",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:45.376474Z",
     "start_time": "2024-01-30T01:00:42.694376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{'documents': [{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n   'start_index': 1585},\n  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n   'start_index': 2192},\n  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n   'start_index': 17804},\n  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n   'start_index': 17414},\n  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n   'start_index': 29630},\n  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n   'start_index': 19373}],\n 'answer': 'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!'}"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여러소스로 도큐먼트 체인을 만들수있음.\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | rag_prompt_custom\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}\n",
    "\n",
    "rag_chain_with_source.invoke(\"What is Task Decomposition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776ae958-cbdc-4471-8669-c6087436f0b5",
   "metadata": {
    "id": "776ae958-cbdc-4471-8669-c6087436f0b5"
   },
   "source": [
    "### Adding memory\n",
    "\n",
    "과거 사용자 입력을 기억하는 상태 저장 애플리케이션을 만들고 싶다고 가정해 보겠습니다. 이를 지원하기 위해 필요한 작업은 크게 두 가지입니다.\n",
    "1. 과거 메시지를 전달할 수 있는 메시지 플레이스홀더를 체인에 추가합니다.\n",
    "2. 최신 사용자 쿼리를 가져와 채팅 기록의 맥락에서 리트리버에 전달할 수 있는 독립형 질문으로 재형성하는 체인을 추가합니다.\n",
    "\n",
    "2부터 시작하겠습니다. 다음과 같이 `condense question` 체인을 구축할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b685428-8b82-4af1-be4f-7232c5d55b73",
   "metadata": {
    "id": "2b685428-8b82-4af1-be4f-7232c5d55b73",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:45.378586Z",
     "start_time": "2024-01-30T01:00:45.335145Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "condense_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "condense_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", condense_q_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), # 메세지 플레이스 홀더로 이전대화를 기억하게 함. 상호작용가능하게\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "condense_q_chain = condense_q_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46ee9aa1-16f1-4509-8dae-f8c71f4ad47d",
   "metadata": {
    "id": "46ee9aa1-16f1-4509-8dae-f8c71f4ad47d",
    "outputId": "5c3f57a9-e514-4e69-a4f6-e6120ee96da8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:46.586656Z",
     "start_time": "2024-01-30T01:00:45.360146Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'What is the definition of \"large\" in the context of a language model?'"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.messages import AIMessage, HumanMessage\n",
    "\n",
    "condense_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"What is meant by large\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "31ee8481-ce37-41ae-8ca5-62196619d4b3",
   "metadata": {
    "id": "31ee8481-ce37-41ae-8ca5-62196619d4b3",
    "outputId": "0ca165c5-710c-4987-b0e4-2c8bf3d051f0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:47.737231Z",
     "start_time": "2024-01-30T01:00:46.583709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'How do transformer models function in natural language processing?'"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condense_q_chain.invoke(\n",
    "    {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What does LLM stand for?\"),\n",
    "            AIMessage(content=\"Large language model\"),\n",
    "        ],\n",
    "        \"question\": \"How do transformers work\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a47168-4a1f-4e39-bd2d-d5b03609a243",
   "metadata": {
    "id": "42a47168-4a1f-4e39-bd2d-d5b03609a243"
   },
   "source": [
    "이제 전체 QA 체인을 구축할 수 있습니다. 채팅 기록이 비어 있지 않을 때만 `condense question chain`을 실행하도록 라우팅 기능을 추가한 것을 주목하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66f275f3-ddef-4678-b90d-ee64576878f9",
   "metadata": {
    "id": "66f275f3-ddef-4678-b90d-ee64576878f9",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:47.907692Z",
     "start_time": "2024-01-30T01:00:47.745435Z"
    }
   },
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def condense_question(input: dict):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return condense_q_chain\n",
    "    else:\n",
    "        return input[\"question\"]\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    RunnablePassthrough.assign(context=condense_question | retriever | format_docs)\n",
    "    | qa_prompt\n",
    "    | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51fd0e54-5bb4-4a9a-b012-87a18ebe2bef",
   "metadata": {
    "id": "51fd0e54-5bb4-4a9a-b012-87a18ebe2bef",
    "outputId": "93fb7c8f-8642-4448-b2ed-270e0c6792db",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-01-30T01:00:59.551645Z",
     "start_time": "2024-01-30T01:00:47.767793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='Common ways of task decomposition include:\\n\\n1. Using Chain of Thought (CoT): CoT is a prompting technique that instructs a model to \"think step by step\" and decompose complex tasks into smaller and simpler steps. This approach utilizes more computation at test-time and helps in interpreting the model\\'s thinking process.\\n\\n2. Using task-specific instructions: Providing specific instructions tailored to the task at hand can help in breaking it down into manageable subtasks. For example, for writing a novel, an instruction like \"Write a story outline\" can guide the task decomposition process.\\n\\n3. Human inputs: In some cases, human inputs are used to decompose tasks. Humans can provide insights, expertise, and domain knowledge to identify the subgoals or steps required to achieve a particular task.\\n\\nThese approaches aim to simplify complex tasks and enable more effective problem-solving and planning.')"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg])\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 연습문제\n",
    "### 1. WebBaseLoader 기반 QA\n",
    "* 주어진 사이트의 정보를 이용해 QA Agent를 구성해보자\n",
    "\n",
    "[https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/)"
   ],
   "metadata": {
    "id": "0I1PE4GaGmjs"
   },
   "id": "0I1PE4GaGmjs"
  },
  {
   "cell_type": "code",
   "source": [
    "# 예제\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\") # 어디를 크롤링 할건지\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings()) # 임베딩 데이타 ㅅ페이스\n",
    "retriever = vectorstore.as_retriever() # 학습\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\") # 웹프롬프트 만들어진것\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = ( # 리트리버가 매번 돌아감, 실시간으로 업데이트됨.\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ],
   "metadata": {
    "id": "e0GPRH1bS95U",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:01:03.664066Z",
     "start_time": "2024-01-30T01:00:59.551360Z"
    }
   },
   "id": "e0GPRH1bS95U",
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",),\n",
    "    bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\",\"token-manipulation\")\n",
    "        )\n",
    "    },\n",
    ")\n",
    "docs = loader.load()"
   ],
   "metadata": {
    "id": "mHnZyNdepjFg",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:01:04.371129Z",
     "start_time": "2024-01-30T01:01:03.669361Z"
    }
   },
   "id": "mHnZyNdepjFg",
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "len(docs[0].page_content) #글자수"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTHCtigMUuSR",
    "outputId": "f862fb42-7a5c-45c2-c3ef-1088ff02f9fa",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:01:04.392477Z",
     "start_time": "2024-01-30T01:01:04.372074Z"
    }
   },
   "id": "lTHCtigMUuSR",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "48603"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(docs[0].page_content[:500])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n7DwU9SxU93j",
    "outputId": "c73fc150-4d57-428b-9585-079375196967",
    "ExecuteTime": {
     "end_time": "2024-01-30T01:01:04.393583Z",
     "start_time": "2024-01-30T01:01:04.376762Z"
    }
   },
   "id": "n7DwU9SxU93j",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      Adversarial Attacks on LLMs\n",
      "    \n",
      "Date: October 25, 2023  |  Estimated Reading Time: 33 min  |  Author: Lilian Weng\n",
      "\n",
      "\n",
      "The use of large language models in the real world has strongly accelerated by the launch of ChatGPT. We (including my team at OpenAI, shoutout to them) have invested a lot of effort to build default safe behavior into the model during the alignment process (e.g. via RLHF). However, adversarial attacks or jailbreak prompts could potentially trigger the model to output some\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

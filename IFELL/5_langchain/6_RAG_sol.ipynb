{"cells":[{"cell_type":"markdown","id":"86fc5bb2-017f-434e-8cd6-53ab214a5604","metadata":{"id":"86fc5bb2-017f-434e-8cd6-53ab214a5604"},"source":["# Retrieval-augmented generation (RAG)"]},{"cell_type":"markdown","id":"5151afed","metadata":{"id":"5151afed"},"source":["## Overview\n","\n","### What is RAG?\n","\n","RAG는 비공개 또는 실시간 데이터를 추가하여 학습자의 지식을 보강하는 기법입니다.\n","\n","인공신경망은 광범위한 주제에 대해 추론할 수 있지만, 그 지식은 학습된 특정 시점까지의 공개 데이터로 제한됩니다. 비공개 데이터나 모델의 마감일 이후에 도입된 데이터에 대해 추론할 수 있는 AI 애플리케이션을 구축하려면 모델에 필요한 특정 정보로 모델에 대한 지식을 보강해야 합니다. 적절한 정보를 가져와서 모델 프롬프트에 삽입하는 프로세스를 검색 증강 생성(RAG)이라고 합니다.\n","\n","### What's in this guide?\n","\n","LangChain에는 RAG 애플리케이션을 구축하는 데 도움이 되도록 특별히 설계된 여러 구성 요소가 있습니다. 이러한 구성 요소에 익숙해지기 위해 텍스트 데이터 소스에 대한 간단한 질문-답변 애플리케이션을 구축해 보겠습니다. 특히, 릴리안 웡의 블로그 게시물 [LLM 기반 자율 에이전트](https://lilianweng.github.io/posts/2023-06-23-agent/)를 통해 QA 봇을 구축해 보겠습니다. 그 과정에서 일반적인 QA 아키텍처를 살펴보고, 관련 LangChain 구성 요소에 대해 논의하며, 고급 QA 기술을 위한 추가 리소스를 강조할 것입니다. 또한, 애플리케이션을 추적하고 이해하는 데 LangSmith가 어떻게 도움이 되는지 살펴볼 것입니다. 애플리케이션의 복잡성이 증가함에 따라 LangSmith는 점점 더 유용해질 것입니다.\n","\n","**Note**\n","여기서는 비정형 데이터를 위한 RAG에 초점을 맞춥니다."]},{"cell_type":"markdown","id":"2f25cbbd-0938-4e3d-87e4-17a204a03ffb","metadata":{"id":"2f25cbbd-0938-4e3d-87e4-17a204a03ffb"},"source":["## Architecture\n","\n","일반적인 RAG 애플리케이션에는 두 가지 주요 구성 요소가 있습니다:\n","\n","**Indexing**: 소스에서 데이터를 수집하고 인덱싱하기 위한 파이프라인. *이 작업은 보통 오프라인에서 이루어집니다.\n","\n","**Retrieval and generation**: 런타임에 사용자 쿼리를 받아 인덱스에서 관련 데이터를 검색한 다음 이를 모델로 전달하는 실제 RAG 체인.\n","\n","Raw data에서 답변에 이르는 가장 일반적인 전체 시퀀스는 다음과 같습니다:\n","\n","#### Indexing\n","1. **Load**: 먼저 데이터를 로드해야 합니다. 이를 위해 문서 로더를 사용하겠습니다.\n","2. **Split**: 텍스트 분할기는 큰 '문서'를 작은 덩어리로 나눕니다. 큰 덩어리는 검색하기 어렵고 모델의 한정된 컨텍스트 창에서는 검색되지 않으므로 이 기능은 데이터를 색인하고 모델에 전달할 때 유용합니다.\n","3. **Store**: 나중에 검색할 수 있도록 분할을 저장하고 색인할 곳이 필요합니다. 이 작업은 대개 벡터스토어와 임베딩 모델을 사용해 수행합니다.\n","\n","#### Retrieval and generation\n","4. **Retrieve**: 사용자 입력이 주어지면 Retriever를 사용하여 스토리지에서 관련 스플릿을 검색합니다.\n","5. **Generate**: ChatModel / LLM은 질문과 검색된 데이터가 포함된 프롬프트를 사용하여 답변을 생성합니다."]},{"cell_type":"markdown","id":"487d8d79-5ee9-4aa4-9fdf-cd5f4303e099","metadata":{"id":"487d8d79-5ee9-4aa4-9fdf-cd5f4303e099"},"source":["## Setup\n","\n","### Dependencies\n","\n","이 단계별 안내에서는 OpenAI 채팅 모델과 임베딩, Chroma 벡터 스토어를 사용하지만, 여기에 표시된 모든 내용은 모든 ChatModel 또는 LLM에서 작동합니다, 임베딩, 벡터스토어 또는 retrievers를 사용합니다.\n","\n","필요 패키지 설치:"]},{"cell_type":"code","execution_count":null,"id":"28d272cd-4e31-40aa-bbb4-0be0a1f49a14","metadata":{"id":"28d272cd-4e31-40aa-bbb4-0be0a1f49a14","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702015083352,"user_tz":-540,"elapsed":43238,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}},"outputId":"f71f2a01-a7d5-46bf-ea8a-4d5fb45f44b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain\n","  Downloading langchain-0.0.348-py3-none-any.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting openai\n","  Downloading openai-1.3.7-py3-none-any.whl (221 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting chromadb\n","  Downloading chromadb-0.4.18-py3-none-any.whl (502 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.4/502.4 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchainhub\n","  Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n","Collecting bs4\n","  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tiktoken\n","  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n","  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Collecting langchain-core<0.1,>=0.0.12 (from langchain)\n","  Downloading langchain_core-0.0.12-py3-none-any.whl (181 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langsmith<0.1.0,>=0.0.63 (from langchain)\n","  Downloading langsmith-0.0.69-py3-none-any.whl (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n","Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n","Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n","Collecting chroma-hnswlib==0.7.3 (from chromadb)\n","  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n","  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n","  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n","  Downloading posthog-3.1.0-py2.py3-none-any.whl (37 kB)\n","Collecting pulsar-client>=3.1.0 (from chromadb)\n","  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n","  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_api-1.21.0-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl (18 kB)\n","Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n","  Downloading opentelemetry_instrumentation_fastapi-0.42b0-py3-none-any.whl (11 kB)\n","Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n","  Downloading opentelemetry_sdk-1.21.0-py3-none-any.whl (105 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n","Collecting pypika>=0.48.9 (from chromadb)\n","  Downloading PyPika-0.48.9.tar.gz (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting overrides>=7.3.1 (from chromadb)\n","  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n","Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.59.3)\n","Collecting bcrypt>=4.0.1 (from chromadb)\n","  Downloading bcrypt-4.1.1-cp37-abi3-manylinux_2_28_x86_64.whl (699 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m699.4/699.4 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n","Collecting kubernetes>=28.1.0 (from chromadb)\n","  Downloading kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting mmh3>=4.0.1 (from chromadb)\n","  Downloading mmh3-4.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (72 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.6/72.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n","  Downloading types_requests-2.31.0.10-py3-none-any.whl (14 kB)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.6)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n","  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n","  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-extensions<5,>=4.5 (from openai)\n","  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n","  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n","Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n","Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.6.4)\n","Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n","Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n","Collecting urllib3<2.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n","  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.1,>=0.0.12->langchain) (23.2)\n","Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n","Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (6.8.0)\n","Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.61.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl (17 kB)\n","Collecting opentelemetry-proto==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n","  Downloading opentelemetry_proto-1.21.0-py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation_asgi-0.42b0-py3-none-any.whl (13 kB)\n","Collecting opentelemetry-instrumentation==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_instrumentation-0.42b0-py3-none-any.whl (25 kB)\n","Collecting opentelemetry-semantic-conventions==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl (36 kB)\n","Collecting opentelemetry-util-http==0.42b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading opentelemetry_util_http-0.42b0-py3-none-any.whl (6.9 kB)\n","Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n","Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n","Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.42b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n","  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n","Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n","Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.19.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n","INFO: pip is looking at multiple versions of types-requests to determine which version is compatible with other requirements. This could take a while.\n","Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n","  Downloading types_requests-2.31.0.9-py3-none-any.whl (14 kB)\n","  Downloading types_requests-2.31.0.8-py3-none-any.whl (14 kB)\n","  Downloading types_requests-2.31.0.7-py3-none-any.whl (14 kB)\n","  Downloading types_requests-2.31.0.6-py3-none-any.whl (14 kB)\n","Collecting types-urllib3 (from types-requests<3.0.0.0,>=2.31.0.2->langchainhub)\n","  Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n","Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n","Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n","  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n","Building wheels for collected packages: bs4, pypika\n","  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=480d019a62f4d564e2877b3193b23694ecc306d315a5b195fd5632558533deca\n","  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n","  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=52757e98e33f8d630cd6bcabec8e17661e87c073af158015b2581c54ac8367bd\n","  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n","Successfully built bs4 pypika\n","Installing collected packages: types-urllib3, pypika, monotonic, mmh3, websockets, uvloop, urllib3, typing-extensions, types-requests, python-dotenv, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, mypy-extensions, marshmallow, jsonpointer, humanfriendly, httptools, h11, deprecated, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, typing-inspect, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, jsonpatch, httpcore, coloredlogs, bs4, asgiref, tiktoken, posthog, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, langsmith, langchainhub, httpx, fastapi, dataclasses-json, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, openai, langchain-core, kubernetes, opentelemetry-instrumentation-fastapi, langchain, chromadb\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.0.7\n","    Uninstalling urllib3-2.0.7:\n","      Successfully uninstalled urllib3-2.0.7\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.5.0\n","    Uninstalling typing_extensions-4.5.0:\n","      Successfully uninstalled typing_extensions-4.5.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.1 bs4-0.0.1 chroma-hnswlib-0.7.3 chromadb-0.4.18 coloredlogs-15.0.1 dataclasses-json-0.6.3 deprecated-1.2.14 fastapi-0.104.1 h11-0.14.0 httpcore-1.0.2 httptools-0.6.1 httpx-0.25.2 humanfriendly-10.0 jsonpatch-1.33 jsonpointer-2.4 kubernetes-28.1.0 langchain-0.0.348 langchain-core-0.0.12 langchainhub-0.1.14 langsmith-0.0.69 marshmallow-3.20.1 mmh3-4.0.1 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.16.3 openai-1.3.7 opentelemetry-api-1.21.0 opentelemetry-exporter-otlp-proto-common-1.21.0 opentelemetry-exporter-otlp-proto-grpc-1.21.0 opentelemetry-instrumentation-0.42b0 opentelemetry-instrumentation-asgi-0.42b0 opentelemetry-instrumentation-fastapi-0.42b0 opentelemetry-proto-1.21.0 opentelemetry-sdk-1.21.0 opentelemetry-semantic-conventions-0.42b0 opentelemetry-util-http-0.42b0 overrides-7.4.0 posthog-3.1.0 pulsar-client-3.3.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 tiktoken-0.5.2 types-requests-2.31.0.6 types-urllib3-1.26.25.14 typing-extensions-4.8.0 typing-inspect-0.9.0 urllib3-1.26.18 uvicorn-0.24.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"]}],"source":["!pip install -U langchain openai chromadb langchainhub bs4 tiktoken"]},{"cell_type":"markdown","id":"51ef48de-70b6-4f43-8e0b-ab9b84c9c02a","metadata":{"id":"51ef48de-70b6-4f43-8e0b-ab9b84c9c02a"},"source":["`OPENAI_API_KEY` 입력하기"]},{"cell_type":"code","execution_count":null,"id":"143787ca-d8e6-4dc9-8281-4374f4d71720","metadata":{"id":"143787ca-d8e6-4dc9-8281-4374f4d71720","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702015093624,"user_tz":-540,"elapsed":10287,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}},"outputId":"885f519c-d6e3-4704-e5ce-6ae35af76899"},"outputs":[{"name":"stdout","output_type":"stream","text":["··········\n"]}],"source":["import getpass\n","import os\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"]},{"cell_type":"markdown","id":"fa6ba684-26cf-4860-904e-a4d51380c134","metadata":{"id":"fa6ba684-26cf-4860-904e-a4d51380c134"},"source":["## Quickstart\n","\n","릴리안 웡의 LLM 기반 자율 에이전트 블로그 게시물을 통해 QA 앱을 구축하고자 한다고 가정해 보겠습니다.\n","\n","이를 위한 간단한 파이프라인을 약 20줄의 코드로 만들 수 있습니다:"]},{"cell_type":"code","execution_count":null,"id":"d8a913b1-0eea-442a-8a64-ec73333f104b","metadata":{"id":"d8a913b1-0eea-442a-8a64-ec73333f104b"},"outputs":[],"source":["import bs4\n","from langchain import hub\n","from langchain.chat_models import ChatOpenAI\n","from langchain.document_loaders import WebBaseLoader\n","from langchain.embeddings import OpenAIEmbeddings\n","from langchain.schema import StrOutputParser\n","from langchain.schema.runnable import RunnablePassthrough\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.vectorstores import Chroma"]},{"cell_type":"code","execution_count":null,"id":"820244ae-74b4-4593-b392-822979dd91b8","metadata":{"id":"820244ae-74b4-4593-b392-822979dd91b8"},"outputs":[],"source":["loader = WebBaseLoader(\n","    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n","    bs_kwargs=dict(\n","        parse_only=bs4.SoupStrainer(\n","            class_=(\"post-content\", \"post-title\", \"post-header\")\n","        )\n","    ),\n",")\n","docs = loader.load()\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","splits = text_splitter.split_documents(docs)\n","\n","vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n","retriever = vectorstore.as_retriever()\n","\n","prompt = hub.pull(\"rlm/rag-prompt\")\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","\n","rag_chain = (\n","    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")"]},{"cell_type":"code","execution_count":null,"id":"0d3b0f36-7b56-49c0-8e40-a1aa9ebcbf24","metadata":{"id":"0d3b0f36-7b56-49c0-8e40-a1aa9ebcbf24","outputId":"97aa8d31-b72a-47f5-a95e-aec7a40a9867","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1702015103785,"user_tz":-540,"elapsed":2442,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through various methods such as using prompting techniques, task-specific instructions, or human inputs. The goal is to make the task more manageable and facilitate the interpretation of the model's thinking process.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}],"source":["rag_chain.invoke(\"What is Task Decomposition?\")"]},{"cell_type":"code","execution_count":null,"id":"7cb344e0-c423-400c-a079-964c08e07e32","metadata":{"id":"7cb344e0-c423-400c-a079-964c08e07e32"},"outputs":[],"source":["# cleanup\n","vectorstore.delete_collection()"]},{"cell_type":"markdown","id":"842cf72d-abbc-468e-a2eb-022470347727","metadata":{"id":"842cf72d-abbc-468e-a2eb-022470347727"},"source":["## Detailed walkthrough\n","\n","위의 코드를 단계별로 살펴보고 무슨 일이 일어나고 있는지 실제로 이해해 보겠습니다."]},{"cell_type":"markdown","id":"ba5daed6","metadata":{"id":"ba5daed6"},"source":["## Step 1. Load\n","\n","먼저 블로그 게시물 콘텐츠를 로드해야 합니다. 이를 위해 소스에서 데이터를 `Documents`로 로드하는 객체인 `DocumentLoader`를 사용할 수 있습니다.  `Documents`는 `page_content`(문자열) 및 `metadata`(딕셔너리) 속성을 가진 객체입니다.\n","\n","이 경우 `urllib`와 `BeautifulSoup`을 사용하여 전달된 웹 URL을 로드하고 구문 분석하여 URL당 하나의 `Document`를 반환하는 `WebBaseLoader`를 사용하겠습니다. 우리는 `bs_kwargs`를 통해 `BeautifulSoup` 구문 분석기에 매개변수를 전달하여 html -> 텍스트 구문 분석을 사용자 정의할 수 있습니다([BeautifulSoup 문서](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup) 참조). 이 경우 클래스가 \"post-content\", \"post-title\" 또는 \"post-header\"인 HTML 태그만 관련이 있으므로 다른 태그는 모두 제거합니다."]},{"cell_type":"code","execution_count":null,"id":"cf4d5c72","metadata":{"id":"cf4d5c72"},"outputs":[],"source":["from langchain.document_loaders import WebBaseLoader\n","\n","loader = WebBaseLoader(\n","    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n","    bs_kwargs={\n","        \"parse_only\": bs4.SoupStrainer(\n","            class_=(\"post-content\", \"post-title\", \"post-header\")\n","        )\n","    },\n",")\n","docs = loader.load()"]},{"cell_type":"code","execution_count":null,"id":"207f87a3-effa-4457-b013-6d233bc7a088","metadata":{"id":"207f87a3-effa-4457-b013-6d233bc7a088","outputId":"1afd6e54-aa61-4ac0-ed31-fe1844412d67","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702015104942,"user_tz":-540,"elapsed":4,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["42824"]},"metadata":{},"execution_count":8}],"source":["len(docs[0].page_content)"]},{"cell_type":"code","execution_count":null,"id":"52469796-5ce4-4c12-bd2a-a903872dac33","metadata":{"id":"52469796-5ce4-4c12-bd2a-a903872dac33","outputId":"29bd477c-1cd5-484a-ab29-29c1a0c880cc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702015104942,"user_tz":-540,"elapsed":3,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","      LLM Powered Autonomous Agents\n","    \n","Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n","\n","\n","Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n","Agent System Overview#\n","In\n"]}],"source":["print(docs[0].page_content[:500])"]},{"cell_type":"markdown","id":"fd2cc9a7","metadata":{"id":"fd2cc9a7"},"source":["## Step 2. Split\n","\n","로드된 문서의 길이가 42,000자가 넘습니다. 이는 많은 모델의 컨텍스트 창에 맞추기에는 너무 깁니다. 또한 컨텍스트 창에 전체 게시물을 넣을 수 있는 모델의 경우에도 경험적으로 모델은 매우 긴 프롬프트에서 관련 컨텍스트를 찾는 데 어려움을 겪습니다.\n","\n","그래서 우리는 'Document'를 임베딩과 벡터 저장을 위해 청크로 분할할 것입니다. 이렇게 하면 런타임에 블로그 게시물에서 가장 관련성이 높은 부분만 검색하는 데 도움이 됩니다.\n","\n","이 경우 문서를 1000자의 청크로 분할하고 청크 간에 200자의 겹침이 있습니다. 겹침은 문장과 관련된 중요한 문맥에서 문장이 분리될 가능성을 줄이는 데 도움이 됩니다. 각 청크가 적절한 크기가 될 때까지 공통 구분 기호(예: 줄 바꿈)를 사용하여 문서를 (재귀적으로) 분할하는 `RecursiveCharacterTextSplitter`를 사용합니다."]},{"cell_type":"code","execution_count":null,"id":"4b11c01d","metadata":{"id":"4b11c01d"},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",")\n","all_splits = text_splitter.split_documents(docs)"]},{"cell_type":"code","execution_count":null,"id":"3741eb67-9caf-40f2-a001-62f49349bff5","metadata":{"id":"3741eb67-9caf-40f2-a001-62f49349bff5","outputId":"1092287d-95b4-4840-de81-b829431b9c18","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702015104942,"user_tz":-540,"elapsed":2,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["66"]},"metadata":{},"execution_count":11}],"source":["len(all_splits)"]},{"cell_type":"code","execution_count":null,"id":"f868d0e5-5670-4d54-b562-f50265e907f4","metadata":{"id":"f868d0e5-5670-4d54-b562-f50265e907f4","outputId":"ce57349e-b809-4226-ee01-94b6b14c36c1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702015105411,"user_tz":-540,"elapsed":471,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["969"]},"metadata":{},"execution_count":12}],"source":["len(all_splits[0].page_content)"]},{"cell_type":"code","execution_count":null,"id":"5c9e5f27-c8e3-4ca7-8a8e-45c5de2901cc","metadata":{"id":"5c9e5f27-c8e3-4ca7-8a8e-45c5de2901cc","outputId":"c57f5658-870a-4e34-8d80-66bfc5130ac6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702015105411,"user_tz":-540,"elapsed":2,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n"," 'start_index': 7056}"]},"metadata":{},"execution_count":13}],"source":["all_splits[10].metadata"]},{"cell_type":"markdown","id":"46547031-2352-4321-9970-d6ea27285c2e","metadata":{"id":"46547031-2352-4321-9970-d6ea27285c2e"},"source":["## Step 3. Store\n","\n","이제 66개의 텍스트 청크가 메모리에 저장되었으므로 나중에 RAG 앱에서 검색할 수 있도록 이를 저장하고 색인을 생성해야 합니다. 이를 수행하는 가장 일반적인 방법은 각 문서 분할의 내용을 임베드하고 해당 임베딩을 벡터 스토어에 업로드하는 것입니다.\n","\n","그런 다음, 분할을 검색하고 싶을 때 검색 쿼리를 가져와서 임베딩하고 일종의 '유사성' 검색을 수행하여 쿼리 임베딩과 가장 유사한 임베딩을 가진 저장된 분할을 식별합니다. 가장 간단한 유사성 측정은 코사인 유사성으로, 각 임베딩 쌍 사이의 각도의 코사인(매우 높은 차원의 벡터)을 측정합니다.\n","\n","`Chroma` 벡터 저장소(vector store)와 `OpenAIEmbeddings` 모델을 사용하여 단일 명령으로 모든 문서 분할을 임베드하고 저장할 수 있습니다."]},{"cell_type":"code","execution_count":null,"id":"e9c302c8","metadata":{"id":"e9c302c8"},"outputs":[],"source":["from langchain.embeddings import OpenAIEmbeddings\n","from langchain.vectorstores import Chroma\n","\n","vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"]},{"cell_type":"markdown","id":"70d64d40-e475-43d9-b64c-925922bb5ef7","metadata":{"id":"70d64d40-e475-43d9-b64c-925922bb5ef7"},"source":["## Step 4. Retrieve\n","\n","이제 실제 애플리케이션 로직을 작성해 보겠습니다. 사용자가 질문을 하고, 그 질문과 관련된 문서를 검색하고, 검색된 문서와 초기 질문을 모델에 전달하고, 마지막으로 답변을 반환하는 간단한 애플리케이션을 만들고자 합니다.\n","\n","LangChain은 문자열 쿼리가 주어지면 관련 문서를 반환할 수 있는 인덱스를 래핑하는 `Retriever` 인터페이스를 정의합니다. 모든 리트리버는 공통 메서드인 `get_relevant_documents()`(및 그 비동기 변형인 `aget_relevant_documents()`)를 구현합니다.\n","\n","`Retriever`의 가장 일반적인 유형은 벡터 저장소의 유사성 검색 기능을 사용해 검색을 용이하게 하는 `VectorStoreRetriever`입니다. 모든 `VectorStore`는 `Retriever`로 쉽게 변환할 수 있습니다:"]},{"cell_type":"code","execution_count":null,"id":"4414df0d-5d43-46d0-85a9-5f47be0dd099","metadata":{"id":"4414df0d-5d43-46d0-85a9-5f47be0dd099"},"outputs":[],"source":["retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"]},{"cell_type":"code","execution_count":null,"id":"e2c26b7d","metadata":{"id":"e2c26b7d"},"outputs":[],"source":["retrieved_docs = retriever.get_relevant_documents(\n","    \"What are the approaches to Task Decomposition?\"\n",")"]},{"cell_type":"code","execution_count":null,"id":"8684291d-0f5e-453a-8d3e-ff9feea765d0","metadata":{"id":"8684291d-0f5e-453a-8d3e-ff9feea765d0","outputId":"921b5765-c452-479f-f02f-2cc114816dd3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702015107824,"user_tz":-540,"elapsed":3,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{},"execution_count":17}],"source":["len(retrieved_docs)"]},{"cell_type":"code","execution_count":null,"id":"9a5dc074-816d-409a-b005-ab4eddfd76af","metadata":{"id":"9a5dc074-816d-409a-b005-ab4eddfd76af","outputId":"c5c5d057-32a1-43b1-edc5-d536cfa41c77","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702015107824,"user_tz":-540,"elapsed":3,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n","Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"]}],"source":["print(retrieved_docs[0].page_content)"]},{"cell_type":"markdown","id":"415d6824","metadata":{"id":"415d6824"},"source":["## Step 5. Generate\n","\n","질문을 받고, 관련 문서를 검색하고, 프롬프트를 구성하고, 이를 모델에 전달하고, 출력을 파싱하는 체인으로 이 모든 것을 합쳐 보겠습니다.\n","\n","여기서는 `gpt-3.5-turbo` OpenAI 채팅 모델을 사용하겠지만, 어떤 LangChain 'LLM' 또는 'ChatModel'로 대체할 수 있습니다."]},{"cell_type":"code","execution_count":null,"id":"d34d998c-9abf-4e01-a4ad-06dadfcf131c","metadata":{"id":"d34d998c-9abf-4e01-a4ad-06dadfcf131c"},"outputs":[],"source":["from langchain.chat_models import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"]},{"cell_type":"markdown","id":"bc826723-36fc-45d1-a3ef-df8c2c8471a8","metadata":{"id":"bc826723-36fc-45d1-a3ef-df8c2c8471a8"},"source":["We'll use a prompt for RAG that is checked into the LangChain prompt hub ([here](https://smith.langchain.com/hub/rlm/rag-prompt))."]},{"cell_type":"code","execution_count":null,"id":"bede955b-9aeb-4fd3-964d-8e43f214ce70","metadata":{"id":"bede955b-9aeb-4fd3-964d-8e43f214ce70"},"outputs":[],"source":["from langchain import hub\n","\n","prompt = hub.pull(\"rlm/rag-prompt\")"]},{"cell_type":"code","execution_count":null,"id":"11c35354-f275-47ec-9f72-ebd5c23731eb","metadata":{"id":"11c35354-f275-47ec-9f72-ebd5c23731eb","outputId":"0d067c37-3cbe-4964-c7fb-f794e68d765b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702015109213,"user_tz":-540,"elapsed":3,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n","Question: filler question \n","Context: filler context \n","Answer:\n"]}],"source":["print(\n","    prompt.invoke(\n","        {\"context\": \"filler context\", \"question\": \"filler question\"}\n","    ).to_string()\n",")"]},{"cell_type":"markdown","id":"51f9a210-1eee-4054-99d7-9d9ddf7e3593","metadata":{"id":"51f9a210-1eee-4054-99d7-9d9ddf7e3593"},"source":["우리는 체인을 정의하기 위해 [LCEL Runnable](https://python.langchain.com/docs/expression_language/) 프로토콜을 사용하여 다음과 같이 할 수 있습니다.\n","\n","- 컴포넌트와 함수를 투명한 방식으로 파이프하고\n","- 스트리밍, 비동기, 일괄 호출을 바로 사용할 수 있습니다."]},{"cell_type":"code","execution_count":null,"id":"99fa1aec","metadata":{"id":"99fa1aec"},"outputs":[],"source":["from langchain.schema import StrOutputParser\n","from langchain.schema.runnable import RunnablePassthrough\n","\n","\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","\n","rag_chain = (\n","    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")"]},{"cell_type":"code","execution_count":null,"id":"8655a152-d7cf-466f-b1bc-fbff9ae2b889","metadata":{"id":"8655a152-d7cf-466f-b1bc-fbff9ae2b889","outputId":"30a1d363-9f1c-435c-d19d-8926b0a21df2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702015111421,"user_tz":-540,"elapsed":2210,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through methods like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the task into manageable subtasks and exploring multiple reasoning possibilities at each step. Task decomposition can be performed by AI models with prompting, task-specific instructions, or human inputs."]}],"source":["for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n","    print(chunk, end=\"\", flush=True)"]},{"cell_type":"markdown","id":"fa82f437","metadata":{"id":"fa82f437"},"source":["#### Customizing the prompt\n","\n","위와 같이 프롬프트 허브에서 프롬프트(예: [이 RAG 프롬프트](https://smith.langchain.com/hub/rlm/rag-prompt))를 로드할 수 있습니다. 프롬프트는 쉽게 사용자 지정할 수도 있습니다:"]},{"cell_type":"code","execution_count":null,"id":"e4fee704","metadata":{"id":"e4fee704","outputId":"2e9f8ffb-afa4-4e67-d06c-2a3354c67606","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1702015113227,"user_tz":-540,"elapsed":1809,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}],"source":["from langchain.prompts import PromptTemplate\n","\n","template = \"\"\"Use the following pieces of context to answer the question at the end.\n","If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","Use three sentences maximum and keep the answer as concise as possible.\n","Always say \"thanks for asking!\" at the end of the answer.\n","{context}\n","Question: {question}\n","Helpful Answer:\"\"\"\n","rag_prompt_custom = PromptTemplate.from_template(template)\n","\n","rag_chain = (\n","    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","    | rag_prompt_custom\n","    | llm\n","    | StrOutputParser()\n",")\n","\n","rag_chain.invoke(\"What is Task Decomposition?\")"]},{"cell_type":"markdown","id":"1c2f99b5-80b4-4178-bf30-c1c0a152638f","metadata":{"id":"1c2f99b5-80b4-4178-bf30-c1c0a152638f"},"source":["### Adding sources\n","\n","LCEL을 사용하면 검색된 문서 또는 문서에서 특정 소스 메타데이터를 쉽게 반환할 수 있습니다:"]},{"cell_type":"code","execution_count":null,"id":"ded41680-b749-4e2a-9daa-b1165d74783b","metadata":{"id":"ded41680-b749-4e2a-9daa-b1165d74783b","outputId":"fa7024d1-55c6-4b20-b949-0329dd2dc798","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702015114795,"user_tz":-540,"elapsed":1569,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'documents': [{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n","   'start_index': 1585},\n","  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n","   'start_index': 2192},\n","  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n","   'start_index': 17804},\n","  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n","   'start_index': 17414},\n","  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n","   'start_index': 29630},\n","  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n","   'start_index': 19373}],\n"," 'answer': 'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!'}"]},"metadata":{},"execution_count":25}],"source":["from operator import itemgetter\n","\n","from langchain.schema.runnable import RunnableParallel\n","\n","rag_chain_from_docs = (\n","    {\n","        \"context\": lambda input: format_docs(input[\"documents\"]),\n","        \"question\": itemgetter(\"question\"),\n","    }\n","    | rag_prompt_custom\n","    | llm\n","    | StrOutputParser()\n",")\n","rag_chain_with_source = RunnableParallel(\n","    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",") | {\n","    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n","    \"answer\": rag_chain_from_docs,\n","}\n","\n","rag_chain_with_source.invoke(\"What is Task Decomposition\")"]},{"cell_type":"markdown","id":"776ae958-cbdc-4471-8669-c6087436f0b5","metadata":{"id":"776ae958-cbdc-4471-8669-c6087436f0b5"},"source":["### Adding memory\n","\n","과거 사용자 입력을 기억하는 상태 저장 애플리케이션을 만들고 싶다고 가정해 보겠습니다. 이를 지원하기 위해 필요한 작업은 크게 두 가지입니다.\n","1. 과거 메시지를 전달할 수 있는 메시지 플레이스홀더를 체인에 추가합니다.\n","2. 최신 사용자 쿼리를 가져와 채팅 기록의 맥락에서 리트리버에 전달할 수 있는 독립형 질문으로 재형성하는 체인을 추가합니다.\n","\n","2부터 시작하겠습니다. 다음과 같이 `condense question` 체인을 구축할 수 있습니다:"]},{"cell_type":"code","execution_count":null,"id":"2b685428-8b82-4af1-be4f-7232c5d55b73","metadata":{"id":"2b685428-8b82-4af1-be4f-7232c5d55b73"},"outputs":[],"source":["from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","\n","condense_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n","which might reference the chat history, formulate a standalone question \\\n","which can be understood without the chat history. Do NOT answer the question, \\\n","just reformulate it if needed and otherwise return it as is.\"\"\"\n","condense_q_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", condense_q_system_prompt),\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        (\"human\", \"{question}\"),\n","    ]\n",")\n","condense_q_chain = condense_q_prompt | llm | StrOutputParser()"]},{"cell_type":"code","execution_count":null,"id":"46ee9aa1-16f1-4509-8dae-f8c71f4ad47d","metadata":{"id":"46ee9aa1-16f1-4509-8dae-f8c71f4ad47d","outputId":"eae5ccd1-aef6-4e9e-b51d-53866e6ead91","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1702015115590,"user_tz":-540,"elapsed":796,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'What is the definition of \"large\" in the context of a language model?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}],"source":["from langchain.schema.messages import AIMessage, HumanMessage\n","\n","condense_q_chain.invoke(\n","    {\n","        \"chat_history\": [\n","            HumanMessage(content=\"What does LLM stand for?\"),\n","            AIMessage(content=\"Large language model\"),\n","        ],\n","        \"question\": \"What is meant by large\",\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"id":"31ee8481-ce37-41ae-8ca5-62196619d4b3","metadata":{"id":"31ee8481-ce37-41ae-8ca5-62196619d4b3","outputId":"ef3fd66a-5b9b-4e77-8f7d-514640422cba","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1702015116017,"user_tz":-540,"elapsed":430,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'How do transformer models function?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28}],"source":["condense_q_chain.invoke(\n","    {\n","        \"chat_history\": [\n","            HumanMessage(content=\"What does LLM stand for?\"),\n","            AIMessage(content=\"Large language model\"),\n","        ],\n","        \"question\": \"How do transformers work\",\n","    }\n",")"]},{"cell_type":"markdown","id":"42a47168-4a1f-4e39-bd2d-d5b03609a243","metadata":{"id":"42a47168-4a1f-4e39-bd2d-d5b03609a243"},"source":["이제 전체 QA 체인을 구축할 수 있습니다. 채팅 기록이 비어 있지 않을 때만 `condense question chain`을 실행하도록 라우팅 기능을 추가한 것을 주목하세요."]},{"cell_type":"code","execution_count":null,"id":"66f275f3-ddef-4678-b90d-ee64576878f9","metadata":{"id":"66f275f3-ddef-4678-b90d-ee64576878f9"},"outputs":[],"source":["qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n","Use the following pieces of retrieved context to answer the question. \\\n","If you don't know the answer, just say that you don't know. \\\n","Use three sentences maximum and keep the answer concise.\\\n","\n","{context}\"\"\"\n","qa_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", qa_system_prompt),\n","        MessagesPlaceholder(variable_name=\"chat_history\"),\n","        (\"human\", \"{question}\"),\n","    ]\n",")\n","\n","\n","def condense_question(input: dict):\n","    if input.get(\"chat_history\"):\n","        return condense_q_chain\n","    else:\n","        return input[\"question\"]\n","\n","\n","rag_chain = (\n","    RunnablePassthrough.assign(context=condense_question | retriever | format_docs)\n","    | qa_prompt\n","    | llm\n",")"]},{"cell_type":"code","execution_count":null,"id":"51fd0e54-5bb4-4a9a-b012-87a18ebe2bef","metadata":{"id":"51fd0e54-5bb4-4a9a-b012-87a18ebe2bef","outputId":"8ae94108-c4c4-4d3d-c5f9-82781dbd839a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702015124561,"user_tz":-540,"elapsed":8547,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='Common ways of task decomposition include:\\n\\n1. Using Chain of Thought (CoT): CoT is a prompting technique that instructs the model to \"think step by step\" and decompose complex tasks into smaller and simpler steps. It allows for the utilization of more computation at test time and provides insights into the model\\'s thinking process.\\n\\n2. Providing task-specific instructions: This involves giving explicit instructions to the model based on the nature of the task. For example, for writing a novel, the instruction could be \"Write a story outline.\" These instructions guide the model in breaking down the task into manageable subgoals.\\n\\n3. Human inputs: Task decomposition can also be done with the help of human inputs. Humans can provide insights, expertise, and guidance in breaking down complex tasks into smaller steps. Their knowledge and experience can be leveraged to create a structured approach to problem-solving.')"]},"metadata":{},"execution_count":30}],"source":["chat_history = []\n","\n","question = \"What is Task Decomposition?\"\n","ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history})\n","chat_history.extend([HumanMessage(content=question), ai_msg])\n","\n","second_question = \"What are common ways of doing it?\"\n","rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})"]},{"cell_type":"markdown","source":["## 연습문제\n","### 1. WebBaseLoader 기반 QA\n","* 주어진 사이트의 정보를 이용해 QA Agent를 구성해보자\n","\n","[https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/)"],"metadata":{"id":"0I1PE4GaGmjs"},"id":"0I1PE4GaGmjs"},{"cell_type":"code","source":["from langchain.document_loaders import WebBaseLoader\n","\n","loader = WebBaseLoader(\n","    web_paths=(\"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",),\n","    bs_kwargs={\n","        \"parse_only\": bs4.SoupStrainer(\n","            class_=(\"post-content\", \"post-title\", \"post-header\")\n","        )\n","    },\n",")\n","docs = loader.load()"],"metadata":{"id":"mHnZyNdepjFg"},"id":"mHnZyNdepjFg","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",")\n","all_splits = text_splitter.split_documents(docs)"],"metadata":{"id":"VOStDu1zADUr"},"id":"VOStDu1zADUr","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.embeddings import OpenAIEmbeddings\n","from langchain.vectorstores import Chroma\n","\n","vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"],"metadata":{"id":"OCT6nayWARnQ"},"id":"OCT6nayWARnQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6}) # 몇개의 문서를 참조할것인지"],"metadata":{"id":"gVsBcv0NAcTV"},"id":"gVsBcv0NAcTV","execution_count":null,"outputs":[]},{"cell_type":"code","source":["retrieved_docs = retriever.get_relevant_documents(\n","    \"What are the approaches to Task Decomposition?\"\n",")"],"metadata":{"id":"xmT9pQ_UAfgu"},"id":"xmT9pQ_UAfgu","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.chat_models import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"],"metadata":{"id":"1W0v1f14Alra"},"id":"1W0v1f14Alra","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain import hub\n","\n","prompt = hub.pull(\"rlm/rag-prompt\")"],"metadata":{"id":"8Bz8ORF4AraG"},"id":"8Bz8ORF4AraG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\n","    prompt.invoke(\n","        {\"context\": \"filler context\", \"question\": \"filler question\"}\n","    ).to_string()\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mJZ02zBBFKYH","executionInfo":{"status":"ok","timestamp":1702016387838,"user_tz":-540,"elapsed":5,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}},"outputId":"6c2c4ca3-0005-4592-aad1-069fd1a23a18"},"id":"mJZ02zBBFKYH","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n","Question: filler question \n","Context: filler context \n","Answer:\n"]}]},{"cell_type":"code","source":["from langchain.schema import StrOutputParser\n","from langchain.schema.runnable import RunnablePassthrough\n","\n","\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","\n","rag_chain = (\n","    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")"],"metadata":{"id":"eaBQoTfvArcx"},"id":"eaBQoTfvArcx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n","    print(chunk, end=\"\", flush=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QvJL9HLvArfL","executionInfo":{"status":"ok","timestamp":1702016390910,"user_tz":-540,"elapsed":2596,"user":{"displayName":"Junseop So (쏘주형)","userId":"07758510494740838877"}},"outputId":"52c766f5-a950-434f-a479-99fefd3435d4"},"id":"QvJL9HLvArfL","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for easier interpretation and execution by autonomous agents or models. Task decomposition can be done through various methods, such as using prompting techniques, task-specific instructions, or human inputs."]}]},{"cell_type":"code","source":[],"metadata":{"id":"T86BWqYPArhx"},"id":"T86BWqYPArhx","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"poetry-venv","language":"python","name":"poetry-venv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"},"colab":{"provenance":[{"file_id":"1WKGrl0xQ5EIPs7HvZJNlsc6KuVSMVkLx","timestamp":1706576605425},{"file_id":"https://github.com/langchain-ai/langchain/blob/master/docs/docs/use_cases/question_answering/index.ipynb","timestamp":1701661586835}]}},"nbformat":4,"nbformat_minor":5}
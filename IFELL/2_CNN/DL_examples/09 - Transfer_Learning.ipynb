{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"code","metadata":{"id":"PWdSqTzMDkQO"},"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","\n","import os\n","import sys\n","import time\n","import shutil\n","import PIL\n","\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"idiPADPXRXD-"},"source":["### 이미지넷에 사용됐던 CNN layers를 사용해보자"]},{"cell_type":"markdown","metadata":{"id":"NjvDSUPfD3tA"},"source":["## Usage `VGG16`\n","* [code link](https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg16.py)\n","* [document link](https://keras.io/applications/#vgg16)\n","\n","```python\n","def VGG16(include_top=True,\n","          weights='imagenet',\n","          input_tensor=None,\n","          input_shape=None,\n","          pooling=None,\n","          classes=1000,\n","          **kwargs):\n","    \"\"\"Instantiates the VGG16 architecture.\n","    Optionally loads weights pre-trained on ImageNet.\n","    Note that the data format convention used by the model is\n","    the one specified in your Keras config at `~/.keras/keras.json`.\n","    # Arguments\n","        include_top: whether to include the 3 fully-connected\n","            layers at the top of the network.\n","        weights: one of `None` (random initialization),\n","              'imagenet' (pre-training on ImageNet),\n","              or the path to the weights file to be loaded.\n","        input_tensor: optional Keras tensor\n","            (i.e. output of `layers.Input()`)\n","            to use as image input for the model.\n","        input_shape: optional shape tuple, only to be specified\n","            if `include_top` is False (otherwise the input shape\n","            has to be `(224, 224, 3)`\n","            (with `channels_last` data format)\n","            or `(3, 224, 224)` (with `channels_first` data format).\n","            It should have exactly 3 input channels,\n","            and width and height should be no smaller than 32.\n","            E.g. `(200, 200, 3)` would be one valid value.\n","        pooling: Optional pooling mode for feature extraction\n","            when `include_top` is `False`.\n","            - `None` means that the output of the model will be\n","                the 4D tensor output of the\n","                last convolutional block.\n","            - `avg` means that global average pooling\n","                will be applied to the output of the\n","                last convolutional block, and thus\n","                the output of the model will be a 2D tensor.\n","            - `max` means that global max pooling will\n","                be applied.\n","        classes: optional number of classes to classify images\n","            into, only to be specified if `include_top` is True, and\n","            if no `weights` argument is specified.\n","    # Returns\n","        A Keras model instance.\n","    # Raises\n","        ValueError: in case of invalid argument for `weights`,\n","            or invalid input shape.\n","    \"\"\"\n","```"]},{"cell_type":"code","metadata":{"id":"D3QeZw8mDv7_"},"source":["conv_base = tf.keras.applications.VGG16(weights='imagenet',\n","                                        include_top=False,\n","                                        input_shape=(150, 150, 3)) #224, 224, 3\n","conv_base.trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QWiiIV9SD8On"},"source":["conv_base.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3FxNDCgfRjFq"},"source":["### VGG를 이용한 모델 구성"]},{"cell_type":"code","metadata":{"id":"5g3_4JwWD9Iv"},"source":["model = tf.keras.Sequential()\n","model.add(pre_layer)\n","model.add(conv_base)\n","model.add(layers.Flatten())\n","model.add(layers.Dense(256, activation='relu'))\n","model.add(layers.Dense(5, activation='softmax'))\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-8ReTCMaRlv4"},"source":["* 학습할 파라미터를 지정해준다"]},{"cell_type":"code","metadata":{"id":"dquMgm_PEATI"},"source":["# assign conv1_1 and dense1 weight to compare itself after training some steps\n","for var in model.variables:\n","  #print(var.name)\n","  if var.name == 'block1_conv1/kernel:0':\n","    conv1_1_w = var\n","  if var.name == 'dense/kernel:0':\n","    dense_w = var"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qvf6gXypEOvH"},"source":["print(conv1_1_w)\n","print(dense_w)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k8MtiG4lYse5"},"source":["### 모델 학습 파라미터 지정\n","* 이미 학습된 CNN layer의 파라미터는 학습이 되지않도록 설정해야한다."]},{"cell_type":"code","metadata":{"id":"kDGUyR8GERZ4"},"source":["# Freeze vgg16 conv base part (means that trainable option is False)\n","for layer in model.layers:\n","  if layer.name == 'vgg16':\n","    layer.trainable = False\n","  print(\"variable name: {}, trainable: {}\".format(layer.name, layer.trainable))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yJPaEn25Y3TX"},"source":["### 테스트용 데이터셋 다운로드\n","* 꽃 이미지 데이터셋"]},{"cell_type":"code","metadata":{"id":"kABCMSjeEUXX"},"source":["import pathlib\n","dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n","data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n","data_dir = pathlib.Path(data_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8lufllvPEWv3"},"source":["image_count = len(list(data_dir.glob('*/*.jpg')))\n","print(image_count)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-dlJVVvgEXnS"},"source":["roses = list(data_dir.glob('roses/*'))\n","PIL.Image.open(str(roses[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ufhtrof2TC6-"},"source":["tulips = list(data_dir.glob('tulips/*'))\n","PIL.Image.open(str(tulips[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a8RFp7FQFDmt"},"source":["batch_size = 32\n","img_height = 150\n","img_width = 150\n","max_epochs = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7FxzUJx2WWEJ"},"source":[" train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    data_dir,\n","    validation_split=0.2,\n","    subset=\"training\",\n","    seed=123,\n","    image_size=(img_height, img_width),\n","    batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fUnB7ykaWc1U"},"source":[" val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hXgQXQWBWfrc"},"source":["class_names = train_ds.class_names\n","print(class_names) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rt-Z4e_xT7JD"},"source":["plt.figure(figsize=(10, 10))\n","for images, labels in train_ds.take(1):\n","    for i in range(9):\n","        ax = plt.subplot(3, 3, i + 1)\n","        plt.imshow(images[i].numpy().astype(\"uint8\"))\n","        plt.title(class_names[labels[i]])\n","        plt.axis(\"off\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GLQ0H7gdY8D_"},"source":["### 데이터 전처리 파트\n","* 모델 데이터에 맞도록 데이터를 0~1 사이 값으로 Normalization"]},{"cell_type":"code","metadata":{"id":"10PlP0myW011"},"source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","\n","train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i15h6fjvXFFi"},"source":[" normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)\n","\n","normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n","image_batch, labels_batch = next(iter(normalized_ds))\n","first_image = image_batch[0]\n","# Notice the pixels values are now in `[0,1]`.\n","print(np.min(first_image), np.max(first_image))  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C56NsDImZDWW"},"source":["### 모델 학습"]},{"cell_type":"code","metadata":{"id":"wOmQCXzuFIsn"},"source":["model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), # from_logits 마지막 레이어의 activation 여부\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yPYH7Pvdbniz"},"source":["### keras callback 함수"]},{"cell_type":"code","metadata":{"id":"vNTxtDPbQ3bE"},"source":["reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n","                              patience=5, min_lr=0.001)\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OWMOZCFLFJnY"},"source":["history = model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    callbacks=[reduce_lr, early_stopping],\n","    epochs=max_epochs\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yp3NE9EYQ3bJ","cellView":"both"},"source":["print(history.history.keys())\n","len_res = len(history.history['accuracy'])\n","print(len_res)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ElNnurGFKP7"},"source":["# Compare weights between before and after training some steps\n","for var in model.variables:\n","  #print(var.name)\n","  if var.name == 'block1_conv1/kernel:0':\n","    conv1_1_w_1 = var\n","  if var.name == 'dense/kernel:0':\n","    dense_w_1 = var"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_n5X2LgFLMY"},"source":["acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","epochs_range = range(len_res)\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iEJ4MWmOFL-Q"},"source":["history = model.evaluate(val_ds)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s76dhxrlFM_4"},"source":["# loss\n","print(\"loss value: {:.3f}\".format(history[0]))\n","# accuracy\n","print(\"accuracy value: {:.3f}\".format(history[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AsXn9Q1nFN2X"},"source":[],"execution_count":null,"outputs":[]}]}
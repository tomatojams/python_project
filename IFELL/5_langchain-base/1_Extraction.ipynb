{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84edb4e",
   "metadata": {
    "id": "b84edb4e"
   },
   "source": [
    "## Use case\n",
    "\n",
    "Raw LLM 생성에서 구조화된 출력을 얻는 것은 어렵습니다.\n",
    "\n",
    "예를 들어, 특정 스키마로 포맷된 모델 출력이 필요하다고 가정해 보겠습니다:\n",
    "\n",
    "- 데이터베이스에 삽입할 구조화된 행을 추출하는 경우\n",
    "- API 매개변수 추출\n",
    "- 사용자 쿼리의 다른 부분 추출(예: 시맨틱 검색과 키워드 검색)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f474d4",
   "metadata": {
    "id": "97f474d4"
   },
   "source": [
    "## Overview\n",
    "\n",
    "이를 위한 두 가지 주요 접근 방식이 있습니다:\n",
    "\n",
    "- `Functions`: 일부 LLM은 functions를 호출하여 LLM 응답에서 임의의 엔티티를 추출할 수 있습니다.\n",
    "\n",
    "- `Parsing`: 출력 구문 분석기는 LLM 응답을 구조화하는 클래스입니다.\n",
    "\n",
    "일부 LLM만 함수(예: OpenAI)를 지원하며, 파서보다 더 일반적입니다.\n",
    "\n",
    "구문 분석기는 제공된 스키마에 열거된 내용(예: 사람의 특정 속성)을 정확하게 추출합니다.\n",
    "\n",
    "함수는 제공된 스키마 이외의 것(예: 사용자가 요청하지 않은 사람에 대한 속성)을 추론할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d89f21",
   "metadata": {
    "id": "25d89f21"
   },
   "source": [
    "## Quickstart\n",
    "\n",
    "OpenAI 함수는 추출을 시작하는 한 가지 방법입니다.\n",
    "\n",
    "LLM 출력에서 추출할 속성을 지정하는 스키마를 정의합니다.\n",
    "\n",
    "그런 다음 `create_extraction_chain`을 사용하여 OpenAI 함수 호출을 통해 원하는 스키마를 추출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5ec7a3",
   "metadata": {
    "id": "3f5ec7a3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706144389736,
     "user_tz": -540,
     "elapsed": 28350,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    },
    "outputId": "f9a3b0c4-ad0f-4391-873e-139d5904c556",
    "ExecuteTime": {
     "end_time": "2024-02-01T01:18:44.722904Z",
     "start_time": "2024-02-01T01:18:42.414695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (0.1.4)\r\n",
      "Requirement already satisfied: openai in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (1.10.0)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (6.0)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (2.0.25)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (3.9.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (4.0.3)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (0.6.3)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (1.33)\r\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (0.0.16)\r\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (0.1.16)\r\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (0.0.83)\r\n",
      "Requirement already satisfied: numpy<2,>=1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (1.23.2)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (2.5.3)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (2.31.0)\r\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain) (8.2.3)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from openai) (3.5.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from openai) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from openai) (0.26.0)\r\n",
      "Requirement already satisfied: sniffio in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from openai) (1.2.0)\r\n",
      "Requirement already satisfied: tqdm>4 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from openai) (4.66.1)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from openai) (4.9.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\r\n",
      "Requirement already satisfied: idna>=2.8 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\r\n",
      "Requirement already satisfied: certifi in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\r\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from langchain-core<0.2,>=0.1.16->langchain) (23.2)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from pydantic<3,>=1->langchain) (2.14.6)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.1.0)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.1.0)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain openai"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import getpass\n",
    "import os\n",
    "# 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\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8mU8PR67jPR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706144394566,
     "user_tz": -540,
     "elapsed": 4845,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    },
    "outputId": "c74caa70-7ef8-4170-99e3-93f12214c92e",
    "ExecuteTime": {
     "end_time": "2024-02-08T13:55:57.519448500Z",
     "start_time": "2024-02-08T13:55:55.637921700Z"
    }
   },
   "id": "l8mU8PR67jPR",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ba4cd2d3e1b28a59"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e017ba0",
   "metadata": {
    "id": "3e017ba0",
    "outputId": "27b40dd4-7a02-420e-d804-8396e1ff39ed",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706144401983,
     "user_tz": -540,
     "elapsed": 7423,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T13:56:05.468322200Z",
     "start_time": "2024-02-08T13:56:01.572613200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "F:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-OtV1c***************************************juX6. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAuthenticationError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 20\u001B[0m\n\u001B[0;32m     18\u001B[0m llm \u001B[38;5;241m=\u001B[39m ChatOpenAI(temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.7\u001B[39m, model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpt-3.5-turbo\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     19\u001B[0m chain \u001B[38;5;241m=\u001B[39m create_extraction_chain(schema, llm)\n\u001B[1;32m---> 20\u001B[0m \u001B[43mchain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43minp\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    143\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    144\u001B[0m     emit_warning()\n\u001B[1;32m--> 145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain\\chains\\base.py:538\u001B[0m, in \u001B[0;36mChain.run\u001B[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[0;32m    536\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    537\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`run` supports only one positional argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 538\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[0;32m    539\u001B[0m         _output_key\n\u001B[0;32m    540\u001B[0m     ]\n\u001B[0;32m    542\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[0;32m    543\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(kwargs, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[0;32m    544\u001B[0m         _output_key\n\u001B[0;32m    545\u001B[0m     ]\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    143\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    144\u001B[0m     emit_warning()\n\u001B[1;32m--> 145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain\\chains\\base.py:363\u001B[0m, in \u001B[0;36mChain.__call__\u001B[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[0;32m    331\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[0;32m    332\u001B[0m \n\u001B[0;32m    333\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[0;32m    355\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    356\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    357\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[0;32m    358\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[0;32m    359\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[0;32m    360\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[0;32m    361\u001B[0m }\n\u001B[1;32m--> 363\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    364\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    365\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    366\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    367\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    368\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain\\chains\\base.py:162\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    161\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[1;32m--> 162\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    163\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[0;32m    164\u001B[0m final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[0;32m    165\u001B[0m     inputs, outputs, return_only_outputs\n\u001B[0;32m    166\u001B[0m )\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain\\chains\\base.py:156\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    149\u001B[0m run_manager \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_chain_start(\n\u001B[0;32m    150\u001B[0m     dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    151\u001B[0m     inputs,\n\u001B[0;32m    152\u001B[0m     name\u001B[38;5;241m=\u001B[39mrun_name,\n\u001B[0;32m    153\u001B[0m )\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    155\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 156\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    157\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[0;32m    159\u001B[0m     )\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    161\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain\\chains\\llm.py:103\u001B[0m, in \u001B[0;36mLLMChain._call\u001B[1;34m(self, inputs, run_manager)\u001B[0m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call\u001B[39m(\n\u001B[0;32m     99\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    100\u001B[0m     inputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[0;32m    101\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    102\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m--> 103\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_outputs(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain\\chains\\llm.py:115\u001B[0m, in \u001B[0;36mLLMChain.generate\u001B[1;34m(self, input_list, run_manager)\u001B[0m\n\u001B[0;32m    113\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m run_manager\u001B[38;5;241m.\u001B[39mget_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm, BaseLanguageModel):\n\u001B[1;32m--> 115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[0;32m    116\u001B[0m         prompts,\n\u001B[0;32m    117\u001B[0m         stop,\n\u001B[0;32m    118\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[0;32m    119\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs,\n\u001B[0;32m    120\u001B[0m     )\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    122\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mbind(stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs)\u001B[38;5;241m.\u001B[39mbatch(\n\u001B[0;32m    123\u001B[0m         cast(List, prompts), {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks}\n\u001B[0;32m    124\u001B[0m     )\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:544\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    537\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    538\u001B[0m     prompts: List[PromptValue],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    541\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    542\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    543\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 544\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:408\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[0;32m    406\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[0;32m    407\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[1;32m--> 408\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    409\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    410\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)\n\u001B[0;32m    411\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[0;32m    412\u001B[0m ]\n\u001B[0;32m    413\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[0;32m    395\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[0;32m    396\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    397\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 398\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[0;32m    399\u001B[0m                 m,\n\u001B[0;32m    400\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    401\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    402\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    403\u001B[0m             )\n\u001B[0;32m    404\u001B[0m         )\n\u001B[0;32m    405\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    406\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:577\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    573\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    574\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    575\u001B[0m     )\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported:\n\u001B[1;32m--> 577\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m    578\u001B[0m         messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    579\u001B[0m     )\n\u001B[0;32m    580\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    581\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_community\\chat_models\\openai.py:439\u001B[0m, in \u001B[0;36mChatOpenAI._generate\u001B[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001B[0m\n\u001B[0;32m    433\u001B[0m message_dicts, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_message_dicts(messages, stop)\n\u001B[0;32m    434\u001B[0m params \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    435\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[0;32m    436\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream} \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}),\n\u001B[0;32m    437\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    438\u001B[0m }\n\u001B[1;32m--> 439\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompletion_with_retry(\n\u001B[0;32m    440\u001B[0m     messages\u001B[38;5;241m=\u001B[39mmessage_dicts, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[0;32m    441\u001B[0m )\n\u001B[0;32m    442\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(response)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_community\\chat_models\\openai.py:356\u001B[0m, in \u001B[0;36mChatOpenAI.completion_with_retry\u001B[1;34m(self, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001B[39;00m\n\u001B[0;32m    355\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_openai_v1():\n\u001B[1;32m--> 356\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    358\u001B[0m retry_decorator \u001B[38;5;241m=\u001B[39m _create_retry_decorator(\u001B[38;5;28mself\u001B[39m, run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[0;32m    360\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[0;32m    361\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\openai\\_utils\\_utils.py:271\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    269\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    270\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 271\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\openai\\resources\\chat\\completions.py:659\u001B[0m, in \u001B[0;36mCompletions.create\u001B[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    608\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    609\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    610\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    657\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[0;32m    658\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[1;32m--> 659\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    660\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    661\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    662\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m    663\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    664\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    665\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    666\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    667\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    668\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    669\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    670\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    671\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    672\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    673\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    674\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    675\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    676\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    677\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    678\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    679\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    680\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    681\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    682\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    683\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    684\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    685\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    686\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    687\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m    688\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    689\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    690\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    691\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    692\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\openai\\_base_client.py:1200\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1187\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1188\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1195\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1196\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1197\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1198\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1199\u001B[0m     )\n\u001B[1;32m-> 1200\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\openai\\_base_client.py:889\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    880\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    881\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    882\u001B[0m     cast_to: Type[ResponseT],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    887\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    888\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m--> 889\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    890\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    891\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    892\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    893\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    894\u001B[0m \u001B[43m        \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    895\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\openai\\_base_client.py:980\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    977\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m    979\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 980\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    982\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[0;32m    983\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m    984\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    987\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m    988\u001B[0m )\n",
      "\u001B[1;31mAuthenticationError\u001B[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-OtV1c***************************************juX6. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_extraction_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Schema\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"height\": {\"type\": \"integer\"},\n",
    "        \"hair_color\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"name\", \"height\"],\n",
    "}\n",
    "\n",
    "# Input\n",
    "inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\"\"\n",
    "\n",
    "# Run chain\n",
    "llm = ChatOpenAI(temperature=0.7, model=\"gpt-3.5-turbo\")\n",
    "chain = create_extraction_chain(schema, llm)\n",
    "chain.run(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7eb826",
   "metadata": {
    "id": "6f7eb826"
   },
   "source": [
    "## Option 1: OpenAI functions\n",
    "\n",
    "### Looking under the hood\n",
    "\n",
    "`create_extraction_chain` 호출할 때 어떤 일이 일어나는지 자세히 살펴봅시다.\n",
    "\n",
    "이 `정보_추출` 함수는 [여기](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/openai_functions/extraction.py)에 정의되어 있으며 딕셔너리를 반환합니다.\n",
    "\n",
    "\n",
    "모델 출력에서 `dict`를 볼 수 있습니다:\n",
    "```\n",
    " {\n",
    "      \"info\": [\n",
    "        {\n",
    "          \"name\": \"Alex\",\n",
    "          \"height\": 5,\n",
    "          \"hair_color\": \"blonde\"\n",
    "        },\n",
    "        {\n",
    "          \"name\": \"Claudia\",\n",
    "          \"height\": 6,\n",
    "          \"hair_color\": \"brunette\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "```\n",
    "\n",
    "그런 다음 `create_extraction_chain`은 [`JsonKeyOutputFunctionsParser`](https://github.com/langchain-ai/langchain/blob/f81e613086d211327b67b0fb591fd4d5f9a85860/libs/langchain/langchain/chains/openai_functions/extraction.py#L62)를 사용하여 원시 LLM 출력을 파싱합니다.\n",
    "\n",
    "그 결과 위의 체인에서 반환된 JSON 객체 목록이 생성됩니다:\n",
    "```\n",
    "[{'name': 'Alex', 'height': 5, 'hair_color': 'blonde'},\n",
    " {'name': 'Claudia', 'height': 6, 'hair_color': 'brunette'}]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb03138",
   "metadata": {
    "id": "dcb03138"
   },
   "source": [
    "### Multiple entity types\n",
    "\n",
    "이를 더 확장할 수 있습니다.\n",
    "\n",
    "개와 사람을 구분하고 싶다고 가정해 봅시다.\n",
    "\n",
    "각 속성에 `person_` 및 `dog_` 접두사를 추가하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01eae733",
   "metadata": {
    "id": "01eae733",
    "outputId": "0b4524a3-2cf7-4f6f-d4b0-c8476cb18db5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706144408194,
     "user_tz": -540,
     "elapsed": 6216,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-02-08T13:56:06.557543400Z",
     "start_time": "2024-02-08T13:56:06.212946100Z"
    }
   },
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-OtV1c***************************************juX6. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAuthenticationError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 17\u001B[0m\n\u001B[0;32m     12\u001B[0m chain \u001B[38;5;241m=\u001B[39m create_extraction_chain(schema, llm)\n\u001B[0;32m     14\u001B[0m inp \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mAlex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\u001B[39m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124mAlex\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms dog Frosty is a labrador and likes to play hide and seek.\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m---> 17\u001B[0m \u001B[43mchain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43minp\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    143\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    144\u001B[0m     emit_warning()\n\u001B[1;32m--> 145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain\\chains\\base.py:538\u001B[0m, in \u001B[0;36mChain.run\u001B[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[0;32m    536\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    537\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`run` supports only one positional argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 538\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[0;32m    539\u001B[0m         _output_key\n\u001B[0;32m    540\u001B[0m     ]\n\u001B[0;32m    542\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[0;32m    543\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(kwargs, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[0;32m    544\u001B[0m         _output_key\n\u001B[0;32m    545\u001B[0m     ]\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:145\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    143\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    144\u001B[0m     emit_warning()\n\u001B[1;32m--> 145\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m wrapped(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain\\chains\\base.py:363\u001B[0m, in \u001B[0;36mChain.__call__\u001B[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[0;32m    331\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[0;32m    332\u001B[0m \n\u001B[0;32m    333\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[0;32m    355\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    356\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    357\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[0;32m    358\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[0;32m    359\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[0;32m    360\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[0;32m    361\u001B[0m }\n\u001B[1;32m--> 363\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    364\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    365\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    366\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    367\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    368\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain\\chains\\base.py:162\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    161\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[1;32m--> 162\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    163\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[0;32m    164\u001B[0m final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[0;32m    165\u001B[0m     inputs, outputs, return_only_outputs\n\u001B[0;32m    166\u001B[0m )\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain\\chains\\base.py:156\u001B[0m, in \u001B[0;36mChain.invoke\u001B[1;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[0;32m    149\u001B[0m run_manager \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_chain_start(\n\u001B[0;32m    150\u001B[0m     dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    151\u001B[0m     inputs,\n\u001B[0;32m    152\u001B[0m     name\u001B[38;5;241m=\u001B[39mrun_name,\n\u001B[0;32m    153\u001B[0m )\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    155\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 156\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    157\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[0;32m    159\u001B[0m     )\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    161\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain\\chains\\llm.py:103\u001B[0m, in \u001B[0;36mLLMChain._call\u001B[1;34m(self, inputs, run_manager)\u001B[0m\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call\u001B[39m(\n\u001B[0;32m     99\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    100\u001B[0m     inputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[0;32m    101\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    102\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m--> 103\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_outputs(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain\\chains\\llm.py:115\u001B[0m, in \u001B[0;36mLLMChain.generate\u001B[1;34m(self, input_list, run_manager)\u001B[0m\n\u001B[0;32m    113\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m run_manager\u001B[38;5;241m.\u001B[39mget_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm, BaseLanguageModel):\n\u001B[1;32m--> 115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[0;32m    116\u001B[0m         prompts,\n\u001B[0;32m    117\u001B[0m         stop,\n\u001B[0;32m    118\u001B[0m         callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[0;32m    119\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs,\n\u001B[0;32m    120\u001B[0m     )\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    122\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mbind(stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs)\u001B[38;5;241m.\u001B[39mbatch(\n\u001B[0;32m    123\u001B[0m         cast(List, prompts), {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks}\n\u001B[0;32m    124\u001B[0m     )\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:544\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    537\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    538\u001B[0m     prompts: List[PromptValue],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    541\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    542\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    543\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 544\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:408\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[0;32m    406\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[0;32m    407\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[1;32m--> 408\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    409\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    410\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)\n\u001B[0;32m    411\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[0;32m    412\u001B[0m ]\n\u001B[0;32m    413\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001B[0m\n\u001B[0;32m    395\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[0;32m    396\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    397\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 398\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[0;32m    399\u001B[0m                 m,\n\u001B[0;32m    400\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    401\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    402\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    403\u001B[0m             )\n\u001B[0;32m    404\u001B[0m         )\n\u001B[0;32m    405\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    406\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:577\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    573\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    574\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    575\u001B[0m     )\n\u001B[0;32m    576\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported:\n\u001B[1;32m--> 577\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m    578\u001B[0m         messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    579\u001B[0m     )\n\u001B[0;32m    580\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    581\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_community\\chat_models\\openai.py:439\u001B[0m, in \u001B[0;36mChatOpenAI._generate\u001B[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001B[0m\n\u001B[0;32m    433\u001B[0m message_dicts, params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_message_dicts(messages, stop)\n\u001B[0;32m    434\u001B[0m params \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    435\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams,\n\u001B[0;32m    436\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: stream} \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}),\n\u001B[0;32m    437\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    438\u001B[0m }\n\u001B[1;32m--> 439\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompletion_with_retry(\n\u001B[0;32m    440\u001B[0m     messages\u001B[38;5;241m=\u001B[39mmessage_dicts, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams\n\u001B[0;32m    441\u001B[0m )\n\u001B[0;32m    442\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(response)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\langchain_community\\chat_models\\openai.py:356\u001B[0m, in \u001B[0;36mChatOpenAI.completion_with_retry\u001B[1;34m(self, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001B[39;00m\n\u001B[0;32m    355\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_openai_v1():\n\u001B[1;32m--> 356\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    358\u001B[0m retry_decorator \u001B[38;5;241m=\u001B[39m _create_retry_decorator(\u001B[38;5;28mself\u001B[39m, run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[0;32m    360\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[0;32m    361\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\openai\\_utils\\_utils.py:271\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    269\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    270\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 271\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\openai\\resources\\chat\\completions.py:659\u001B[0m, in \u001B[0;36mCompletions.create\u001B[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    608\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    609\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    610\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    657\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[0;32m    658\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[1;32m--> 659\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    660\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    661\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    662\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m    663\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    664\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    665\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    666\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    667\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    668\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    669\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    670\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    671\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    672\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    673\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    674\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    675\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    676\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    677\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    678\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    679\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    680\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    681\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    682\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    683\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    684\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    685\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    686\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    687\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m    688\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    689\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    690\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    691\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    692\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\openai\\_base_client.py:1200\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1187\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1188\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1195\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1196\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1197\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1198\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1199\u001B[0m     )\n\u001B[1;32m-> 1200\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\openai\\_base_client.py:889\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    880\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    881\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    882\u001B[0m     cast_to: Type[ResponseT],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    887\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    888\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m--> 889\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    890\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    891\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    892\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    893\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    894\u001B[0m \u001B[43m        \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    895\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\Anaconda3_envs\\tensor2\\lib\\site-packages\\openai\\_base_client.py:980\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    977\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m    979\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 980\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    982\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[0;32m    983\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m    984\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    987\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[0;32m    988\u001B[0m )\n",
      "\u001B[1;31mAuthenticationError\u001B[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-OtV1c***************************************juX6. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"person_name\": {\"type\": \"string\"},\n",
    "        \"person_height\": {\"type\": \"integer\"},\n",
    "        \"person_hair_color\": {\"type\": \"string\"},\n",
    "        \"dog_name\": {\"type\": \"string\"},\n",
    "        \"dog_breed\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [\"person_name\", \"person_height\"],\n",
    "}\n",
    "\n",
    "chain = create_extraction_chain(schema, llm)\n",
    "\n",
    "inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\n",
    "Alex's dog Frosty is a labrador and likes to play hide and seek.\"\"\"\n",
    "\n",
    "chain.run(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f205905c",
   "metadata": {
    "id": "f205905c"
   },
   "source": [
    "### Unrelated entities\n",
    "\n",
    "`필수: []`를 사용하면 모델이 단일 엔티티(사람 또는 개)에 대해 **사람 속성만** 또는  **개 속성만**을 반환하도록 허용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ff4ac7e",
   "metadata": {
    "id": "6ff4ac7e",
    "outputId": "8a78b286-673d-4aee-aa09-8dc119261671",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706144415191,
     "user_tz": -540,
     "elapsed": 7003,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'person_name': 'Alex', 'person_height': 5, 'person_hair_color': 'blonde'},\n",
       " {'person_name': 'Claudia',\n",
       "  'person_height': 6,\n",
       "  'person_hair_color': 'brunette'},\n",
       " {'dog_name': 'Willow', 'dog_breed': 'German Shepherd'},\n",
       " {'dog_name': 'Milo', 'dog_breed': 'Border Collie'}]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"person_name\": {\"type\": \"string\"},\n",
    "        \"person_height\": {\"type\": \"integer\"},\n",
    "        \"person_hair_color\": {\"type\": \"string\"},\n",
    "        \"dog_name\": {\"type\": \"string\"},\n",
    "        \"dog_breed\": {\"type\": \"string\"},\n",
    "    },\n",
    "    \"required\": [],\n",
    "}\n",
    "\n",
    "chain = create_extraction_chain(schema, llm)\n",
    "\n",
    "inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\n",
    "Willow is a German Shepherd that likes to play with other dogs and can always be found playing with Milo, a border collie that lives close by.\"\"\"\n",
    "\n",
    "chain.run(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3b958",
   "metadata": {
    "id": "34f3b958"
   },
   "source": [
    "### Extra information\n",
    "\n",
    "The power of functions (파서만 사용할 때와 비교했을 때) 의미 추출을 수행할 수 있는 능력에 있습니다.\n",
    "\n",
    "특히 '스키마에 명시적으로 열거되지 않은 것을 요청할 수 있다'는 점입니다.\n",
    "\n",
    "개에 대한 지정되지 않은 추가 정보를 원한다고 가정해 봅시다.\n",
    "\n",
    "비정형 추출을 위한 자리 표시자인 `dog_extra_info`를 추가할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c7b26f",
   "metadata": {
    "id": "40c7b26f",
    "outputId": "8fdc7ef7-9fe9-4d0d-901e-4bb499ac50c2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706144424605,
     "user_tz": -540,
     "elapsed": 9416,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'person_name': 'Alex', 'person_height': 5, 'person_hair_color': 'blonde'},\n",
       " {'person_name': 'Claudia',\n",
       "  'person_height': 6,\n",
       "  'person_hair_color': 'brunette'},\n",
       " {'dog_name': 'Willow',\n",
       "  'dog_breed': 'German Shepherd',\n",
       "  'dog_extra_info': 'likes to play with other dogs'},\n",
       " {'dog_name': 'Milo',\n",
       "  'dog_breed': 'border collie',\n",
       "  'dog_extra_info': 'lives close by'}]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"person_name\": {\"type\": \"string\"},\n",
    "        \"person_height\": {\"type\": \"integer\"},\n",
    "        \"person_hair_color\": {\"type\": \"string\"},\n",
    "        \"dog_name\": {\"type\": \"string\"},\n",
    "        \"dog_breed\": {\"type\": \"string\"},\n",
    "        \"dog_extra_info\": {\"type\": \"string\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "chain = create_extraction_chain(schema, llm)\n",
    "chain.run(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a949c60",
   "metadata": {
    "id": "3a949c60"
   },
   "source": [
    "이를 통해 개에 대한 추가 정보를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf71ddce",
   "metadata": {
    "id": "bf71ddce"
   },
   "source": [
    "### Pydantic\n",
    "\n",
    "Pydantic은 Python용 데이터 유효성 검사 및 설정 관리 라이브러리입니다.\n",
    "\n",
    "이 라이브러리를 사용하면 객체를 인스턴스화할 때 자동으로 유효성이 검사되는 속성을 가진 데이터 클래스를 만들 수 있습니다.\n",
    "\n",
    "유형으로 주석이 달린 속성을 가진 클래스를 정의할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d36a743b",
   "metadata": {
    "id": "d36a743b",
    "outputId": "6f187478-c7da-4b4b-90b6-9b4fcb201538",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706144429483,
     "user_tz": -540,
     "elapsed": 4898,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Properties(person_name='Alex', person_height=5, person_hair_color='blonde', dog_breed=None, dog_name=None),\n",
       " Properties(person_name='Claudia', person_height=6, person_hair_color='brunette', dog_breed=None, dog_name=None)]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain.chains import create_extraction_chain_pydantic\n",
    "from langchain.pydantic_v1 import BaseModel\n",
    "\n",
    "\n",
    "# Pydantic data class\n",
    "class Properties(BaseModel):\n",
    "    person_name: str\n",
    "    person_height: int\n",
    "    person_hair_color: str\n",
    "    dog_breed: Optional[str]\n",
    "    dog_name: Optional[str]\n",
    "\n",
    "\n",
    "# Extraction\n",
    "chain = create_extraction_chain_pydantic(pydantic_schema=Properties, llm=llm)\n",
    "\n",
    "# Run\n",
    "inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\"\"\n",
    "chain.run(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a0351a",
   "metadata": {
    "id": "07a0351a"
   },
   "source": [
    "trace에서 볼 수 있듯이, 위와 같이 Pydantic 스키마와 함께 `information_extraction` 함수를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd9f121",
   "metadata": {
    "id": "cbd9f121"
   },
   "source": [
    "## 옵션 2: 파싱\n",
    "\n",
    "출력 구문 분석기는 언어 모델 응답을 구조화하는 데 도움이 되는 클래스입니다.\n",
    "\n",
    "위에 표시된 것처럼 `create_extraction_chain`에서 OpenAI 함수 호출의 출력을 구문 분석하는 데 사용됩니다.\n",
    "\n",
    "하지만 함수와 독립적으로 사용할 수도 있습니다.\n",
    "\n",
    "### Pydantic\n",
    "\n",
    "위와 마찬가지로 Pydantic 데이터 클래스를 기반으로 한 세대를 파싱해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64650362",
   "metadata": {
    "id": "64650362",
    "outputId": "33d9f2c8-0c99-41a0-ca7a-00a4948c743b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706144431765,
     "user_tz": -540,
     "elapsed": 2288,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n",
      "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "People(people=[Person(person_name='Alex', person_height=5, person_hair_color='blonde', dog_breed=None, dog_name=None), Person(person_name='Claudia', person_height=6, person_hair_color='brunette', dog_breed=None, dog_name=None)])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from typing import Optional, Sequence\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    ")\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    person_name: str\n",
    "    person_height: int\n",
    "    person_hair_color: str\n",
    "    dog_breed: Optional[str]\n",
    "    dog_name: Optional[str]\n",
    "\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: Sequence[Person]\n",
    "\n",
    "\n",
    "# Run\n",
    "query = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\"\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=People)\n",
    "# print(parser.get_format_instructions())\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Run\n",
    "_input = prompt.format_prompt(query=query)\n",
    "# print(_input)\n",
    "model = OpenAI(temperature=0)\n",
    "output = model(_input.to_string())\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826899df",
   "metadata": {
    "id": "826899df"
   },
   "source": [
    "LLM이 원하는 형식으로 출력하도록하기 위해 투샷 프롬프트를 제공합니다.\n",
    "\n",
    "그리고 조금 더 작업해야 합니다:\n",
    "\n",
    "* `Person`의 여러 인스턴스를 보유하는 클래스를 정의합니다.\n",
    "* LLM의 출력을 Pydantic 클래스로 명시적으로 파싱합니다.\n",
    "\n",
    "다른 경우에도 이와 같은 결과를 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "837c350e",
   "metadata": {
    "id": "837c350e",
    "outputId": "a321fdb9-163d-4953-daa5-313b2451ad3e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706144432642,
     "user_tz": -540,
     "elapsed": 882,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\n",
      "```\n",
      "text='Answer the user query.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"setup\": {\"title\": \"Setup\", \"description\": \"question to set up a joke\", \"type\": \"string\"}, \"punchline\": {\"title\": \"Punchline\", \"description\": \"answer to resolve the joke\", \"type\": \"string\"}}, \"required\": [\"setup\", \"punchline\"]}\\n```\\nTell me a joke.\\n'\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Joke(setup='Why did the tomato turn red?', punchline='Because it saw the salad dressing!')"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    ")\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "    # You can add custom validation logic easily with Pydantic.\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != \"?\":\n",
    "            raise ValueError(\"Badly formed question!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "print(parser.get_format_instructions())\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Run\n",
    "_input = prompt.format_prompt(query=joke_query)\n",
    "print(_input)\n",
    "model = OpenAI(temperature=0)\n",
    "output = model(_input.to_string())\n",
    "parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3601bde",
   "metadata": {
    "id": "d3601bde"
   },
   "source": [
    "보시다시피, 우리가 원래 원했던 스키마를 따르는 `Joke` 클래스의 출력을 얻습니다: `setup`, `punchline`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 연습문제\n",
    "\n",
    "### 1. 기본 추출 연습\n",
    "* 주어진 텍스트에서 언급된 사람들의 이름과 나이를 추출하는 파이썬 스크립트를 작성합니다.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "1. 스키마 정의: 사람의 이름과 나이를 추출하기 위한 스키마를 정의합니다. 이 스키마는 이름을 문자열로, 나이를 정수로 갖는 구조를 가집니다.\n",
    "\n",
    "2. 텍스트 입력: 추출할 정보를 포함하는 샘플 텍스트를 준비합니다.\n",
    "\n",
    "3. 추출 체인 생성: langchain 라이브러리를 사용하여 추출 체인을 생성합니다. 이 체인은 입력된 텍스트에서 이름과 나이 정보를 추출합니다.\n",
    "\n",
    "4. 추출 실행 및 결과 확인: 생성된 체인을 실행하여 텍스트에서 정보를 추출하고, 결과를 확인합니다.\n",
    "```\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "_gPL4ONYFgmK"
   },
   "id": "_gPL4ONYFgmK"
  },
  {
   "cell_type": "code",
   "source": [
    "# 스키마 정의\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"age\": {\"type\": \"integer\"}\n",
    "    },\n",
    "    \"required\": [\"name\", \"age\"]\n",
    "}\n",
    "\n",
    "# 텍스트 입력\n",
    "input_text = \"John is 30 years old. Mary is two years younger than John.\"\n",
    "\n",
    "# 추출 체인 생성\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "chain = create_extraction_chain(schema, llm)\n",
    "\n",
    "# 추출 실행\n",
    "result = chain.run(input_text)\n",
    "\n",
    "# 결과 출력\n",
    "print(result)"
   ],
   "metadata": {
    "id": "XUjshGcN-q1q",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706153357950,
     "user_tz": -540,
     "elapsed": 4413,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    },
    "outputId": "d9ebfffc-0c0e-4286-f962-9ba1dd3b6555"
   },
   "id": "XUjshGcN-q1q",
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'name': 'John', 'age': 30}, {'name': 'Mary', 'age': 28}]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# from langchain.chains import create_extraction_chain_pydantic\n",
    "\n",
    "class Properties(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "# 텍스트 입력\n",
    "input_text = \"John is 30 years old. Mary is two years younger than John.\"\n",
    "\n",
    "# 추출 체인 생성\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "chain = create_extraction_chain_pydantic(pydantic_schema=Properties, llm=llm)\n",
    "\n",
    "# 추출 실행\n",
    "result = chain.run(input_text)\n",
    "\n",
    "# 결과 출력\n",
    "print(result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2N_asPJjL6Xi",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706153453969,
     "user_tz": -540,
     "elapsed": 3757,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    },
    "outputId": "82953c8c-d783-46b8-b87e-300013618ba1"
   },
   "id": "2N_asPJjL6Xi",
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Properties(name='John', age=30), Properties(name='Mary', age=28)]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 복잡한 쿼리 처리\n",
    "* 서술 문단에서 이벤트에 대한 자세한 정보(예: 이벤트 이름, 날짜, 위치, 참가자)를 추출하는 스크립트를 작성합니다.\n",
    "\n",
    "\n",
    "```\n",
    "1. 이벤트 정보 스키마 정의: 이벤트 이름, 날짜, 위치, 참가자 등의 정보를 포함하는 스키마를 정의합니다.\n",
    "\n",
    "2. 텍스트 입력: 추출할 정보를 포함하는 복잡한 서술 문단을 준비합니다\n",
    "\n",
    "3. 추출 체인 생성 및 실행: langchain 라이브러리를 사용하여 추출 체인을 생성하고, 준비된 텍스트에서 정보를 추출합니다.\n",
    "\n",
    "4. 결과 확인 및 분석: 추출된 결과를 확인하고, 스크립트가 복잡한 쿼리를 어떻게 처리하는지 분석합니다.\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "dKYyaq_SGmn5"
   },
   "id": "dKYyaq_SGmn5"
  },
  {
   "cell_type": "code",
   "source": [
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"event_name\": {\"type\": \"string\"},\n",
    "        \"date\": {\"type\": \"string\"}, # 날짜검증이 필요할 수도\n",
    "        \"location\": {\"type\": \"string\"},\n",
    "        \"participants\": {\"type\": \"array\", \"items\":{\"type\": \"string\"}},\n",
    "    },\n",
    "    \"required\": [\"event_name\", \"date\", \"location\"]\n",
    "}\n",
    "\n",
    "# 텍스트 입력\n",
    "input_text = \"The annual science fair, which will be held on March 15th at the city hall, is expected to attract many notable scientists including Dr. Jane Doe and Prof. John Smith.\"\n",
    "\n",
    "# 추출 체인 생성\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "chain = create_extraction_chain(schema, llm)\n",
    "\n",
    "# 추출 실행\n",
    "result = chain.run(input_text)\n",
    "\n",
    "# 결과 출력\n",
    "print(result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VY2y3hinGNLS",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706153632547,
     "user_tz": -540,
     "elapsed": 4748,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    },
    "outputId": "05d7d52c-de87-452b-f449-f0ae54c70098"
   },
   "id": "VY2y3hinGNLS",
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'event_name': 'annual science fair', 'date': 'March 15th', 'location': 'city hall', 'participants': ['Dr. Jane Doe', 'Prof. John Smith']}]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class Event(BaseModel):\n",
    "    event_name: str\n",
    "    date: str\n",
    "    location: str\n",
    "    participants: list\n",
    "\n",
    "query = \"The annual science fair, which will be held on March 15th at the city hall, is expected to attract many notable scientists including Dr. Jane Doe and Prof. John Smith.\"\n",
    "\n",
    "# Set Parser\n",
    "parser = PydanticOutputParser(pydantic_object=Event)\n",
    "\n",
    "# Prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the query. \\n {format_instructions} \\m {query} \\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# run chain\n",
    "_input = prompt.format_prompt(query=query)\n",
    "model = OpenAI(temperature=0)\n",
    "outputs = model(_input.to_string())\n",
    "parser.parse(outputs)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bDPAPiX4VQvT",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706153718265,
     "user_tz": -540,
     "elapsed": 4615,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    },
    "outputId": "8eb3ee0f-e3b7-4f6d-95ef-069b294de0f3"
   },
   "id": "bDPAPiX4VQvT",
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Event(event_name='Annual Science Fair', date='March 15th', location='City Hall', participants=['Dr. Jane Doe', 'Prof. John Smith'])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class Event(BaseModel):\n",
    "    name: str\n",
    "    event_name: str\n",
    "    date: str\n",
    "    location: str\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Event Participants\"\"\"\n",
    "    participants: Sequence[Event]\n",
    "\n",
    "query = \"The annual science fair, which will be held on March 15th at the city hall, is expected to attract many notable scientists including Dr. Jane Doe and Prof. John Smith.\"\n",
    "\n",
    "# Set Parser\n",
    "parser = PydanticOutputParser(pydantic_object=People)\n",
    "\n",
    "# Prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the query. \\n {format_instructions} \\m {query} \\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# run chain\n",
    "_input = prompt.format_prompt(query=query)\n",
    "model = OpenAI(temperature=0)\n",
    "outputs = model(_input.to_string())\n",
    "parser.parse(outputs)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrnKdmzHWfS7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706153917948,
     "user_tz": -540,
     "elapsed": 6246,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    },
    "outputId": "d4b60008-706a-425d-9c83-40e0fb054813"
   },
   "id": "DrnKdmzHWfS7",
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "People(participants=[Event(name='Dr. Jane Doe', event_name='Annual Science Fair', date='March 15th', location='City Hall'), Event(name='Prof. John Smith', event_name='Annual Science Fair', date='March 15th', location='City Hall')])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 동적 스키마 생성\n",
    "* 사용자 입력에 기반하여 스키마를 자동으로 생성하는 파이썬 스크립트를 작성합니다. 이 스크립트는 사용자가 원하는 정보 유형을 입력하면 해당하는 스키마를 생성하고, 언어 모델을 사용하여 이 스키마에 따라 텍스트에서 정보를 추출합니다.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "1. 사용자 입력 처리: 사용자가 추출하고자 하는 정보 유형을 입력할 수 있는 인터페이스를 구현합니다. 예를 들어, 사용자가 \"이름\", \"나이\", \"직업\" 등을 입력할 수 있습니다.\n",
    "\n",
    "2. 동적 스키마 생성: 사용자의 입력을 기반으로 동적으로 스키마를 생성하는 함수를 구현합니다. 이 함수는 사용자가 입력한 정보 유형에 따라 적절한 스키마를 만듭니다.\n",
    "\n",
    "3. 추출 체인 설정 및 실행: 생성된 스키마를 사용하여 langchain 라이브러리의 추출 체인을 설정하고 실행합니다.\n",
    "\n",
    "4. 결과 출력 및 검증: 추출된 결과를 출력하고, 스크립트가 정확하게 사용자 요구 사항에 맞는 스키마를 생성했는지 검증합니다.\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "WanR5a9WIOi6"
   },
   "id": "WanR5a9WIOi6"
  },
  {
   "cell_type": "code",
   "source": [
    "def create_dynamic_schema(user_inputs):\n",
    "    schema = {\"properties\": {}, \"required\": []}\n",
    "    for input in user_inputs:\n",
    "        # 여기에서 각 입력 유형에 대한 스키마 정의를 추가합니다.\n",
    "        schema[\"properties\"][input] = {\"type\": \"string\"}  # 예시: 모든 필드를 문자열로 처리\n",
    "        schema[\"required\"].append(input)\n",
    "    return schema\n",
    "\n",
    "# 사용자 입력\n",
    "user_inputs = [\"name\", \"age\"]  # 예시: 사용자가 입력한 필드\n",
    "\n",
    "# 스키마 생성\n",
    "dynamic_schema = create_dynamic_schema(user_inputs)\n",
    "\n",
    "# 텍스트 입력\n",
    "input_text = \"John, a 30-year-old engineer, works at Acme Corp.\"\n",
    "\n",
    "# 추출 체인 생성\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "chain = create_extraction_chain(dynamic_schema, llm)\n",
    "\n",
    "# 추출 실행\n",
    "result = chain.run(input_text)\n",
    "\n",
    "# 결과 출력\n",
    "print(result)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1mD6pADTHS95",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706154156334,
     "user_tz": -540,
     "elapsed": 4641,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    },
    "outputId": "26aa2392-e98a-495f-a366-063d5b29feb2"
   },
   "id": "1mD6pADTHS95",
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'name': 'John', 'age': '30'}, {'name': 'engineer'}, {'name': 'Acme Corp.'}]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "vi5mss-BIJ6b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1706144450033,
     "user_tz": -540,
     "elapsed": 7,
     "user": {
      "displayName": "Junseop So (쏘주형)",
      "userId": "07758510494740838877"
     }
    }
   },
   "id": "vi5mss-BIJ6b",
   "execution_count": 15,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/langchain-ai/langchain/blob/master/docs/docs/use_cases/extraction.ipynb",
     "timestamp": 1701661588526
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

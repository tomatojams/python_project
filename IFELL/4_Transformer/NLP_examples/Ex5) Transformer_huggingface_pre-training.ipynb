{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jG82MsD9YPEn",
    "ExecuteTime": {
     "end_time": "2024-01-16T02:44:08.794297Z",
     "start_time": "2024-01-16T02:43:39.630286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\r\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/20/0a/739426a81f7635b422fbe6cb8d1d99d1235579a6ac8024c13d743efa6847/transformers-4.36.2-py3-none-any.whl.metadata\r\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m126.8/126.8 kB\u001B[0m \u001B[31m708.0 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting datasets\r\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/ec/93/454ada0d1b289a0f4a86ac88dbdeab54921becabac45da3da787d136628f/datasets-2.16.1-py3-none-any.whl.metadata\r\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: filelock in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from transformers) (3.13.1)\r\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\r\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.19.3 from https://files.pythonhosted.org/packages/3d/0a/aed3253a9ce63d9c90829b1d36bc44ad966499ff4f5827309099c8c9184b/huggingface_hub-0.20.2-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from transformers) (1.23.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from transformers) (23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from transformers) (2023.10.3)\r\n",
      "Requirement already satisfied: requests in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from transformers) (2.31.0)\r\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\r\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/9e/2d/35a9f9378962fd954cd097efc0d3b7beee86c12d4e61cf6199bdef58e091/tokenizers-0.15.0-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading tokenizers-0.15.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\r\n",
      "Collecting safetensors>=0.3.1 (from transformers)\r\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/ea/7c/a4a49a22fc082279d36c416f0fe2d89ca558fd214ab9e9afd889dfb8180f/safetensors-0.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading safetensors-0.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.8 kB)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from transformers) (4.66.1)\r\n",
      "Collecting pyarrow>=8.0.0 (from datasets)\r\n",
      "  Obtaining dependency information for pyarrow>=8.0.0 from https://files.pythonhosted.org/packages/4a/11/1c2d07b7e14bf3501b8946691e3df26302b71adb40fd24cc4056f78d3bcb/pyarrow-14.0.2-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading pyarrow-14.0.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.0 kB)\r\n",
      "Collecting pyarrow-hotfix (from datasets)\r\n",
      "  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\r\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\r\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\r\n",
      "  Obtaining dependency information for dill<0.3.8,>=0.3.0 from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\r\n",
      "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\r\n",
      "Requirement already satisfied: pandas in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from datasets) (2.1.1)\r\n",
      "Collecting xxhash (from datasets)\r\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/a4/24/8092b5db6d459cb9e62409edfebb0253b3760d9ab7e4c52e5493c3b026b1/xxhash-3.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading xxhash-3.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\r\n",
      "Collecting multiprocess (from datasets)\r\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/c6/c9/820b5ab056f4ada76fbe05bd481a948f287957d6cbfd59e2dd2618b408c1/multiprocess-0.70.15-py39-none-any.whl.metadata\r\n",
      "  Downloading multiprocess-0.70.15-py39-none-any.whl.metadata (7.2 kB)\r\n",
      "Collecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets)\r\n",
      "  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\r\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\r\n",
      "Collecting aiohttp (from datasets)\r\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/69/73/bda9aba49fa741c5390368e91a9a36012041f48d542772af097c43dd0963/aiohttp-3.9.1-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading aiohttp-3.9.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.4 kB)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\r\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\r\n",
      "  Downloading multidict-6.0.4-cp39-cp39-macosx_11_0_arm64.whl (29 kB)\r\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\r\n",
      "  Obtaining dependency information for yarl<2.0,>=1.0 from https://files.pythonhosted.org/packages/f9/b0/c213007560d001c9908649ff4b1dd11d1ff388235e773828e19d4637f502/yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (31 kB)\r\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\r\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/8b/c9/a81e9af48291954a883d35686f32308238dc968043143133b8ac9e2772af/frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\r\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\r\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\r\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\r\n",
      "  Obtaining dependency information for async-timeout<5.0,>=4.0 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\r\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.7.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from requests->transformers) (3.1.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from requests->transformers) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from pandas->datasets) (2022.7)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.15.0)\r\n",
      "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.2/8.2 MB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m507.1/507.1 kB\u001B[0m \u001B[31m1.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m115.3/115.3 kB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading aiohttp-3.9.1-cp39-cp39-macosx_11_0_arm64.whl (387 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m387.3/387.3 kB\u001B[0m \u001B[31m2.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m330.3/330.3 kB\u001B[0m \u001B[31m2.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading pyarrow-14.0.2-cp39-cp39-macosx_11_0_arm64.whl (24.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m24.0/24.0 MB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading safetensors-0.4.1-cp39-cp39-macosx_11_0_arm64.whl (426 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m426.1/426.1 kB\u001B[0m \u001B[31m3.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0mm\r\n",
      "\u001B[?25hDownloading tokenizers-0.15.0-cp39-cp39-macosx_11_0_arm64.whl (2.5 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.5/2.5 MB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading multiprocess-0.70.15-py39-none-any.whl (133 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m133.3/133.3 kB\u001B[0m \u001B[31m8.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\r\n",
      "Downloading xxhash-3.4.1-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\r\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\r\n",
      "Downloading frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl (53 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m53.7/53.7 kB\u001B[0m \u001B[31m6.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.4/166.4 kB\u001B[0m \u001B[31m2.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m0:01\u001B[0m\r\n",
      "\u001B[?25hDownloading yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl (81 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m81.8/81.8 kB\u001B[0m \u001B[31m3.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: xxhash, safetensors, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, datasets\r\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.16.1 dill-0.3.7 frozenlist-1.4.1 fsspec-2023.10.0 huggingface-hub-0.20.2 multidict-6.0.4 multiprocess-0.70.15 pyarrow-14.0.2 pyarrow-hotfix-0.6 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.36.2 xxhash-3.4.1 yarl-1.9.4\r\n"
     ]
    }
   ],
   "source": [
    "# Transformers installation\n",
    "! pip install transformers datasets\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWIi6sDrYPEo"
   },
   "source": [
    "# Fine-tune a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pt768tyUYPEp"
   },
   "source": [
    "사전 학습된 모델을 사용하면 상당한 이점이 있습니다. 계산 비용과 탄소 배출량을 줄이고 처음부터 훈련하지 않고도 최신 모델을 사용할 수 있습니다. 🤗 Transformers는 다양한 작업을 위해 수천 개의 사전 훈련된 모델에 대한 액세스를 제공합니다. 사전 훈련된 모델을 사용하는 경우 작업에 특정한 데이터 세트에서 모델을 훈련합니다. 이 튜토리얼에서는 선택한 딥러닝 프레임워크로 사전 학습된 모델을 미세 조정합니다.\n",
    "\n",
    "* Fine-tune a pretrained model with 🤗 Transformers [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer).\n",
    "* Fine-tune a pretrained model in TensorFlow with Keras.\n",
    "\n",
    "<a id='data-processing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ad9QLLWYPEq"
   },
   "source": [
    "## Prepare a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzFuIrndYPEq"
   },
   "source": [
    "사전 훈련된 모델을 미세 조정하기 전에 데이터 세트를 다운로드하고 훈련을 위해 준비하세요. \n",
    "\n",
    "Begin by loading the [Yelp Reviews](https://huggingface.co/datasets/yelp_review_full) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DnOWzpWgYPEq",
    "ExecuteTime": {
     "end_time": "2024-01-16T02:46:07.901056Z",
     "start_time": "2024-01-16T02:44:13.588054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/6.72k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a751b94981d6445d9924cb88d86eb4ed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/299M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19349f2d560048a69acd802334eaa892"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/23.5M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2381717df87548c9bed86dd27aca3d56"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/650000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cebc73da927a484d89f5b64b229165cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/50000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c8012e6f4da84bf8963ee1ea94083fc8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "{'label': 0,\n 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\\"serving off their orders\\\\\" when they didn\\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\'ve eaten at various McDonalds restaurants for over 30 years. I\\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q98PAZXeYPEr"
   },
   "source": [
    "텍스트를 처리하고 가변 시퀀스 길이를 처리하기 위한 패딩 및 truncation 전략을 포함하려면 토크나이저가 필요합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PcQAADoWYPEr",
    "ExecuteTime": {
     "end_time": "2024-01-16T02:47:57.994127Z",
     "start_time": "2024-01-16T02:46:07.900712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7bf6a748a51c44a4a6c2d6b58d54dc59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "818f4667a7d14e5284719742049053ae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f81803d9bc74fd2940d6c9369af820a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8c5710510944a38888731558802ddb0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/650000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26655bb3880a4d2492887372fb68a70d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29569bfcd64d47368575aa57518ff47b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUSU1cTkYPEr"
   },
   "source": [
    "원하는 경우 전체 데이터 세트의 더 작은 하위 집합을 만들어 미세 조정하여 소요 시간을 줄일 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFprVB42YPEr",
    "ExecuteTime": {
     "start_time": "2024-01-16T03:06:25.449344Z"
    }
   },
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMZ9jGOBYPEs"
   },
   "source": [
    "<a id='trainer'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SralBKDRYPEs"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0s0gHSwYPEs"
   },
   "source": [
    "🤗 Transformers는 교육에 최적화된 [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) 클래스를 제공합니다. 자신의 훈련 루프. [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) API는 로깅, 기울기 누적, 혼합 정밀도와 같은 다양한 훈련 옵션과 기능을 지원합니다.\n",
    "\n",
    "모델을 로드하여 시작하고 예상 레이블 수를 지정합니다. Yelp Review [데이터 세트 카드](https://huggingface.co/datasets/yelp_review_full#data-fields)에서 5개의 레이블이 있음을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyFEy_lQYPEs",
    "ExecuteTime": {
     "start_time": "2024-01-16T03:06:25.449437Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQwEc4UJYPEs"
   },
   "source": [
    "### Training hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cF-akgyNYPEs"
   },
   "source": [
    "다음으로 조정할 수 있는 모든 하이퍼파라미터와 다양한 훈련 옵션을 활성화하기 위한 플래그가 포함된 [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) 클래스를 만듭니다. \n",
    "\n",
    "이 튜토리얼의 경우 기본 [하이퍼파라미터](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)로 시작할 수 있지만 자유롭게 실험하여 최적의 설정을 찾으십시오.\n",
    "\n",
    "교육에서 체크포인트를 저장할 위치를 지정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BHE2m__mYPEs",
    "ExecuteTime": {
     "end_time": "2024-01-16T03:00:47.737858Z",
     "start_time": "2024-01-16T03:00:47.687284Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBlN8CxeYPEs"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wftvJTpUYPEt"
   },
   "source": [
    "[트레이너](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)는 트레이닝 중에 모델 성능을 자동으로 평가하지 않습니다. 메트릭을 계산하고 보고하려면 [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)에 함수를 전달해야 합니다. 🤗 데이터셋 라이브러리에서는 `load_metric`(자세한 내용은 이 [튜토리얼](https://huggingface.co/docs/datasets/metrics.html) 함수를 참조하세요) 함수와 함께 로드할 수 있는 간단한 [`accuracy`](https://huggingface.co/metrics/accuracy) 함수를 제공합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "E3YrEnhoYPEt",
    "ExecuteTime": {
     "end_time": "2024-01-16T03:00:54.089232Z",
     "start_time": "2024-01-16T03:00:49.696589Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b4/sv3y8crx3jx6kn5k220bs8bh0000gn/T/ipykernel_66702/2135776078.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n",
      "/Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages/datasets/load.py:752: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88cdf1f54be2482e9e4b5807e52db959"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0T4dE7SdYPEt"
   },
   "source": [
    "`metric`에서 `compute`를 호출하여 예측의 정확도를 계산합니다. 계산`에 예측을 전달하기 전에 예측을 로그로 변환해야 합니다(모든 🤗 트랜스포머 모델은 로그를 반환한다는 점을 기억하세요):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qsu8oe3mYPEt",
    "ExecuteTime": {
     "end_time": "2024-01-16T03:00:54.114702Z",
     "start_time": "2024-01-16T03:00:54.089927Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBLY7W2fYPEt"
   },
   "source": [
    "미세 조정 중에 평가 지표를 모니터링하려면 훈련 인수에 `evaluation_strategy` 매개 변수를 지정하여 각 에포크가 끝날 때 평가 지표를 보고하세요:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "90PrvJHuYPEt",
    "ExecuteTime": {
     "end_time": "2024-01-16T03:00:54.141517Z",
     "start_time": "2024-01-16T03:00:54.101541Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvIl7C95YPEt"
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veLNmLXPYPEt"
   },
   "source": [
    "모델, 훈련 인수, 훈련 및 테스트 데이터 세트, 평가 함수를 사용하여 [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) 개체를 만듭니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LB34Rs1CYPEt",
    "ExecuteTime": {
     "end_time": "2024-01-16T03:00:54.209357Z",
     "start_time": "2024-01-16T03:00:54.135827Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m----> 2\u001B[0m     model\u001B[38;5;241m=\u001B[39m\u001B[43mmodel\u001B[49m,\n\u001B[1;32m      3\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m      4\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39msmall_train_dataset,\n\u001B[1;32m      5\u001B[0m     eval_dataset\u001B[38;5;241m=\u001B[39msmall_eval_dataset,\n\u001B[1;32m      6\u001B[0m     compute_metrics\u001B[38;5;241m=\u001B[39mcompute_metrics,\n\u001B[1;32m      7\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNkIhvl5YPEt"
   },
   "source": [
    "Then fine-tune your model by calling [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64mBHIIwYPEt",
    "ExecuteTime": {
     "start_time": "2024-01-16T03:00:54.197997Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "co9uV5XpYPEt"
   },
   "source": [
    "<a id='keras'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSqrmVbMYPEt"
   },
   "source": [
    "🤗 트랜스포머 모델은 Keras API를 통해 텐서플로우의 트레이닝도 지원합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QMc_CsmYPEt"
   },
   "source": [
    "### Convert dataset to TensorFlow format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81RTdn7VYPEt"
   },
   "source": [
    "[DefaultDataCollator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator)는 모델이 학습할 텐서를 일괄 처리로 조립합니다. 텐서플로 텐서를 반환하려면 `return_tensors`를 지정해야 합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "43IPsyWoYPEt",
    "ExecuteTime": {
     "end_time": "2024-01-16T03:01:00.341024Z",
     "start_time": "2024-01-16T03:00:54.389419Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYwPhTJsYPEt"
   },
   "source": [
    "<Tip>\n",
    "\n",
    "[트레이너](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)는 기본적으로 [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding)을 사용하므로 데이터 콜레이터를 명시적으로 지정할 필요가 없습니다.\n",
    "\n",
    "</Tip>\n",
    "\n",
    "다음으로, [`to_tf_dataset`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset) 메서드를 사용하여 토큰화된 데이터셋을 텐서플로 데이터셋으로 변환합니다. 입력은 `columns`에, 레이블은 `label_cols`에 지정합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ECvyhmAnYPEu",
    "ExecuteTime": {
     "end_time": "2024-01-16T03:01:02.093316Z",
     "start_time": "2024-01-16T03:01:00.343781Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soma/anaconda3/envs/tensor2/lib/python3.9/site-packages/datasets/arrow_dataset.py:400: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n",
      "2024-01-16 12:01:00.375512: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-01-16 12:01:00.376493: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    }
   ],
   "source": [
    "tf_train_dataset = small_train_dataset.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "\n",
    "tf_validation_dataset = small_eval_dataset.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator, # map합수에 해당 토치텐서를 텐서플로우 텐서로 변환하는 함수를 넣어준다.\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTrHUaH3YPEu"
   },
   "source": [
    "### Compile and fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUYOaT9SYPEu"
   },
   "source": [
    "예상 레이블 수로 TensorFlow 모델을 로드합니다:"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T03:11:25.990439Z",
     "start_time": "2024-01-16T03:11:21.308632Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "그런 다음 다른 Keras 모델과 마찬가지로 [`fit`](https://keras.io/api/models/model_training_apis/)을 사용하여 모델을 컴파일하고 미세 조정합니다:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lZumOFlOYPEu",
    "ExecuteTime": {
     "end_time": "2024-01-16T03:13:11.899375Z",
     "start_time": "2024-01-16T03:11:33.009020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 12:11:39.279207: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/125 [..............................] - ETA: 36:08 - loss: 1.7206 - sparse_categorical_accuracy: 0.1562"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 7\u001B[0m\n\u001B[1;32m      1\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(\n\u001B[1;32m      2\u001B[0m     optimizer\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mAdam(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5e-5\u001B[39m),\n\u001B[1;32m      3\u001B[0m     loss\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlosses\u001B[38;5;241m.\u001B[39mSparseCategoricalCrossentropy(from_logits\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m      4\u001B[0m     metrics\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mSparseCategoricalAccuracy(),\n\u001B[1;32m      5\u001B[0m )\n\u001B[0;32m----> 7\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtf_train_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf_validation_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/keras/engine/training.py:1564\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1556\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1557\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1558\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1561\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   1562\u001B[0m ):\n\u001B[1;32m   1563\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1564\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1565\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1566\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    912\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    914\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 915\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    917\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    918\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    944\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    945\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    946\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 947\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stateless_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[1;32m    948\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stateful_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    949\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    950\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    951\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2493\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m   2494\u001B[0m   (graph_function,\n\u001B[1;32m   2495\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[0;32m-> 2496\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2497\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgraph_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1858\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1859\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1860\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1861\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1862\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1863\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1864\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1865\u001B[0m     args,\n\u001B[1;32m   1866\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1867\u001B[0m     executing_eagerly)\n\u001B[1;32m   1868\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    498\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 499\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    505\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    506\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m    507\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[1;32m    508\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    511\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[1;32m    512\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[0;32m~/anaconda3/envs/tensor2/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 54\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     57\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=tf.metrics.SparseCategoricalAccuracy(),\n",
    ")\n",
    "\n",
    "model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ihxfklkYPEu"
   },
   "source": [
    "<a id='pytorch_native'></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "SbGfBzrrZMrs",
    "ExecuteTime": {
     "end_time": "2024-01-16T03:06:25.449806Z",
     "start_time": "2024-01-16T03:06:25.448742Z"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/huggingface/notebooks/blob/master/transformers_doc/training.ipynb",
     "timestamp": 1683448994633
    }
   ],
   "machine_shape": "hm",
   "gpuType": "T4",
   "collapsed_sections": [
    "_QMc_CsmYPEt",
    "FTrHUaH3YPEu"
   ]
  },
  "accelerator": "GPU",
  "gpuClass": "standard",
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

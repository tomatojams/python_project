{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Q9GvJiJ9gzgm"},"outputs":[],"source":["!mkdir -p ~/data\n","!mkdir -p ~/models"]},{"cell_type":"code","source":["!wget https://d3s0tskafalll9.cloudfront.net/media/documents/song_lyrics.zip\n","!unzip song_lyrics.zip -d ~/data/lyrics"],"metadata":{"id":"XGcJB7zkg9CF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls ~/data/lyrics"],"metadata":{"id":"dtCAky7Dum3E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import glob\n","import os\n","import tensorflow as tf\n","\n","txt_file_path = os.getenv('HOME')+'/data/lyrics/*'\n","\n","txt_list = glob.glob(txt_file_path)\n","\n","raw_corpus = []\n","\n","# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n","for txt_file in txt_list:\n","    with open(txt_file, \"r\") as f:\n","        raw = f.read().splitlines()\n","        raw_corpus.extend(raw)\n","\n","print(\"데이터 크기:\", len(raw_corpus))\n","print(\"Examples:\\n\", raw_corpus[:3])"],"metadata":{"id":"47yZzPsVg-Gv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for idx, sentence in enumerate(raw_corpus):\n","    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n","    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n","\n","    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n","\n","    print(sentence)"],"metadata":{"id":"XVh804JbhDvK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 입력된 문장을\n","#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n","#     2. 특수문자 양쪽에 공백을 넣고\n","#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n","#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n","#     5. 다시 양쪽 공백을 지웁니다\n","#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n","# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n","def preprocess_sentence(sentence):\n","    sentence = sentence.lower().strip() # 1\n","    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n","    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n","    sentence = sentence.strip() # 5\n","    sentence = '<start> ' + sentence + ' <end>' # 6\n","    return sentence\n","\n","# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n","print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"],"metadata":{"id":"zeAxj8DfhGaT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 여기에 정제된 문장을 모을겁니다\n","corpus = []\n","\n","for sentence in raw_corpus:\n","    # 우리가 원하지 않는 문장은 건너뜁니다\n","    if len(sentence) == 0: continue\n","    if sentence[-1] == \":\": continue\n","\n","    # 정제를 하고 담아주세요\n","    preprocessed_sentence = preprocess_sentence(sentence)\n","    corpus.append(preprocessed_sentence)\n","\n","# 정제된 결과를 10개만 확인해보죠\n","corpus[:10]"],"metadata":{"id":"Y70II8rMhJIn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n","# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n","# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n","# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n","def tokenize(corpus):\n","    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n","    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n","    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        num_words=10000,\n","        filters=' ',\n","        oov_token=\"<unk>\"\n","    )\n","    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n","    tokenizer.fit_on_texts(corpus)\n","    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n","    tensor = tokenizer.texts_to_sequences(corpus)\n","    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n","    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n","    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=15,\n","                                                           padding=#TODO)\n","\n","    print(tensor,tokenizer)\n","    return tensor, tokenizer\n","\n","tensor, tokenizer = tokenize(corpus)"],"metadata":{"id":"XL-eDUdGhL-C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tensor[:3, :10])"],"metadata":{"id":"1BYgp1RqhRK8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for idx in tokenizer.index_word:\n","    print(idx, \":\", tokenizer.index_word[idx])\n","\n","    if idx >= 10: break"],"metadata":{"id":"7tvBtTEChRaE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n","# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n","src_input = tensor[:, :-1]\n","# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n","tgt_input = tensor[:, 1:]\n","\n","print(src_input[0])\n","print(tgt_input[0])"],"metadata":{"id":"xU9CW7SUhUy4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BUFFER_SIZE = len(src_input)\n","BATCH_SIZE = 256\n","steps_per_epoch = len(src_input) // BATCH_SIZE\n","\n"," # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n","VOCAB_SIZE = len(tokenizer.index_word) + 1\n","print(VOCAB_SIZE)\n","\n","# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n","# 데이터셋에 대해서는 아래 문서를 참고하세요\n","# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n","# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n","dataset = tf.data.Dataset.from_tensor_slices((#TODO, #TODO))\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","dataset"],"metadata":{"id":"II-mftHXhW3K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TextGenerator(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_size, hidden_size):\n","        super().__init__()\n","\n","        self.embedding = #TODO\n","        self.rnn_1 = #TODO\n","        self.rnn_2 = #TODO\n","        self.linear = #TODO\n","\n","    def call(self, x):\n","        out = self.embedding(x)\n","        out = self.rnn_1(out)\n","        out = self.rnn_2(out)\n","        out = self.linear(out)\n","\n","        return out\n","\n","embedding_size = 256\n","hidden_size = 1024\n","model = TextGenerator(VOCAB_SIZE, embedding_size , hidden_size)"],"metadata":{"id":"6fHb3rg9hY3i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n","for src_sample, tgt_sample in dataset.take(1): break\n","\n","# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n","model(src_sample)"],"metadata":{"id":"FHBHyr_6hbTk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"83iaRiZqhdHN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# optimizer와 loss등은 차차 배웁니다\n","# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n","\n","# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n","# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n","\n","optimizer = tf.keras.optimizers.Adam()\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True,\n","    reduction='none'\n",")\n","\n","model.compile(loss=loss, optimizer=optimizer)\n","\n","# model.fit() 함수에 들어가는 다양한 인자를 알고 싶다면 아래의 문서를 참고하세요.\n","# https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n","\n","model.fit(#TODO, epochs=30)"],"metadata":{"id":"i38Lj6Jmheoo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n","    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n","    test_input = tokenizer.texts_to_sequences([init_sentence])\n","    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n","    end_token = tokenizer.word_index[\"<end>\"]\n","\n","    # 단어 하나씩 예측해 문장을 만듭니다\n","    #    1. 입력받은 문장의 텐서를 입력합니다\n","    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n","    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n","    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n","    while True:\n","        # 1\n","        predict = model(test_tensor)\n","        tmp = 1 # 1을 기준으로 작거나 큰 값\n","        predict = predict / tmp\n","\n","        # 2\n","        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n","        # predict_word = tf.random.categorical(tf.nn.softmax(predict, axis=-1)[:, -1], 1)\n","\n","        # 3\n","        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n","        # test_tensor = tf.concat([test_tensor, predict_word], axis=-1)\n","        # 4\n","        if predict_word.numpy()[0] == end_token: break\n","        if test_tensor.shape[1] >= max_len: break\n","\n","    generated = \"\"\n","    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다\n","    for word_index in test_tensor[0].numpy():\n","        generated += tokenizer.index_word[word_index] + \" \"\n","\n","    return generated"],"metadata":{"id":"iX_iOzt1hhZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generate_text(model, tokenizer, init_sentence=\"<start> i\")"],"metadata":{"id":"nPQWEoQKhj2F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gaI8vHX5bP89"},"execution_count":null,"outputs":[]}]}
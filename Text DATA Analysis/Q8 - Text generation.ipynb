{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hcD2nPQvPOFM"},"source":["# Text Generation with RNN"]},{"cell_type":"markdown","metadata":{"id":"YpaIgZGRrSaw"},"source":["* RNN을 사용하여 어떻게 텍스트를 생성하는지 알아보자.\n","* 주어진 텍스트를 기반으로 텍스트를 생성하는 모델을 구현해보자."]},{"cell_type":"markdown","metadata":{"id":"BwpJ5IffzRG6"},"source":["### 생성의 한계\n","\n","문장 중 일부는 문법적으로 맞지만 대부분 자연스럽지 않다.\n","\n","이 모델은 단어의 의미를 학습하지는 않았지만, 고려해야 할 점으로:\n","\n","* 데이터는 문자 기반이다. 훈련이 시작되었을 때, 이 모델은 영어 단어의 철자를 모르고 심지어 텍스트의 단위가 단어라는 것도 모른다."]},{"cell_type":"markdown","metadata":{"id":"srXC6pLGLwS6"},"source":["## 설정"]},{"cell_type":"markdown","metadata":{"id":"46TMSZVszoc9"},"source":["### Drive 연결\n"]},{"cell_type":"code","metadata":{"id":"YiWZ-S31zokw"},"source":["use_colab = True\n","assert use_colab in [True, False]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LjWuSiewzw-R"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yG_n40gFzf9s"},"source":["import tensorflow as tf\n","\n","import numpy as np\n","import os\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EHDoRoc5PKWz"},"source":["### 셰익스피어 데이터셋 다운로드"]},{"cell_type":"code","metadata":{"id":"pD_55cOxLkAb"},"source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UHjdCjDuSvX_"},"source":["### 데이터 읽기"]},{"cell_type":"code","metadata":{"id":"aavnuByVymwK"},"source":["# 데이터를 불러와서 디코딩\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","\n","# 문자의 수\n","print ('텍스트의 길이: {}'.format(len(text)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Duhg9NrUymwO"},"source":["# 텍스트 처음 250자 출력\n","print(text[1500:2500])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IlCgQBRVymwR"},"source":["# 파일의 고유 문자수를 출력\n","vocab = sorted(set(text)) # 내가 불러온 text 데이터를 집합으로 만들어서 정렬시킨 상태입니다.\n","print ('고유 문자수 {}개'.format(len(vocab)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rNnrKn_lL-IJ"},"source":["## 텍스트 처리"]},{"cell_type":"markdown","metadata":{"id":"LFjSVAlWzf-N"},"source":["### 텍스트 벡터화\n","\n","* 학습을 위해서 텍스트들을 수치화할 필요가 있다.\n","* 텍스트를 인덱스화 시켜 학습에 사용"]},{"cell_type":"code","metadata":{"id":"IalZLbvOzf-F"},"source":["# 고유 문자에서 인덱스로 매핑 생성\n","char2idx = {u:i for i, u in enumerate(vocab)}\n","idx2char = np.array(vocab) # idx <=> char 변환할 수 있는 사전을 들고있는 것이 중요합니다.\n","\n","text_as_int = np.array([char2idx[c] for c in text]) # 람다식"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OPQ2A1mwOQUd"},"source":["idx2char"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YdVhhjYa31Uk"},"source":["print(text_as_int)\n","text_as_int.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tZfqhkYCymwX"},"source":["* 텍스트 0번부터 전체 텍스트 길이까지 인덱스화"]},{"cell_type":"code","metadata":{"id":"FYyNlCNXymwY"},"source":["print('{')\n","for char,_ in zip(char2idx, range(65)):\n","    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l1VKcQHcymwb"},"source":["# 텍스트 맵핑\n","print ('{} ---- Index ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bbmsf23Bymwe"},"source":["### 예측 과정"]},{"cell_type":"markdown","metadata":{"id":"wssHQ1oGymwe"},"source":["주어진 문자나 문자 시퀀스가 주어졌을 때, 다음 문자로 가장 가능성 있는 문자는 무엇일까?\n","\n","* 이는 모델을 훈련하여 수행할 작업이다.\n","* 모델의 입력은 문자열 시퀀스가 될 것이고, 모델을 훈련시켜 출력을 예측한다.\n","* 이 출력은 현재 타임 스텝(time step)의 다음 문자이다."]},{"cell_type":"markdown","metadata":{"id":"hgsVvVxnymwf"},"source":["### 훈련 샘플과 타깃 만들기\n","\n","* 다음으로 텍스트를 샘플 시퀀스로 나누자.\n","\n","* 각 입력 시퀀스에는 텍스트에서 나온 `seq_length`개의 문자가 포함된다.\n","\n","* 각 입력 시퀀스에서, 해당 타깃은 한 문자를 오른쪽으로 이동한 것을 제외하고는 동일한 길이의 텍스트를 포함한다.\n","\n","* 텍스트를`seq_length + 1`개의 청크(chunk)로 나누자\n","    * 예를 들어, `seq_length`는 4이고 텍스트를 \"Hello\"이라고 가정해 봅시다. 입력 시퀀스는 \"Hell\"이고 타깃 시퀀스는 \"ello\"가 된다.\n","\n","* 이렇게 하기 위해 먼저 `tf.data.Dataset.from_tensor_slices` 함수를 사용해 텍스트 벡터를 문자 인덱스의 스트림으로 변환한다."]},{"cell_type":"code","metadata":{"id":"HjXwjMSQbNnD"},"source":["# Hello 라는 단어를 만들고 싶습니다.\n","# 학습 데이터를 아래와 같이 세팅을 해줘야합니다.\n","# input : H -> e -> l -> l\n","# output : e -> l -> l -> o"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0UHJDA39zf-O"},"source":["# 단일 입력에 대해 원하는 문장의 최대 길이\n","seq_length = 100\n","examples_per_epoch = len(text)//seq_length\n","\n","# 훈련 샘플/타깃 만들기\n","char_dataset = tf.data.Dataset.from_tensor_slices(#) # 알파벳을 하나씩 생성합니다.\n","\n","for i in char_dataset.take(5):\n","    print(i.numpy())\n","    print(idx2char[i.numpy()])#확인가능"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-ZSYAcQV8OGP"},"source":["* `batch`를 이용해 몇개의 텍스트를 가져올 것인지 정할 수 있다."]},{"cell_type":"code","metadata":{"id":"l4hkDU3i7ozi"},"source":["sequences = char_dataset.batch(#, drop_remainder=True) # 문장을 가져와합니다.\n","                               # 문장을 가져올 수 있도록 batch 를 구성해줍니다.\n","\n","for item in sequences.take(5):\n","    print(repr(''.join(idx2char[item.numpy()]))) # repr 개행문자 출력"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UbLcIPBj_mWZ"},"source":["각 시퀀스에서, `map` 메서드를 사용해 각 배치에 간단한 함수를 적용하고 입력 텍스트와 타깃 텍스트를 복사 및 이동\n","* 모델이 Text를 생성할때, 한 텍스트 단위로 생성하기 때문에 그 다음 텍스트를 target으로 사용"]},{"cell_type":"code","metadata":{"id":"9NGu-FkO_kYU"},"source":["# Hello -> 불러온 데이터의 길이\n","# input : Hell # input의 길이\n","# output : ello # output의 길이\n","\n","def split_input_target(chunk):\n","    input_text = #\n","    target_text = #\n","    return input_text, target_text\n","\n","dataset = sequences.map(split_input_target)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hiCopyGZymwi"},"source":["첫 번째 샘플의 타깃 값을 출력해보자"]},{"cell_type":"code","metadata":{"id":"GNbw-iR0ymwj"},"source":["for input_example, target_example in  dataset.take(1):\n","    print ('입력 데이터: ', repr(''.join(idx2char[input_example.numpy()])))\n","    print ('타깃 데이터: ', repr(''.join(idx2char[target_example.numpy()])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_33OHL3b84i0"},"source":["* 이 벡터의 각 인덱스는 하나의 타임 스텝(time step)으로 처리됩니다. 타임 스텝 0의 입력으로 모델은 \"F\"의 인덱스를 받고 다음 문자로 \"i\"의 인덱스를 예측한다.\n","\n","* 다음 타임 스텝에서도 같은 일을 하지만 RNN은 현재 입력 문자 외에 이전 타임 스텝의 컨텍스트**(context)**를 고려한다."]},{"cell_type":"code","metadata":{"id":"0eBu9WZG84i0"},"source":["for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n","    print(\"iter {:4d}\".format(i))\n","    print(\"  inputs: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n","    print(\"  generated text: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MJdfPmdqzf-R"},"source":["### 훈련 배치 생성\n","\n","* 텍스트를 다루기 쉬운 시퀀스로 분리하기 위해 `tf.data`를 사용\n","* 이 데이터를 모델에 넣기 전에 데이터를 섞은 후 배치를 만들어야 한다."]},{"cell_type":"code","metadata":{"id":"p2pGotuNzf-S"},"source":["# 배치 크기\n","BATCH_SIZE = #\n","\n","dataset = dataset.shuffle(10000).batch(BATCH_SIZE, drop_remainder=True)\n","\n","dataset\n","# 알파벳을 기준으로 했던 데이터에서 문장단위로 데이터를 불러오는 dataset을 구성할 수 있습니다."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r6oUuElIMgVx"},"source":["## 모델 설계"]},{"cell_type":"markdown","metadata":{"id":"m8gPwEjRzf-Z"},"source":["모델을 정의하려면 `tf.keras.Sequential`을 사용한다.\n","\n","이 예제에서는 3개의 층을 사용하여 모델을 정의한다:\n","\n","* `tf.keras.layers.Embedding` : 입력층. `embedding_dim` 차원 벡터에 각 문자의 정수 코드를 매핑하는 훈련 가능한 검색 테이블.\n","* `tf.keras.layers.GRU` : 크기가 `units = rnn_units`인 RNN의 유형(여기서 LSTM층을 사용할 수도 있다.)\n","* `tf.keras.layers.Dense` : 크기가 `vocab_size`인 출력을 생성하는 출력층.\n","\n","각 문자에 대해 모델은 임베딩을 검색하고, 임베딩을 입력으로 하여 GRU를 1개의 타임 스텝으로 실행하고, FC layers를 적용하여 다음 문자의 로그 가능도(log-likelihood)를 예측하는 로짓을 생성한다:"]},{"cell_type":"code","metadata":{"id":"zHT8cLh7EAsg"},"source":["# 문자로 된 어휘 사전의 크기\n","vocab_size = #TODO\n","\n","# 임베딩 차원\n","embedding_dim = # TODO\n","\n","# RNN 유닛(unit) 개수\n","rnn_units = # TODO"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wwsrpOik5zhv"},"source":["def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","    model = tf.keras.Sequential([\n","        tf.keras.layers.Embedding(#TODO, #TODO,\n","                                batch_input_shape=[batch_size, None]),\n","        tf.keras.layers.LSTM(#TODO,\n","                            return_sequences=#,\n","                            stateful=True,\n","                            recurrent_initializer='glorot_uniform'),\n","        tf.keras.layers.Dense(#TODO)\n","    ])\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xgzQLegT2H_J"},"source":["model = build_model(\n","    vocab_size = #,\n","    embedding_dim=#,\n","    rnn_units=#,\n","    batch_size=BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbNnqjvhI9lk"},"source":["for layer in model.layers:\n","    print(layer.output_shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-ubPo0_9Prjb"},"source":["## 모델 사용\n","\n","이제 모델을 실행하여 원하는대로 동작하는지 확인해보자.\n","\n","먼저 출력의 형태를 확인하자."]},{"cell_type":"code","metadata":{"id":"C-_70kKAPrPU"},"source":["for input_example_batch, target_example_batch in dataset.take(1):\n","    example_batch_predictions = model(input_example_batch)\n","    print(example_batch_predictions.shape, \"# (배치 크기, 시퀀스 길이, 어휘 사전 크기)\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q6NzLBi4VM4o"},"source":["위 예제에서 입력의 시퀀스 길이는 100이지만 모델은 임의 시퀀스 길이의 입력도 사용 가능하다."]},{"cell_type":"code","metadata":{"id":"vPGmAAXmVLGC"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LJL0Q0YPY6Ee"},"source":["## 모델 훈련"]},{"cell_type":"markdown","metadata":{"id":"YCbHQHiaa4Ic"},"source":["이 문제는 표준 분류 문제로 취급될 수 있습니다. 이전 RNN 상태와 이번 타임 스텝(time step)의 입력으로 다음 문자의 클래스를 예측한다."]},{"cell_type":"markdown","metadata":{"id":"trpqTWyvk0nr"},"source":["### Optimizer, Loss function"]},{"cell_type":"markdown","metadata":{"id":"UAjbjY03eiQ4"},"source":["`tf.keras.losses.sparse_softmax_crossentropy` 를 사용해 label을 벡터로 바꾸지 않고 loss를 계산한다.\n","\n","이 모델은 로짓을 반환하기 때문에 `from_logits` 플래그를 설정해야 한다."]},{"cell_type":"code","metadata":{"id":"4HrXTACTdzY-"},"source":["def loss(labels, logits):\n","    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=#)\n","\n","example_batch_loss = loss(target_example_batch, example_batch_predictions)\n","print(\"예측 배열 크기(shape): \", example_batch_predictions.shape, \" # (배치 크기, 시퀀스 길이, 어휘 사전 크기)\")\n","print(\"Loss: \", example_batch_loss.numpy().mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jeOXriLcymww"},"source":["### 학습준비\n","* `tf.keras.Model.compile` 메서드를 사용하여 훈련 절차를 설정\n","* 기본 매개변수의 `tf.keras.optimizers.Adam`과 손실 함수를 사용"]},{"cell_type":"code","metadata":{"id":"DDl1_Een6rL0"},"source":["model.compile(optimizer=tf.keras.optimizers.Adam(#TODO),\n","              loss=loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ieSJdchZggUj"},"source":["### 체크포인트 구성"]},{"cell_type":"markdown","metadata":{"id":"C6XBUUavgF56"},"source":["`tf.keras.callbacks.ModelCheckpoint`를 사용하여 훈련 중 체크포인트(checkpoint)가 저장되도록 설정한다."]},{"cell_type":"code","metadata":{"id":"W6fWTriUZP-n"},"source":["# the save point\n","if use_colab:\n","    checkpoint_dir ='./drive/My Drive/train_ckpt/text_gen/exp1'\n","    if not os.path.isdir(checkpoint_dir):\n","        os.makedirs(checkpoint_dir)\n","else:\n","    checkpoint_dir = 'text_gen/exp1'\n","\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n","                                                 save_weights_only=True,\n","                                                 monitor='loss',\n","                                                 mode='auto',\n","                                                 save_best_only=True,\n","                                                 verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Ky3F_BhgkTW"},"source":["### 훈련 실행"]},{"cell_type":"code","metadata":{"id":"7yGBE2zxMMHs"},"source":["EPOCHS=#TODO 5~10 에폭정도"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UK-hmKjYVoll"},"source":["history = model.fit(#TODO,\n","                    epochs=#TODO,\n","                    callbacks=[cp_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kKkD5M6eoSiN"},"source":["## 텍스트 생성"]},{"cell_type":"markdown","metadata":{"id":"JIPcXllKjkdr"},"source":["### 최근 체크포인트 복원"]},{"cell_type":"markdown","metadata":{"id":"LyeYRiuVjodY"},"source":["이 예측 단계에선 Batch size 1을 사용한다.\n","\n","* RNN 상태가 타임 스텝에서 타임 스텝으로 전달되는 방식이기 때문에 모델은 한 번 빌드된 고정 배치 크기만 허용\n","* 다른 배치 크기로 모델을 실행하려면 모델을 다시 빌드하고 체크포인트에서 가중치를 복원해야 한다."]},{"cell_type":"code","metadata":{"id":"LycQ-ot_jjyu"},"source":["model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n","\n","model.load_weights(checkpoint_dir)\n","\n","model.build(tf.TensorShape([1, None]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"71xa6jnYVrAN"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DjGz1tDkzf-u"},"source":["### 예측 루프\n","\n","텍스트 생성:\n","\n","* 시작 문자열 선택과 순환 신경망 상태를 초기화하고 생성할 문자 수를 설정\n","\n","* 시작 문자열과 순환 신경망 상태를 사용하여 다음 문자의 예측 배열을 가져온다.\n","\n","* 다음, 범주형 배열을 사용하여 예측된 문자의 인덱스를 계산\n","\n","* 이 예측된 문자를 모델의 다음 입력으로 활용\n","\n","* 모델에 의해 리턴된 RNN 상태는 모델로 피드백되어 이제는 하나의 단어가 아닌 더 많은 컨텍스트를 갖추게 된다.\n","\n","* 다음 단어를 예측한 후 수정된 RNN 상태가 다시 모델로 피드백되어 이전에 예측된 단어에서 더 많은 컨텍스트를 얻으면서 학습하는 방식\n","\n","![텍스트를 생성하기 위해 모델의 출력이 입력으로 피드백](https://tensorflow.org/tutorials/text/images/text_generation_sampling.png)\n","\n","* 생성된 텍스트를 보면 모델이 언제 대문자로 나타나고, 절을 만들고 셰익스피어와 유사한 어휘를 가져오는지 볼 수 있다."]},{"cell_type":"code","metadata":{"id":"WvuwZBX5Ogfd"},"source":["def generate_text(model, start_string):\n","  # 평가 단계 (학습된 모델을 사용하여 텍스트 생성)\n","\n","  # 생성할 문자의 수\n","  num_generate = 1000\n","\n","  # 시작 문자열을 숫자로 변환(벡터화)\n","  input_eval = [char2idx[s] for s in start_string]\n","  input_eval = tf.expand_dims(input_eval, 0)\n","\n","  # 결과를 저장할 빈 문자열\n","  text_generated = []\n","\n","  # 온도가 낮으면 더 예측 가능한 텍스트 생성\n","  # 온도가 높으면 더 의외의 텍스트 생성 (불확실성)\n","  # 최적의 세팅을 찾기 위한 실험\n","  temperature = 1.0\n","\n","  # 여기에서 배치 크기 == 1\n","  model.reset_states()\n","  for i in range(num_generate):\n","      predictions = model(input_eval)\n","      # 배치 차원 제거\n","      predictions = tf.squeeze(predictions, 0)\n","\n","      # 범주형 분포를 사용하여 모델에서 리턴한 단어 예측\n","      predictions = predictions / temperature\n","      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","      # 예측된 단어를 다음 입력으로 모델에 전달\n","      # 이전 은닉 상태와 함께\n","      input_eval = tf.expand_dims([predicted_id], 0)\n","\n","      text_generated.append(idx2char[predicted_id])\n","\n","  return (start_string + ''.join(text_generated))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ktovv0RFhrkn"},"source":["print(generate_text(model, start_string=u\"ROMEO: \"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S5NJ0DrPzJsz"},"source":[],"execution_count":null,"outputs":[]}]}
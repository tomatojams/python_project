{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "10_2 seq2seq_kor_keras_with_embedding.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "T1sg7g593SeN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184969689,
     "user_tz": -540,
     "elapsed": 9,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "from IPython.display import display"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rsqy9xm52Xtg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c9ae26cd-2fc5-4d3f-ecde-562513b659ef",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184969690,
     "user_tz": -540,
     "elapsed": 8,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "!curl -O 'https://raw.githubusercontent.com/aifactory-team/AFClass/master/cuk/dl4office_worker/dataset/keras2kor.txt'\n",
    "dataset_filepath = 'keras2kor.txt'"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 42472  100 42472    0     0   220k      0 --:--:-- --:--:-- --:--:--  221k\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kVqYA8weONX2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "570dfefa-1cfc-40ff-ff9a-ad91698c5117",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184969690,
     "user_tz": -540,
     "elapsed": 3,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "# 데이터셋 로딩\n",
    "\n",
    "source_texts = []\n",
    "target_texts = []\n",
    "\n",
    "with open(dataset_filepath, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "for line in lines:\n",
    "    if len(line) > 0 :\n",
    "        target_text, source_text = line.split('\\t')   \n",
    "        source_texts.append(source_text)\n",
    "        target_texts.append(target_text)\n",
    "    \n",
    "for i in range(10):\n",
    "    print('[' + str(i) + ']')\n",
    "    print('Source: ' + source_texts[i])\n",
    "    print('Target: ' + target_texts[i])"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0]\n",
      "Source: 시퀀셜 모델을 생성합니다.\n",
      "Target: model = Sequential()\n",
      "[1]\n",
      "Source: 모델에 입력 벡터가 1개이고 출력 벡터가 1개인 전결합 레이어를 추가합니다.\n",
      "Target: model.add(Dense(1, input_dim=1))\n",
      "[2]\n",
      "Source: 모델을 rmsprop 옵티마이져와 mse 손실함수로 컴파일합니다.\n",
      "Target: model.compile(optimizer='rmsprop', loss='mse')\n",
      "[3]\n",
      "Source: x_train과 y_train을 사용하여 50번의 에포크와 64개 샘플을 한 배치로 모델을 학습합니다.\n",
      "Target: model.fit(x_train, y_train, epochs=50, batch_size=64)\n",
      "[4]\n",
      "Source: x_text와 y_text를 이용하여 32개 샘플을 한 배치로 모델을 평가합니다.\n",
      "Target: model.evaluate(x_test, y_test, batch_size=32)\n",
      "[5]\n",
      "Source: 시퀀셜 모델을 하나 생성합니다.\n",
      "Target: model = Sequential()\n",
      "[6]\n",
      "Source: 모델에 입력 형태가 1차원이고 출력 뉴런이 64개이고 활성화 함수가 relu인 전결합 레이어를 추가합니다.\n",
      "Target: model.add(Dense(64, input_dim=1, activation='relu'))\n",
      "[7]\n",
      "Source: 모델에 출력 벡터가 1개인 전결합 레이어를 추가합니다.\n",
      "Target: model.add(Dense(1))\n",
      "[8]\n",
      "Source: 모델을 rmsprop 옵티마이져와 mse 손실함수로 컴파일합니다.\n",
      "Target: model.compile(optimizer='rmsprop', loss='mse')\n",
      "[9]\n",
      "Source: x_train과 y_train을 사용하여 50번의 에포크와 샘플 64개를 한 배치로 모델을 학습합니다.\n",
      "Target: model.fit(x_train, y_train, epochs=50, batch_size=64)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J9QByiQOqz25",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "80f2c8f3-c0b2-49a2-d738-bd0694e8ce15",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184993207,
     "user_tz": -540,
     "elapsed": 23519,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "# 소스 텍스트 토크나이징\n",
    "\n",
    "!pip install konlpy\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "tokenizer = Okt()\n",
    "\n",
    "texts = []\n",
    "\n",
    "for text in source_texts:\n",
    "    \n",
    "    # 배열인 형태소분석의 출력을 띄어쓰기로 구분하여 붙임\n",
    "    text = \" \".join(tokenizer.morphs(text))\n",
    "    texts.append(text)\n",
    "\n",
    "source_texts = texts"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001B[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n",
      "\u001B[?25hCollecting JPype1>=0.7.0\n",
      "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
      "\u001B[K     |████████████████████████████████| 448 kB 43.1 MB/s \n",
      "\u001B[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
      "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n",
      "Installing collected packages: JPype1, konlpy\n",
      "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lcvEU2c-vQGI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184994769,
     "user_tz": -540,
     "elapsed": 1563,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "# 대상 텍스트 토크나이징\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "texts = []\n",
    "\n",
    "# 모든 문장 반복\n",
    "for text in target_texts:\n",
    "\n",
    "    # 배열인 형태소분석의 출력을 띄어쓰기로 구분하여 붙임\n",
    "    text = \" \".join(tokenizer.tokenize(text))\n",
    "    texts.append(text)\n",
    "\n",
    "target_texts = texts"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OpGpDZwOz132",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d08379d9-8803-4d35-98dc-f6f79fb6230e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184994769,
     "user_tz": -540,
     "elapsed": 10,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "for i in range(10):\n",
    "    print('[' + str(i) + ']')\n",
    "    print('Source: ' + source_texts[i])\n",
    "    print('Target: ' + target_texts[i])"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0]\n",
      "Source: 시퀀셜 모델 을 생 성합니다 .\n",
      "Target: model = Sequential ( )\n",
      "[1]\n",
      "Source: 모델 에 입력 벡터 가 1 개 이고 출력 벡터 가 1 개인 전결 합 레이어 를 추가 합니다 .\n",
      "Target: model.add ( Dense ( 1 , input_dim = 1 ) )\n",
      "[2]\n",
      "Source: 모델 을 rmsprop 옵티 마이 져와 mse 손실함수 로 컴파일 합니다 .\n",
      "Target: model.compile ( optimizer = ' rmsprop ' , loss = ' mse ' )\n",
      "[3]\n",
      "Source: x _ train 과 y _ train 을 사용 하여 50 번의 에 포크 와 64 개 샘플 을 한 배치 로 모델 을 학습 합니다 .\n",
      "Target: model.fit ( x_train , y_train , epochs = 50 , batch_size = 64 )\n",
      "[4]\n",
      "Source: x _ text 와 y _ text 를 이용 하여 32 개 샘플 을 한 배치 로 모델 을 평가 합니다 .\n",
      "Target: model.evaluate ( x_test , y_test , batch_size = 32 )\n",
      "[5]\n",
      "Source: 시퀀셜 모델 을 하나 생 성합니다 .\n",
      "Target: model = Sequential ( )\n",
      "[6]\n",
      "Source: 모델 에 입력 형태 가 1 차원 이고 출력 뉴런 이 64 개 이고 활성화 함수 가 relu 인 전결 합 레이어 를 추가 합니다 .\n",
      "Target: model.add ( Dense ( 64 , input_dim = 1 , activation = ' relu ' ) )\n",
      "[7]\n",
      "Source: 모델 에 출력 벡터 가 1 개인 전결 합 레이어 를 추가 합니다 .\n",
      "Target: model.add ( Dense ( 1 ) )\n",
      "[8]\n",
      "Source: 모델 을 rmsprop 옵티 마이 져와 mse 손실함수 로 컴파일 합니다 .\n",
      "Target: model.compile ( optimizer = ' rmsprop ' , loss = ' mse ' )\n",
      "[9]\n",
      "Source: x _ train 과 y _ train 을 사용 하여 50 번의 에 포크 와 샘플 64 개 를 한 배치 로 모델 을 학습 합니다 .\n",
      "Target: model.fit ( x_train , y_train , epochs = 50 , batch_size = 64 )\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RoIIe1oorRHs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9577632f-c7b8-4049-bb73-e0e943ca6c1d",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184994769,
     "user_tz": -540,
     "elapsed": 7,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "# 소스 토큰 사전 생성\n",
    "\n",
    "source_tokens = []\n",
    "\n",
    "# 단어들의 배열 생성\n",
    "for text in source_texts:\n",
    "    for token in text.split():\n",
    "        source_tokens.append(token)\n",
    "\n",
    "# 중복된 단어 삭제\n",
    "source_tokens = sorted(list(source_tokens))\n",
    "source_tokens = list(set(source_tokens))\n",
    "\n",
    "source_tokens[:0] = [\"<PAD>\", \"<UNKNOWN>\"]\n",
    "\n",
    "print(len(source_tokens))\n",
    "print(source_tokens[:10])\n",
    "\n",
    "source_token_to_index = dict((token, index) for index, token in enumerate(source_tokens))\n",
    "source_index_to_token = dict((index, token) for token, index in source_token_to_index.items())"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "218\n",
      "['<PAD>', '<UNKNOWN>', '0.25', '펑션', '후', 'softmax', 'rmsprop', 'D', '그', '함수']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-AbWsLQ9QqC7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "93d642ac-bbcb-4085-d898-7a082f8e99ed",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184994769,
     "user_tz": -540,
     "elapsed": 4,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "# 대상 토큰 사전 생성\n",
    "\n",
    "target_tokens = []\n",
    "\n",
    "# 단어들의 배열 생성\n",
    "for text in target_texts:\n",
    "    for token in text.split():\n",
    "        target_tokens.append(token)\n",
    "        \n",
    "\n",
    "# 중복된 단어 삭제\n",
    "target_tokens = sorted(list(target_tokens))\n",
    "target_tokens = list(set(target_tokens))\n",
    "\n",
    "target_tokens[:0] = [\"<PAD>\", \"<START>\", \"<END>\", \"<UNKNOWN>\"]\n",
    "\n",
    "print(len(target_tokens))\n",
    "print(target_tokens[:10])\n",
    "\n",
    "target_token_to_index = dict((token, index) for index, token in enumerate(target_tokens))\n",
    "target_index_to_token = dict((index, token) for token, index in target_token_to_index.items())"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "96\n",
      "['<PAD>', '<START>', '<END>', '<UNKNOWN>', '0.25', 'softmax', 'rmsprop', 'max_features', 'validation_data', 'stateful']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oZW0gZyOKrtL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "494861bf-5afe-4a93-bda8-3b4077226d80",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184994769,
     "user_tz": -540,
     "elapsed": 3,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "num_samples = len(source_texts)\n",
    "num_encoder_tokens = len(source_tokens)\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "\n",
    "max_encoder_seq_length = max([len(text) for text in source_texts])\n",
    "max_decoder_seq_length = max([len(text) for text in target_texts])\n",
    "max_decoder_seq_length += 1\n",
    "\n",
    "print('Number of samples:', num_samples)\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of samples: 279\n",
      "Number of unique input tokens: 218\n",
      "Number of unique output tokens: 96\n",
      "Max sequence length for inputs: 106\n",
      "Max sequence length for outputs: 113\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mPJnmjls8mDg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184994770,
     "user_tz": -540,
     "elapsed": 4,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "encoder_input_data = np.zeros((num_samples, max_encoder_seq_length), dtype='int')\n",
    "decoder_input_data = np.zeros((num_samples, max_decoder_seq_length), dtype='int')\n",
    "decoder_target_data = np.zeros((num_samples, max_decoder_seq_length), dtype='int')"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mfUgl2-ODZCM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184994770,
     "user_tz": -540,
     "elapsed": 4,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "# 인코더 입력 데이터\n",
    "\n",
    "for i, text in enumerate(source_texts):\n",
    "    tokens = text.split()\n",
    "    for t in range(max_encoder_seq_length):\n",
    "        if t < len(tokens):\n",
    "            encoder_input_data[i, t] = source_token_to_index[tokens[t]]\n",
    "        else:\n",
    "            encoder_input_data[i, t] = source_token_to_index[\"<PAD>\"]"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ISCMWkiGY3My",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184994770,
     "user_tz": -540,
     "elapsed": 4,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "# 디코더 입력 데이터\n",
    "\n",
    "for i, text in enumerate(target_texts):\n",
    "    text = '<START> ' + text\n",
    "    tokens = text.split()\n",
    "    for t in range(max_decoder_seq_length):\n",
    "        if t < len(tokens):\n",
    "            decoder_input_data[i, t] = target_token_to_index[tokens[t]]\n",
    "        else:\n",
    "            decoder_input_data[i, t] = target_token_to_index[\"<PAD>\"]\n",
    "            \n",
    "# 디코더 대상 데이터\n",
    "\n",
    "for i, text in enumerate(target_texts):\n",
    "    text = text + ' <END>'\n",
    "    tokens = text.split()\n",
    "    for t in range(max_decoder_seq_length):\n",
    "        if t < len(tokens):\n",
    "            decoder_target_data[i, t] = target_token_to_index[tokens[t]]\n",
    "        else:\n",
    "            decoder_target_data[i, t] = target_token_to_index[\"<PAD>\"]            "
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nkuSqbrePc7E",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4b830d05-e55c-40c2-bcad-1d449f745931",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184994770,
     "user_tz": -540,
     "elapsed": 4,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "decoder_input_data[0]"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 1, 49, 58, 67, 72, 41,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dbOYMdh2bD3M",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a52e9061-2c32-410c-c7c8-fe7035f6e7f3",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184994770,
     "user_tz": -540,
     "elapsed": 3,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "decoder_target_data[0]"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([49, 58, 67, 72, 41,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eEtX0b-UNcYa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184995164,
     "user_tz": -540,
     "elapsed": 397,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "decoder_target_data_onehot = np.zeros((num_samples, max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "\n",
    "# 디코더 목표를 원핫인코딩으로 변환\n",
    "# 학습시 입력은 인덱스이지만, 출력은 원핫인코딩 형식임\n",
    "for i in range(num_samples):\n",
    "    for j in range(max_decoder_seq_length):\n",
    "        token_index = decoder_target_data[i, j]\n",
    "        decoder_target_data_onehot[i, j, token_index] = 1.0"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYVGxsalDB5g"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GsnZhV9tO5OG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c9a2822f-aa1d-4152-ab14-8229528199ac",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184995164,
     "user_tz": -540,
     "elapsed": 7,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "decoder_target_data_onehot[0][0]"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GSELnqeRpbng",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651184998699,
     "user_tz": -540,
     "elapsed": 3538,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 20*100  # Number of epochs to train for.\n",
    "latent_dim = 128  # Latent dimensionality of the encoding space.\n",
    "# Path to the data txt file on disk.\n",
    "\n",
    "# 임베딩 벡터 차원\n",
    "encoder_embedding_dim = 100\n",
    "decoder_embedding_dim = 100"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TMSI8AOPRa5K",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b0404bac-ad04-49a8-f608-ed196004765c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651185003875,
     "user_tz": -540,
     "elapsed": 5178,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "encoder_embedding_layer = Embedding(num_encoder_tokens, encoder_embedding_dim)\n",
    "encoder_lstm_layer = LSTM(128, dropout=0.1, recurrent_dropout=0.5, return_state=True)\n",
    "\n",
    "decoder_embedding_layer = Embedding(num_decoder_tokens, decoder_embedding_dim)\n",
    "decoder_lstm_layer = LSTM(128, dropout=0.1, recurrent_dropout=0.5, return_sequences=True, return_state=True)\n",
    "decoder_dense_layer = Dense(num_decoder_tokens, activation='softmax')"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-MirXVsR424j",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651185006753,
     "user_tz": -540,
     "elapsed": 2882,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedded = encoder_embedding_layer(encoder_inputs)\n",
    "_, state_h, state_c = encoder_lstm_layer(encoder_embedded)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "decoder_embedded = decoder_embedding_layer(decoder_inputs)\n",
    "decoder_lstm_outputs, _, _ = decoder_lstm_layer(decoder_embedded, initial_state=encoder_states)\n",
    "decoder_outputs = decoder_dense_layer(decoder_lstm_outputs)"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w-tl3rg8PLwx",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "6f46fb43-c032-4900-9289-070f2963f903",
    "executionInfo": {
     "status": "error",
     "timestamp": 1651186118692,
     "user_tz": -540,
     "elapsed": 1111941,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit([encoder_input_data, decoder_input_data], decoder_target_data_onehot,\n",
    "          batch_size=64,\n",
    "          epochs=800)"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/800\n",
      "5/5 [==============================] - 22s 2s/step - loss: 3.5923 - accuracy: 0.6775\n",
      "Epoch 2/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6792 - accuracy: 0.8786\n",
      "Epoch 3/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.6237 - accuracy: 0.8843\n",
      "Epoch 4/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.5495 - accuracy: 0.8907\n",
      "Epoch 5/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.5073 - accuracy: 0.8922\n",
      "Epoch 6/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.5201 - accuracy: 0.8900\n",
      "Epoch 7/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.4644 - accuracy: 0.8939\n",
      "Epoch 8/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.4536 - accuracy: 0.8945\n",
      "Epoch 9/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.4451 - accuracy: 0.8959\n",
      "Epoch 10/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.4189 - accuracy: 0.8962\n",
      "Epoch 11/800\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.4178 - accuracy: 0.8971\n",
      "Epoch 12/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.3907 - accuracy: 0.8996\n",
      "Epoch 13/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.3900 - accuracy: 0.9012\n",
      "Epoch 14/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.3753 - accuracy: 0.9060\n",
      "Epoch 15/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.3681 - accuracy: 0.9082\n",
      "Epoch 16/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.3574 - accuracy: 0.9095\n",
      "Epoch 17/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.3547 - accuracy: 0.9100\n",
      "Epoch 18/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.3416 - accuracy: 0.9122\n",
      "Epoch 19/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.3430 - accuracy: 0.9107\n",
      "Epoch 20/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.3276 - accuracy: 0.9158\n",
      "Epoch 21/800\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.3296 - accuracy: 0.9136\n",
      "Epoch 22/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.3150 - accuracy: 0.9182\n",
      "Epoch 23/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.3135 - accuracy: 0.9168\n",
      "Epoch 24/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.3035 - accuracy: 0.9214\n",
      "Epoch 25/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.3037 - accuracy: 0.9190\n",
      "Epoch 26/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.3178 - accuracy: 0.9194\n",
      "Epoch 27/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.2871 - accuracy: 0.9246\n",
      "Epoch 28/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.2812 - accuracy: 0.9253\n",
      "Epoch 29/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.3164 - accuracy: 0.9201\n",
      "Epoch 30/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.2674 - accuracy: 0.9285\n",
      "Epoch 31/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.2661 - accuracy: 0.9277\n",
      "Epoch 32/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.2537 - accuracy: 0.9310\n",
      "Epoch 33/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.2518 - accuracy: 0.9307\n",
      "Epoch 34/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.2444 - accuracy: 0.9331\n",
      "Epoch 35/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.2353 - accuracy: 0.9363\n",
      "Epoch 36/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.2264 - accuracy: 0.9401\n",
      "Epoch 37/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.2206 - accuracy: 0.9423\n",
      "Epoch 38/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.2133 - accuracy: 0.9456\n",
      "Epoch 39/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.2042 - accuracy: 0.9481\n",
      "Epoch 40/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.2002 - accuracy: 0.9486\n",
      "Epoch 41/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.1885 - accuracy: 0.9554\n",
      "Epoch 42/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.1816 - accuracy: 0.9566\n",
      "Epoch 43/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.1760 - accuracy: 0.9582\n",
      "Epoch 44/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.1679 - accuracy: 0.9610\n",
      "Epoch 45/800\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.1595 - accuracy: 0.9625\n",
      "Epoch 46/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.1554 - accuracy: 0.9630\n",
      "Epoch 47/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.1478 - accuracy: 0.9657\n",
      "Epoch 48/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.1421 - accuracy: 0.9662\n",
      "Epoch 49/800\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.1373 - accuracy: 0.9670\n",
      "Epoch 50/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.1298 - accuracy: 0.9694\n",
      "Epoch 51/800\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.1264 - accuracy: 0.9697\n",
      "Epoch 52/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.1211 - accuracy: 0.9702\n",
      "Epoch 53/800\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.1165 - accuracy: 0.9719\n",
      "Epoch 54/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.1142 - accuracy: 0.9725\n",
      "Epoch 55/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.1075 - accuracy: 0.9735\n",
      "Epoch 56/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.1055 - accuracy: 0.9741\n",
      "Epoch 57/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.1008 - accuracy: 0.9750\n",
      "Epoch 58/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0974 - accuracy: 0.9755\n",
      "Epoch 59/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0961 - accuracy: 0.9753\n",
      "Epoch 60/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0920 - accuracy: 0.9763\n",
      "Epoch 61/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0896 - accuracy: 0.9757\n",
      "Epoch 62/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0868 - accuracy: 0.9769\n",
      "Epoch 63/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0846 - accuracy: 0.9769\n",
      "Epoch 64/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0824 - accuracy: 0.9773\n",
      "Epoch 65/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0801 - accuracy: 0.9776\n",
      "Epoch 66/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0777 - accuracy: 0.9782\n",
      "Epoch 67/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0789 - accuracy: 0.9773\n",
      "Epoch 68/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0737 - accuracy: 0.9786\n",
      "Epoch 69/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0729 - accuracy: 0.9788\n",
      "Epoch 70/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0726 - accuracy: 0.9788\n",
      "Epoch 71/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0700 - accuracy: 0.9793\n",
      "Epoch 72/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0688 - accuracy: 0.9797\n",
      "Epoch 73/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0668 - accuracy: 0.9796\n",
      "Epoch 74/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0658 - accuracy: 0.9798\n",
      "Epoch 75/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0658 - accuracy: 0.9795\n",
      "Epoch 76/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0640 - accuracy: 0.9805\n",
      "Epoch 77/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0626 - accuracy: 0.9805\n",
      "Epoch 78/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0620 - accuracy: 0.9807\n",
      "Epoch 79/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0614 - accuracy: 0.9804\n",
      "Epoch 80/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0602 - accuracy: 0.9807\n",
      "Epoch 81/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0592 - accuracy: 0.9812\n",
      "Epoch 82/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0593 - accuracy: 0.9813\n",
      "Epoch 83/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0577 - accuracy: 0.9813\n",
      "Epoch 84/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0925 - accuracy: 0.9737\n",
      "Epoch 85/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0562 - accuracy: 0.9814\n",
      "Epoch 86/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0557 - accuracy: 0.9818\n",
      "Epoch 87/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0558 - accuracy: 0.9816\n",
      "Epoch 88/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0546 - accuracy: 0.9820\n",
      "Epoch 89/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0544 - accuracy: 0.9819\n",
      "Epoch 90/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0537 - accuracy: 0.9818\n",
      "Epoch 91/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0528 - accuracy: 0.9823\n",
      "Epoch 92/800\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0521 - accuracy: 0.9828\n",
      "Epoch 93/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0524 - accuracy: 0.9824\n",
      "Epoch 94/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0519 - accuracy: 0.9827\n",
      "Epoch 95/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0511 - accuracy: 0.9828\n",
      "Epoch 96/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0503 - accuracy: 0.9829\n",
      "Epoch 97/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0502 - accuracy: 0.9827\n",
      "Epoch 98/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0498 - accuracy: 0.9830\n",
      "Epoch 99/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0492 - accuracy: 0.9831\n",
      "Epoch 100/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0487 - accuracy: 0.9834\n",
      "Epoch 101/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0485 - accuracy: 0.9837\n",
      "Epoch 102/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0494 - accuracy: 0.9835\n",
      "Epoch 103/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0470 - accuracy: 0.9840\n",
      "Epoch 104/800\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0473 - accuracy: 0.9834\n",
      "Epoch 105/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0464 - accuracy: 0.9843\n",
      "Epoch 106/800\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0467 - accuracy: 0.9839\n",
      "Epoch 107/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0459 - accuracy: 0.9841\n",
      "Epoch 108/800\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0457 - accuracy: 0.9842\n",
      "Epoch 109/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0453 - accuracy: 0.9843\n",
      "Epoch 110/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0450 - accuracy: 0.9845\n",
      "Epoch 111/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0450 - accuracy: 0.9846\n",
      "Epoch 112/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0443 - accuracy: 0.9847\n",
      "Epoch 113/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0439 - accuracy: 0.9847\n",
      "Epoch 114/800\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0432 - accuracy: 0.9852\n",
      "Epoch 115/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0432 - accuracy: 0.9852\n",
      "Epoch 116/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0428 - accuracy: 0.9852\n",
      "Epoch 117/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0425 - accuracy: 0.9854\n",
      "Epoch 118/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0427 - accuracy: 0.9854\n",
      "Epoch 119/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0423 - accuracy: 0.9857\n",
      "Epoch 120/800\n",
      "5/5 [==============================] - 12s 3s/step - loss: 0.0424 - accuracy: 0.9856\n",
      "Epoch 121/800\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0418 - accuracy: 0.9856\n",
      "Epoch 122/800\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0414 - accuracy: 0.9857\n",
      "Epoch 123/800\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0410 - accuracy: 0.9859\n",
      "Epoch 124/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0407 - accuracy: 0.9857\n",
      "Epoch 125/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0406 - accuracy: 0.9855\n",
      "Epoch 126/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0410 - accuracy: 0.9856\n",
      "Epoch 127/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0397 - accuracy: 0.9859\n",
      "Epoch 128/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0398 - accuracy: 0.9854\n",
      "Epoch 129/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0394 - accuracy: 0.9861\n",
      "Epoch 130/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0389 - accuracy: 0.9860\n",
      "Epoch 131/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0387 - accuracy: 0.9859\n",
      "Epoch 132/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0389 - accuracy: 0.9856\n",
      "Epoch 133/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0384 - accuracy: 0.9863\n",
      "Epoch 134/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0376 - accuracy: 0.9861\n",
      "Epoch 135/800\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0380 - accuracy: 0.9856\n",
      "Epoch 136/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0372 - accuracy: 0.9863\n",
      "Epoch 137/800\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0371 - accuracy: 0.9861\n",
      "Epoch 138/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0371 - accuracy: 0.9863\n",
      "Epoch 139/800\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0368 - accuracy: 0.9865\n",
      "Epoch 140/800\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0365 - accuracy: 0.9861\n",
      "Epoch 141/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0361 - accuracy: 0.9866\n",
      "Epoch 142/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0355 - accuracy: 0.9865\n",
      "Epoch 143/800\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0356 - accuracy: 0.9861\n",
      "Epoch 144/800\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0354 - accuracy: 0.9866\n",
      "Epoch 145/800\n",
      "2/5 [===========>..................] - ETA: 4s - loss: 0.0349 - accuracy: 0.9865"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-20-b9f065b28fe9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      8\u001B[0m hist = model.fit([encoder_input_data, decoder_input_data], decoder_target_data_onehot,\n\u001B[1;32m      9\u001B[0m           \u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m64\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m           epochs=800)\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     63\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 64\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     65\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=broad-except\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1382\u001B[0m                 _r=1):\n\u001B[1;32m   1383\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1384\u001B[0;31m               \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1385\u001B[0m               \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1386\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36merror_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    913\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    914\u001B[0m       \u001B[0;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 915\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    916\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    917\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    945\u001B[0m       \u001B[0;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    946\u001B[0m       \u001B[0;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 947\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=not-callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    948\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    949\u001B[0m       \u001B[0;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2955\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[1;32m   2956\u001B[0m     return graph_function._call_flat(\n\u001B[0;32m-> 2957\u001B[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0m\u001B[1;32m   2958\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2959\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1852\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1853\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0;32m-> 1854\u001B[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[1;32m   1855\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[1;32m   1856\u001B[0m         \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    502\u001B[0m               \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    503\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 504\u001B[0;31m               ctx=ctx)\n\u001B[0m\u001B[1;32m    505\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    506\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     53\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0;32m---> 55\u001B[0;31m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[1;32m     56\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "hist = model.fit([encoder_input_data, decoder_input_data], decoder_target_data_onehot,\n",
    "          batch_size=64,\n",
    "          epochs=200)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MAXkp0j0X6Dc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651187837492,
     "user_tz": -540,
     "elapsed": 1522468,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    },
    "outputId": "72949865-87d0-4173-f60b-c0c5be3adb1e"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.0351 - accuracy: 0.9864\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0355 - accuracy: 0.9857\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0351 - accuracy: 0.9868\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0346 - accuracy: 0.9869\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0345 - accuracy: 0.9866\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0346 - accuracy: 0.9867\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0337 - accuracy: 0.9866\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0337 - accuracy: 0.9871\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0342 - accuracy: 0.9868\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0333 - accuracy: 0.9872\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.0333 - accuracy: 0.9871\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0333 - accuracy: 0.9872\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0335 - accuracy: 0.9873\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0336 - accuracy: 0.9870\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0328 - accuracy: 0.9870\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0332 - accuracy: 0.9872\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0329 - accuracy: 0.9872\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.0327 - accuracy: 0.9869\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0329 - accuracy: 0.9873\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0340 - accuracy: 0.9867\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0329 - accuracy: 0.9872\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0325 - accuracy: 0.9870\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0319 - accuracy: 0.9873\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0322 - accuracy: 0.9875\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0318 - accuracy: 0.9876\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0325 - accuracy: 0.9872\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0340 - accuracy: 0.9869\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0315 - accuracy: 0.9874\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0315 - accuracy: 0.9873\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0315 - accuracy: 0.9873\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0318 - accuracy: 0.9870\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0308 - accuracy: 0.9872\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0313 - accuracy: 0.9872\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0320 - accuracy: 0.9879\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0307 - accuracy: 0.9879\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0324 - accuracy: 0.9875\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0310 - accuracy: 0.9877\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0311 - accuracy: 0.9875\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0310 - accuracy: 0.9880\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0326 - accuracy: 0.9875\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0335 - accuracy: 0.9877\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0320 - accuracy: 0.9876\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0310 - accuracy: 0.9881\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0301 - accuracy: 0.9882\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0300 - accuracy: 0.9880\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0296 - accuracy: 0.9883\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0292 - accuracy: 0.9885\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0289 - accuracy: 0.9889\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0286 - accuracy: 0.9886\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0281 - accuracy: 0.9892\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0279 - accuracy: 0.9892\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0293 - accuracy: 0.9884\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0271 - accuracy: 0.9895\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0268 - accuracy: 0.9900\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0282 - accuracy: 0.9887\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0268 - accuracy: 0.9895\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0272 - accuracy: 0.9893\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0261 - accuracy: 0.9899\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0264 - accuracy: 0.9902\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0280 - accuracy: 0.9895\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0264 - accuracy: 0.9897\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0257 - accuracy: 0.9905\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0252 - accuracy: 0.9905\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0250 - accuracy: 0.9908\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0259 - accuracy: 0.9899\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0254 - accuracy: 0.9903\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0253 - accuracy: 0.9902\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0248 - accuracy: 0.9900\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0256 - accuracy: 0.9897\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0240 - accuracy: 0.9911\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0233 - accuracy: 0.9916\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0239 - accuracy: 0.9916\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0253 - accuracy: 0.9909\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0237 - accuracy: 0.9911\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0234 - accuracy: 0.9916\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0224 - accuracy: 0.9918\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0238 - accuracy: 0.9915\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0216 - accuracy: 0.9922\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0215 - accuracy: 0.9919\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0240 - accuracy: 0.9908\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0219 - accuracy: 0.9918\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0230 - accuracy: 0.9912\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0211 - accuracy: 0.9919\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0215 - accuracy: 0.9919\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0213 - accuracy: 0.9917\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0215 - accuracy: 0.9915\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0206 - accuracy: 0.9921\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0227 - accuracy: 0.9913\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0202 - accuracy: 0.9924\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0195 - accuracy: 0.9922\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0199 - accuracy: 0.9922\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0203 - accuracy: 0.9919\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0202 - accuracy: 0.9920\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0199 - accuracy: 0.9922\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0217 - accuracy: 0.9914\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0195 - accuracy: 0.9920\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0189 - accuracy: 0.9923\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0194 - accuracy: 0.9921\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0200 - accuracy: 0.9923\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0194 - accuracy: 0.9925\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0189 - accuracy: 0.9923\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0195 - accuracy: 0.9927\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0202 - accuracy: 0.9922\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0195 - accuracy: 0.9924\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0204 - accuracy: 0.9925\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0198 - accuracy: 0.9926\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0211 - accuracy: 0.9917\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0205 - accuracy: 0.9920\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0231 - accuracy: 0.9907\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0210 - accuracy: 0.9917\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0197 - accuracy: 0.9924\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0218 - accuracy: 0.9918\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0262 - accuracy: 0.9910\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0283 - accuracy: 0.9898\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0248 - accuracy: 0.9905\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0250 - accuracy: 0.9900\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0226 - accuracy: 0.9908\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0244 - accuracy: 0.9905\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0215 - accuracy: 0.9914\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0216 - accuracy: 0.9912\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0204 - accuracy: 0.9914\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0210 - accuracy: 0.9915\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0211 - accuracy: 0.9914\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0209 - accuracy: 0.9917\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0189 - accuracy: 0.9924\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0203 - accuracy: 0.9925\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0229 - accuracy: 0.9912\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0240 - accuracy: 0.9905\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0220 - accuracy: 0.9910\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0212 - accuracy: 0.9919\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0219 - accuracy: 0.9915\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0204 - accuracy: 0.9920\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0226 - accuracy: 0.9911\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0205 - accuracy: 0.9920\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0204 - accuracy: 0.9920\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0205 - accuracy: 0.9918\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0217 - accuracy: 0.9910\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0212 - accuracy: 0.9917\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0219 - accuracy: 0.9912\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0199 - accuracy: 0.9917\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0202 - accuracy: 0.9920\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0202 - accuracy: 0.9922\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0197 - accuracy: 0.9921\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0192 - accuracy: 0.9926\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0224 - accuracy: 0.9908\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0188 - accuracy: 0.9926\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0196 - accuracy: 0.9917\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0204 - accuracy: 0.9916\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0188 - accuracy: 0.9925\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 8s 1s/step - loss: 0.0219 - accuracy: 0.9911\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0199 - accuracy: 0.9921\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0195 - accuracy: 0.9923\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0195 - accuracy: 0.9919\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0196 - accuracy: 0.9921\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0193 - accuracy: 0.9920\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0187 - accuracy: 0.9923\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0231 - accuracy: 0.9907\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0181 - accuracy: 0.9927\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0181 - accuracy: 0.9926\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0193 - accuracy: 0.9920\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0183 - accuracy: 0.9926\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.0205 - accuracy: 0.9918\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0187 - accuracy: 0.9927\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.0177 - accuracy: 0.9926\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0179 - accuracy: 0.9926\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0200 - accuracy: 0.9918\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0196 - accuracy: 0.9920\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0175 - accuracy: 0.9928\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0193 - accuracy: 0.9923\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0178 - accuracy: 0.9928\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0177 - accuracy: 0.9927\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0181 - accuracy: 0.9925\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0176 - accuracy: 0.9927\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0194 - accuracy: 0.9919\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0172 - accuracy: 0.9930\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0171 - accuracy: 0.9930\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0178 - accuracy: 0.9924\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0190 - accuracy: 0.9922\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0172 - accuracy: 0.9927\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0167 - accuracy: 0.9928\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0169 - accuracy: 0.9929\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0216 - accuracy: 0.9912\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0162 - accuracy: 0.9934\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0168 - accuracy: 0.9933\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0182 - accuracy: 0.9927\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0161 - accuracy: 0.9935\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0167 - accuracy: 0.9930\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0172 - accuracy: 0.9925\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0169 - accuracy: 0.9928\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0165 - accuracy: 0.9929\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0161 - accuracy: 0.9932\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0176 - accuracy: 0.9930\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0160 - accuracy: 0.9933\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0187 - accuracy: 0.9924\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0156 - accuracy: 0.9940\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0165 - accuracy: 0.9931\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0170 - accuracy: 0.9929\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0157 - accuracy: 0.9935\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0180 - accuracy: 0.9924\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 7s 1s/step - loss: 0.0159 - accuracy: 0.9933\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y8XvP79sks55",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651187889017,
     "user_tz": -540,
     "elapsed": 371,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UVibLqjOR_lo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651187889414,
     "user_tz": -540,
     "elapsed": 3,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_embedded = decoder_embedding_layer(decoder_inputs)\n",
    "decoder_lstm_outputs, state_h, state_c = decoder_lstm_layer(decoder_embedded, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense_layer(decoder_lstm_outputs)\n",
    "\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r53UgPqLkUrv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651187889761,
     "user_tz": -540,
     "elapsed": 1,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_to_index['<START>']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = target_index_to_token[sampled_token_index]\n",
    "        decoded_sentence += sampled_token + ' '\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_token == '<END>' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s6v8pNZYVt0_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1651187986455,
     "user_tz": -540,
     "elapsed": 96372,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    },
    "outputId": "33d00b11-2e30-4f06-e584-393406b0d830"
   },
   "source": [
    "for i in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[i: i + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('[' + str(i) + ']')\n",
    "    print('Input sentence:', source_texts[i])\n",
    "    print('Target sentence:', target_texts[i])    \n",
    "    print('Decoded sentence:', decoded_sentence)"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0]\n",
      "Input sentence: 시퀀셜 모델 을 생 성합니다 .\n",
      "Target sentence: model = Sequential ( )\n",
      "Decoded sentence: model = Sequential ( ) <END> \n",
      "[1]\n",
      "Input sentence: 모델 에 입력 벡터 가 1 개 이고 출력 벡터 가 1 개인 전결 합 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 1 , input_dim = 1 ) )\n",
      "Decoded sentence: model.add ( Dense ( 1 , activation = ' sigmoid ' ) ) <END> \n",
      "[2]\n",
      "Input sentence: 모델 을 rmsprop 옵티 마이 져와 mse 손실함수 로 컴파일 합니다 .\n",
      "Target sentence: model.compile ( optimizer = ' rmsprop ' , loss = ' mse ' )\n",
      "Decoded sentence: model.compile ( optimizer = ' rmsprop ' , loss = ' mse ' ) <END> \n",
      "[3]\n",
      "Input sentence: x _ train 과 y _ train 을 사용 하여 50 번의 에 포크 와 64 개 샘플 을 한 배치 로 모델 을 학습 합니다 .\n",
      "Target sentence: model.fit ( x_train , y_train , epochs = 50 , batch_size = 64 )\n",
      "Decoded sentence: model.fit ( x_train , y_train , epochs = 1000 , batch_size = 64 ) <END> \n",
      "[4]\n",
      "Input sentence: x _ text 와 y _ text 를 이용 하여 32 개 샘플 을 한 배치 로 모델 을 평가 합니다 .\n",
      "Target sentence: model.evaluate ( x_test , y_test , batch_size = 32 )\n",
      "Decoded sentence: model.evaluate ( x_test , y_test , batch_size = 32 ) <END> \n",
      "[5]\n",
      "Input sentence: 시퀀셜 모델 을 하나 생 성합니다 .\n",
      "Target sentence: model = Sequential ( )\n",
      "Decoded sentence: model = Sequential ( ) <END> \n",
      "[6]\n",
      "Input sentence: 모델 에 입력 형태 가 1 차원 이고 출력 뉴런 이 64 개 이고 활성화 함수 가 relu 인 전결 합 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 64 , input_dim = 1 , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) ) <END> \n",
      "[7]\n",
      "Input sentence: 모델 에 출력 벡터 가 1 개인 전결 합 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 1 ) )\n",
      "Decoded sentence: model.add ( Dense ( 1 ) ) <END> \n",
      "[8]\n",
      "Input sentence: 모델 을 rmsprop 옵티 마이 져와 mse 손실함수 로 컴파일 합니다 .\n",
      "Target sentence: model.compile ( optimizer = ' rmsprop ' , loss = ' mse ' )\n",
      "Decoded sentence: model.compile ( optimizer = ' rmsprop ' , loss = ' mse ' ) <END> \n",
      "[9]\n",
      "Input sentence: x _ train 과 y _ train 을 사용 하여 50 번의 에 포크 와 샘플 64 개 를 한 배치 로 모델 을 학습 합니다 .\n",
      "Target sentence: model.fit ( x_train , y_train , epochs = 50 , batch_size = 64 )\n",
      "Decoded sentence: model.fit ( x_train , y_train , epochs = 1000 , batch_size = 64 ) <END> \n",
      "[10]\n",
      "Input sentence: x _ text 와 y _ text 를 이용 하여 32 개 샘플 을 한 배치 로 모델 을 평가 합니다 .\n",
      "Target sentence: model.evaluate ( x_test , y_test , batch_size = 32 )\n",
      "Decoded sentence: model.evaluate ( x_test , y_test , batch_size = 32 ) <END> \n",
      "[11]\n",
      "Input sentence: 시퀀셜 모델 을 하나 만듭니 다 .\n",
      "Target sentence: model = Sequential ( )\n",
      "Decoded sentence: model = Sequential ( ) <END> \n",
      "[12]\n",
      "Input sentence: 모델 에 입력 데이터 형태 가 1 차원 이고 64 개 의 출력 뉴런 을 가지 고 활성화 함수 가 relu 인 전결 합 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 64 , input_dim = 1 , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) ) <END> \n",
      "[13]\n",
      "Input sentence: 모델 에 64 개 의 출력 뉴런 을 가지 고 활성화 함수 가 relu 인 전결 합 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 64 , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) ) <END> \n",
      "[14]\n",
      "Input sentence: 모델 에 출력 벡터 가 하나 인 전결 합 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 1 ) )\n",
      "Decoded sentence: model.add ( Dense ( 1 ) ) <END> \n",
      "[15]\n",
      "Input sentence: rmsprop 인 옵티 마이 져랑 mse 손실함수 로 모델 을 컴파일 합니다 .\n",
      "Target sentence: model.compile ( optimizer = ' rmsprop ' , loss = ' mse ' )\n",
      "Decoded sentence: model.compile ( optimizer = ' rmsprop ' , loss = ' mse ' ) <END> \n",
      "[16]\n",
      "Input sentence: x _ train 과 y _ train 을 이용 해서 번의 에 포크 와 64 개 의 샘플 을 한 배치 로 모델 을 학습 합니다 .\n",
      "Target sentence: model.fit ( x_train , y_train , epochs = 50 , batch_size = 64 )\n",
      "Decoded sentence: model.fit ( x_train , y_train , epochs = 1000 , batch_size = 64 ) <END> \n",
      "[17]\n",
      "Input sentence: x _ test 와 y _ test 을 이용 해서 32 개 샘플 을 한 배치 로 모델 을 평가 합니다 .\n",
      "Target sentence: model.evaluate ( x_test , y_test , batch_size = 32 )\n",
      "Decoded sentence: model.evaluate ( x_test , y_test , batch_size = 32 ) <END> \n",
      "[18]\n",
      "Input sentence: 시퀀셜 모델 하나 를 만듭니 다 .\n",
      "Target sentence: model = Sequential ( )\n",
      "Decoded sentence: model = Sequential ( ) <END> \n",
      "[19]\n",
      "Input sentence: 모델 에 입력 벡터 가 12 개 이고 출력 뉴런 이 1 개인 전결 합 레이어 를 추가 합니다 . 이 때 활성화 함수 는 시 그 모 이드 입니다 .\n",
      "Target sentence: model.add ( Dense ( 1 , input_dim = 12 , activation = ' sigmoid ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) ) <END> \n",
      "[20]\n",
      "Input sentence: rmsprop 옵티 마이 져와 바이너리 크로스 엔트로피 손실함수 로 모델 을 컴파일 합니다 . 메 트릭 은 정확도 로 설정 합니다 .\n",
      "Target sentence: model.compile ( optimizer = ' rmsprop ' , loss = ' binary_crossentropy ' , metrics =[ ' accuracy ' ] )\n",
      "Decoded sentence: model.add ( Dense ( 1 , activation = ' sigmoid ' ) ) <END> \n",
      "[21]\n",
      "Input sentence: x _ train 과 y _ train 을 이용 하여 천 번의 에 포크 와 64 개 샘플 을 한 배치 로 모델 을 학습 시킵니다 .\n",
      "Target sentence: model.fit ( x_train , y_train , epochs = 1000 , batch_size = 64 )\n",
      "Decoded sentence: model.fit ( x_train , y_train , epochs = 1000 , batch_size = 64 ) <END> \n",
      "[22]\n",
      "Input sentence: x _ test 와 y _ test 를 이용 하여 64 개 샘플 을 한 배치 로 모델 을 평가 합니다 .\n",
      "Target sentence: model.evaluate ( x_test , y_test , batch_size = 32 )\n",
      "Decoded sentence: model.evaluate ( x_test , y_test , batch_size = 32 ) <END> \n",
      "[23]\n",
      "Input sentence: 시퀀셜 모델 을 하나 생 성합니다 .\n",
      "Target sentence: model = Sequential ( )\n",
      "Decoded sentence: model = Sequential ( ) <END> \n",
      "[24]\n",
      "Input sentence: 입력 벡터 가 12 개 이고 출력 뉴런 이 64 개 이고 활성화 함수 가 relu 인 전결 합층 을 모델 에 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) ) <END> \n",
      "[25]\n",
      "Input sentence: 활성화 함수 가 시 그 모 이드 이고 출력 벡터 가 한 개인 전결 합 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 1 , activation = ' sigmoid ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 1 , activation = ' sigmoid ' ) ) <END> \n",
      "[26]\n",
      "Input sentence: rmsprop 옵티 마이 져와 바이너리 크로스 엔트로피 손실함수 를 사용 하여 모델 을 컴파일 합니다 . 메 트릭 은 정확도 로 설정 합니다 .\n",
      "Target sentence: model.compile ( optimizer = ' rmsprop ' , loss = ' binary_crossentropy ' , metrics =[ ' accuracy ' ] )\n",
      "Decoded sentence: model.add ( Dense ( 1 , activation = ' sigmoid ' ) ) <END> \n",
      "[27]\n",
      "Input sentence: 천 번의 에 포크 와 64 개 의 배치 사이즈 설정 으로 트레이닝 셋 을 학습 합니다 .\n",
      "Target sentence: model.fit ( x_train , y_train , epochs = 1000 , batch_size = 64 )\n",
      "Decoded sentence: model.fit ( x_train , y_train , epochs = 1000 , batch_size = 64 ) <END> \n",
      "[28]\n",
      "Input sentence: 테스트 셋 으로 모델 을 평가 합니다 .\n",
      "Target sentence: model.evaluate ( x_test , y_test , batch_size = 32 )\n",
      "Decoded sentence: model.evaluate ( x_test , y_test , batch_size = 32 ) <END> \n",
      "[29]\n",
      "Input sentence: 시퀀셜 모델 을 하나 생 성합니다 .\n",
      "Target sentence: model = Sequential ( )\n",
      "Decoded sentence: model = Sequential ( ) <END> \n",
      "[30]\n",
      "Input sentence: 입력 벡터 가 12 개 이고 출력 뉴런 이 64 개인 덴 스레이 어 를 추가 합니다 . 액티 베이 션 함수 는 렐루 로 지정 합니다 .\n",
      "Target sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) ) <END> \n",
      "[31]\n",
      "Input sentence: 모델 에 액티 베이 션 평션 이 렐루 이고 출력 뉴런 수가 64 개인 덴 스레이 어 를 모델 에 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 64 , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) ) <END> \n",
      "[32]\n",
      "Input sentence: 출력 벡터 가 1 이고 활성화 함수 가 시 그 모 이드 인 전결 합층 을 모델 에 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 1 , activation = ' sigmoid ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 1 , activation = ' sigmoid ' ) ) <END> \n",
      "[33]\n",
      "Input sentence: 최적화 기는 rmsprop 으로 손실함수 는 바이너리 크로스 엔트로피 로 매 트릭 은 정확도 로 설정 하여 모델 을 컴파일 합니다 .\n",
      "Target sentence: model.compile ( optimizer = ' rmsprop ' , loss = ' binary_crossentropy ' , metrics =[ ' accuracy ' ] )\n",
      "Decoded sentence: model.add ( Dense ( 1 , activation = ' sigmoid ' ) ) <END> \n",
      "[34]\n",
      "Input sentence: 훈련 셋 으로 모델 을 학습 합니다 . 에 포크 는 천 번 으로 배치 사이즈 는 64 로 설정 합니다 .\n",
      "Target sentence: model.fit ( x_train , y_train , epochs = 1000 , batch_size = 64 )\n",
      "Decoded sentence: model.fit ( x_train , y_train , epochs = 1000 , batch_size = 64 ) <END> \n",
      "[35]\n",
      "Input sentence: 모델 을 시험 셋 으로 평가 합니다 . 배치 사이즈 는 32 로 설정 합니다 .\n",
      "Target sentence: model.evaluate ( x_test , y_test , batch_size = 32 )\n",
      "Decoded sentence: model.evaluate ( x_test , y_test , batch_size = 32 ) <END> \n",
      "[36]\n",
      "Input sentence: 시퀀셜 모델 을 하나 생 성합니다 .\n",
      "Target sentence: model = Sequential ( )\n",
      "Decoded sentence: model = Sequential ( ) <END> \n",
      "[37]\n",
      "Input sentence: 모델 에 입력 벡터 가 12 개 이고 출력 벡터 가 10 개인 전결 합층 을 추가 합니다 . 활성화 함수 를 소프트맥스 로 설정 합니다 .\n",
      "Target sentence: model.add ( Dense ( 10 , input_dim = 12 , activation = ' softmax ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 256 , activation = ' relu ' ) ) <END> \n",
      "[38]\n",
      "Input sentence: rmsprop 최적화 기와 카테고리 컬 크로스 엔트로피 손실함수 를 사용 하여 모델 을 컴파일 합니다 . 메 트릭 은 정확도 로 설정 합니다 .\n",
      "Target sentence: model.compile ( optimizer = ' rmsprop ' , loss = ' categorical_crossentropy ' , metrics =[ ' accuracy ' ] )\n",
      "Decoded sentence: model.add ( Dense ( 1 ) ) <END> \n",
      "[39]\n",
      "Input sentence: 훈련 셋 으로 모델 을 학습 합니다 . 에 포크 는 천 번 으로 배치 사이즈 는 64 로 설정 합니다 .\n",
      "Target sentence: model.fit ( x_train , y_train , epochs = 1000 , batch_size = 64 )\n",
      "Decoded sentence: model.fit ( x_train , y_train , epochs = 1000 , batch_size = 64 ) <END> \n",
      "[40]\n",
      "Input sentence: 테스트 셋 을 이용 하여 배치 사이즈 를 32 로 모델 을 평가 합니다 .\n",
      "Target sentence: model.evaluate ( x_test , y_test , batch_size = 32 )\n",
      "Decoded sentence: model.evaluate ( x_test , y_test , batch_size = 32 ) <END> \n",
      "[41]\n",
      "Input sentence: 시퀀셜 모델 을 하나 생 성합니다 .\n",
      "Target sentence: model = Sequential ( )\n",
      "Decoded sentence: model = Sequential ( ) <END> \n",
      "[42]\n",
      "Input sentence: 입력 벡터 가 12 개 출력 뉴런 이 64 개 이고 활성화 함수 가 relu 인 덴스 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) ) <END> \n",
      "[43]\n",
      "Input sentence: 출력 벡터 가 10 개 이고 활성화 함수 가 소프트맥스 인 전결 합층 을 모델 에 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 10 , activation = ' softmax ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) ) <END> \n",
      "[44]\n",
      "Input sentence: rmsprop 최적화 기와 카테고리 컬 크로스 엔트로피 손실함수 를 사용 하여 모델 을 컴파일 합니다 . 메 트릭 은 정확도 로 설정 합니다 .\n",
      "Target sentence: model.compile ( optimizer = ' rmsprop ' , loss = ' categorical_crossentropy ' , metrics =[ ' accuracy ' ] )\n",
      "Decoded sentence: model.add ( Dense ( 1 ) ) <END> \n",
      "[45]\n",
      "Input sentence: 훈련 셋 으로 모델 을 학습 합니다 . 에 포크 는 천 번 으로 배치 사이즈 는 64 로 설정 합니다 .\n",
      "Target sentence: model.fit ( x_train , y_train , epochs = 1000 , batch_size = 64 )\n",
      "Decoded sentence: model.fit ( x_train , y_train , epochs = 1000 , batch_size = 64 ) <END> \n",
      "[46]\n",
      "Input sentence: 테스트 셋 을 이용 하여 배치 사이즈 를 32 로 모델 을 평가 합니다 .\n",
      "Target sentence: model.evaluate ( x_test , y_test , batch_size = 32 )\n",
      "Decoded sentence: model.evaluate ( x_test , y_test , batch_size = 32 ) <END> \n",
      "[47]\n",
      "Input sentence: 시퀀셜 모델 을 하나 생 성합니다 .\n",
      "Target sentence: model = Sequential ( )\n",
      "Decoded sentence: model = Sequential ( ) <END> \n",
      "[48]\n",
      "Input sentence: 입력 벡터 가 12 개 출력 뉴런 이 64 개 이고 활성화 함수 가 렐루 인 덴스 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) ) <END> \n",
      "[49]\n",
      "Input sentence: 활성화 함수 가 relu 이고 출력 뉴런 수가 64 개인 덴스 레이어 를 모델 에 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 64 , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) ) <END> \n",
      "[50]\n",
      "Input sentence: 출력 벡터 가 10 개 이고 액티 베이 션 함수 가 소프트맥스 인 덴스 레이어 를 모델 에 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 10 , activation = ' softmax ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) ) <END> \n",
      "[51]\n",
      "Input sentence: rmsprop 최적화 기와 카테고리 컬 크로스 엔트로피 손실함수 를 사용 하여 모델 을 컴파일 합니다 . 메 트릭 은 정확도 로 설정 합니다 .\n",
      "Target sentence: model.compile ( optimizer = ' rmsprop ' , loss = ' categorical_crossentropy ' , metrics =[ ' accuracy ' ] )\n",
      "Decoded sentence: model.add ( Dense ( 1 ) ) <END> \n",
      "[52]\n",
      "Input sentence: 훈련 셋 으로 모델 을 학습 합니다 . 에 포크 는 천 번 으로 배치 사이즈 는 64 로 설정 합니다 .\n",
      "Target sentence: model.fit ( x_train , y_train , epochs = 1000 , batch_size = 64 )\n",
      "Decoded sentence: model.fit ( x_train , y_train , epochs = 1000 , batch_size = 64 ) <END> \n",
      "[53]\n",
      "Input sentence: 테스트 셋 을 이용 하여 배치 사이즈 를 32 로 모델 을 평가 합니다 .\n",
      "Target sentence: model.evaluate ( x_test , y_test , batch_size = 32 )\n",
      "Decoded sentence: model.evaluate ( x_test , y_test , batch_size = 32 ) <END> \n",
      "[54]\n",
      "Input sentence: 시퀀셜 모델 을 하나 생 성합니다 .\n",
      "Target sentence: model = Sequential ( )\n",
      "Decoded sentence: model = Sequential ( ) <END> \n",
      "[55]\n",
      "Input sentence: 모델 에 가로 곱하기 세로 크기 만큼 벡터 를 입력 하고 출력 뉴런 수가 256 개 , 활성화 함수 가 relu 인 전결 합층 을 모델 에 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 256 , activation = ' relu ' , input_dim = width * height ) )\n",
      "Decoded sentence: model.add ( Dense ( 256 , activation = ' relu ' ) ) <END> \n",
      "[56]\n",
      "Input sentence: 활성화 함수 를 렐루 로 출력 뉴런 수 를 256 로 설정 한 덴스 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 256 , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 256 , activation = ' relu ' ) ) <END> \n",
      "[57]\n",
      "Input sentence: 출력 뉴런 수가 256 개 이고 활성화 함수 가 리 니 어인 덴스 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 256 ) )\n",
      "Decoded sentence: model.add ( Dense ( 256 , activation = ' relu ' ) ) <END> \n",
      "[58]\n",
      "Input sentence: 출력 벡터 가 1 개인 전결 합층 을 모델 에 추가 합니다 . 활성화 함수 는 디폴트 인 리 니 어로 설정 됩니다 .\n",
      "Target sentence: model.add ( Dense ( 1 ) )\n",
      "Decoded sentence: model.add ( Dense ( 256 , activation = ' relu ' ) ) <END> \n",
      "[59]\n",
      "Input sentence: 로스 를 mse 로 옵터 마이 져를 adam 으로 설정 한 후 모델 을 컴파일 합니다 .\n",
      "Target sentence: model.compile ( loss = ' mse ' , optimizer = ' adam ' )\n",
      "Decoded sentence: model.compile ( loss = ' mse ' , optimizer = ' adam ' ) <END> \n",
      "[60]\n",
      "Input sentence: 훈련 셋 으로 모델 을 학습 하고 검증 셋 으로 매 에 포크 마다 검증 합니다 . 에 포크 는 천 번 으로 배치 사이즈 는 32 로 설정 합니다 .\n",
      "Target sentence: model.fit ( x_train , y_train , batch_size = 32 , epochs = 1000 , validation_data =( x_val , y_val ) )\n",
      "Decoded sentence: model.fit ( x_train , y_train , epochs = 10 , batch_size = 64 , validation_data =( x_val , y_val ) ) <END> \n",
      "[61]\n",
      "Input sentence: 테스트 셋 을 이용 하여 배치 사이즈 를 32 로 모델 을 평가 합니다 .\n",
      "Target sentence: model.evaluate ( x_test , y_test , batch_size = 32 )\n",
      "Decoded sentence: model.evaluate ( x_test , y_test , batch_size = 32 ) <END> \n",
      "[62]\n",
      "Input sentence: 시퀀셜 모델 을 하나 생 성합니다 .\n",
      "Target sentence: model = Sequential ( )\n",
      "Decoded sentence: model = Sequential ( ) <END> \n",
      "[63]\n",
      "Input sentence: 입력 형태 가 너비 , 높이 , 채널 이 1 개 이고 필터 의 수가 32 개 이고 필터 사이즈 가 3 곱하기 3 이고 활성화 함수 가 relu 인 컨볼루션 2 D 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Conv 2D ( 32 , ( 3 , 3 ) , activation = ' relu ' , input_shape =( width , height , 1 ) ) )\n",
      "Decoded sentence: model.add ( Conv 2D ( 32 , ( 3 , 3 ) , activation = ' relu ' , input_shape =( width , height , 1 ) ) ) <END> \n",
      "[64]\n",
      "Input sentence: 가로 세로 2 배 로 줄이는 맥 스풀링 2 D 레이어 를 모델 에 추가 합니다 .\n",
      "Target sentence: model.add ( MaxPooling 2D ( pool_size =( 2 , 2 ) ) )\n",
      "Decoded sentence: model.add ( Dropout ( 0.3 ) ) <END> \n",
      "[65]\n",
      "Input sentence: 필터 의 수가 32 개 이고 필터 사이즈 가 3 곱하기 3 이고 활성화 함수 가 relu 인 컨볼루션 2 D 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Conv 2D ( 32 , ( 3 , 3 ) , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Conv 2D ( 32 , ( 3 , 3 ) , activation = ' relu ' , input_shape =( width , height , 1 ) ) ) <END> \n",
      "[66]\n",
      "Input sentence: 가로 세로 2 배 로 줄이는 맥 스풀링 2 D 레이어 를 모델 에 추가 합니다 .\n",
      "Target sentence: model.add ( MaxPooling 2D ( pool_size =( 2 , 2 ) ) )\n",
      "Decoded sentence: model.add ( Dropout ( 0.3 ) ) <END> \n",
      "[67]\n",
      "Input sentence: 1 차원 벡터 로 바꿔주는 플 래 튼 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Flatten ( ) )\n",
      "Decoded sentence: model.add ( Dropout ( 0.3 ) ) <END> \n",
      "[68]\n",
      "Input sentence: 활성화 함수 를 렐루 로 출력 뉴런 수 를 256 로 설정 한 덴스 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 256 , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 256 , activation = ' relu ' ) ) <END> \n",
      "[69]\n",
      "Input sentence: 출력 벡터 가 1 개인 덴스 레이어 를 모델 에 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 1 ) )\n",
      "Decoded sentence: model.add ( Dense ( 1 ) ) <END> \n",
      "[70]\n",
      "Input sentence: 로스 를 mse 로 옵터 마이 져를 adam 으로 설정 한 후 모델 을 컴파일 합니다 .\n",
      "Target sentence: model.compile ( loss = ' mse ' , optimizer = ' adam ' )\n",
      "Decoded sentence: model.compile ( loss = ' mse ' , optimizer = ' adam ' ) <END> \n",
      "[71]\n",
      "Input sentence: 훈련 셋 으로 모델 을 학습 하고 검증 셋 으로 매 에 포크 마다 검증 합니다 . 에 포크 는 천 번 으로 배치 사이즈 는 32 로 설정 합니다 .\n",
      "Target sentence: model.fit ( x_train , y_train , batch_size = 32 , epochs = 1000 , validation_data =( x_val , y_val ) )\n",
      "Decoded sentence: model.fit ( x_train , y_train , epochs = 10 , batch_size = 64 , validation_data =( x_val , y_val ) ) <END> \n",
      "[72]\n",
      "Input sentence: 시험 셋 으로 모델 을 평가 합니다 .\n",
      "Target sentence: model.evaluate ( x_test , y_test , batch_size = 32 )\n",
      "Decoded sentence: model.evaluate ( x_test , y_test , batch_size = 32 ) <END> \n",
      "[73]\n",
      "Input sentence: 시퀀셜 모델 을 하나 생 성합니다 .\n",
      "Target sentence: model = Sequential ( )\n",
      "Decoded sentence: model = Sequential ( ) <END> \n",
      "[74]\n",
      "Input sentence: 입력 이 가로 곱하기 세로 크기 만큼 벡터 를 가지 고 출력 뉴런 수가 256 개 이고 활성화 함수 가 렐루 인 전결 합층 을 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 256 , input_dim = width * height , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 256 , activation = ' relu ' ) ) <END> \n",
      "[75]\n",
      "Input sentence: 엑 티 베이 션 펑션 이 relu 이고 출력 뉴런 수가 256 개인 덴 스레이 어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 256 , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 256 , activation = ' relu ' ) ) <END> \n",
      "[76]\n",
      "Input sentence: 엑 티 베이 션 펑션 이 렐루 이고 출력 뉴런 수가 256 개인 덴 스레이 어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 256 , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 256 , activation = ' relu ' ) ) <END> \n",
      "[77]\n",
      "Input sentence: 활성화 함수 가 시 그 모 이드 이고 출력 벡터 가 1 개인 전결 합 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 1 , activation = ' sigmoid ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 1 , activation = ' sigmoid ' ) ) <END> \n",
      "[78]\n",
      "Input sentence: 로스 를 mse 로 옵터 마이 져를 adam 으로 설정 한 후 모델 을 컴파일 합니다 .\n",
      "Target sentence: model.compile ( loss = ' mse ' , optimizer = ' adam ' )\n",
      "Decoded sentence: model.compile ( loss = ' mse ' , optimizer = ' adam ' ) <END> \n",
      "[79]\n",
      "Input sentence: 훈련 셋 으로 모델 을 학습 하고 검증 셋 으로 매 에 포크 마다 검증 합니다 . 에 포크 는 천 번 으로 배치 사이즈 는 32 로 설정 합니다 .\n",
      "Target sentence: model.fit ( x_train , y_train , batch_size = 32 , epochs = 1000 , validation_data =( x_val , y_val ) )\n",
      "Decoded sentence: model.fit ( x_train , y_train , epochs = 10 , batch_size = 64 , validation_data =( x_val , y_val ) ) <END> \n",
      "[80]\n",
      "Input sentence: 테스트 셋 을 이용 하여 배치 사이즈 를 32 로 모델 을 평가 합니다 .\n",
      "Target sentence: model.evaluate ( x_test , y_test , batch_size = 32 )\n",
      "Decoded sentence: model.evaluate ( x_test , y_test , batch_size = 32 ) <END> \n",
      "[81]\n",
      "Input sentence: 시퀀셜 모델 을 하나 생 성합니다 .\n",
      "Target sentence: model = Sequential ( )\n",
      "Decoded sentence: model = Sequential ( ) <END> \n",
      "[82]\n",
      "Input sentence: 입력 이미지 가 너비 , 높이 , 채널 이 1 개 이고 필터 의 수가 32 개 이고 필터 사이즈 가 3 곱하기 3 이고 활성화 함수 가 렐루 인 컨볼루션 2 D 층 을 추가 합니다 .\n",
      "Target sentence: model.add ( Conv 2D ( 32 , ( 3 , 3 ) , activation = ' relu ' , input_shape =( width , height , 1 ) ) )\n",
      "Decoded sentence: model.add ( Conv 2D ( 32 , ( 3 , 3 ) , activation = ' relu ' , input_shape =( width , height , 1 ) ) ) <END> \n",
      "[83]\n",
      "Input sentence: 2 배 로 줄이는 맥 스풀링 2 D 레이어 를 모델 에 추가 합니다 .\n",
      "Target sentence: model.add ( MaxPooling 2D ( pool_size =( 2 , 2 ) ) )\n",
      "Decoded sentence: model.add ( Dropout ( 0.3 ) ) <END> \n",
      "[84]\n",
      "Input sentence: 필터 의 수가 32 개 이고 필터 사이즈 가 3 곱하기 3 이고 활성화 함수 가 렐루 인 컨볼루션 2 D 층 을 추가 합니다 .\n",
      "Target sentence: model.add ( Conv 2D ( 32 , ( 3 , 3 ) , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Conv 2D ( 32 , ( 3 , 3 ) , activation = ' relu ' , input_shape =( width , height , 1 ) ) ) <END> \n",
      "[85]\n",
      "Input sentence: 2 배 로 줄이는 맥 스풀링 2 D 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( MaxPooling 2D ( pool_size =( 2 , 2 ) ) )\n",
      "Decoded sentence: model.add ( Dropout ( 0.3 ) ) <END> \n",
      "[86]\n",
      "Input sentence: 플 래 튼 레이어 를 모델 에 추가 합니다 .\n",
      "Target sentence: model.add ( Flatten ( ) )\n",
      "Decoded sentence: model.add ( Dropout ( 0.3 ) ) <END> \n",
      "[87]\n",
      "Input sentence: 활성화 함수 를 relu 로 출력 뉴런 수 를 256 로 설정 한 덴스 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 256 , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 256 , activation = ' relu ' ) ) <END> \n",
      "[88]\n",
      "Input sentence: 활성화 함수 가 sigmoid 이고 출력 벡터 가 1 개인 전결 합 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dense ( 1 , activation = ' sigmoid ' ) )\n",
      "Decoded sentence: model.add ( Dense ( 1 , activation = ' sigmoid ' ) ) <END> \n",
      "[89]\n",
      "Input sentence: 최적화 기는 sgd 으로 설정 하고 손실함수 는 바이너리 크로스 엔트로피 로 설정 하고 매 트릭 은 정확도 로 설정 하여 모델 을 컴파일 합니다 .\n",
      "Target sentence: model.compile ( loss = ' binary_crossentropy ' , optimizer = ' sgd ' , metrics =[ ' accuracy ' ] )\n",
      "Decoded sentence: model.add ( Dense ( 64 , input_dim = 12 , activation = ' relu ' ) ) <END> \n",
      "[90]\n",
      "Input sentence: 훈련 셋 으로 모델 을 학습 하고 검증 셋 으로 매 에 포크 마다 검증 합니다 . 에 포크 는 32 번 으로 배치 사이즈 는 32 로 설정 합니다 .\n",
      "Target sentence: model.fit ( x_train , y_train , epochs = 30 , batch_size = 32 , validation_data =( x_val , y_val ) )\n",
      "Decoded sentence: model.fit ( x_train , y_train , epochs = 10 , batch_size = 64 , validation_data =( x_val , y_val ) ) <END> \n",
      "[91]\n",
      "Input sentence: 시험 셋 으로 모델 을 평가 합니다 . 배치 사이즈 는 32 로 설정 합니다 .\n",
      "Target sentence: model.evaluate ( x_test , y_test , batch_size = 32 )\n",
      "Decoded sentence: model.evaluate ( x_test , y_test , batch_size = 32 ) <END> \n",
      "[92]\n",
      "Input sentence: 시퀀셜 모델 을 하나 생 성합니다 .\n",
      "Target sentence: model = Sequential ( )\n",
      "Decoded sentence: model = Sequential ( ) <END> \n",
      "[93]\n",
      "Input sentence: 이미지 형태 가 너비 , 높이 , 채널 이 1 개 이고 필터 의 수가 32 개 이고 필터 사이즈 가 3 x 3 이고 액티 베이 션 함수 가 렐루 인 컨볼루션 2 D 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Conv 2D ( 32 , ( 3 , 3 ) , activation = ' relu ' , input_shape =( width , height , 1 ) ) )\n",
      "Decoded sentence: model.add ( Conv 2D ( 32 , ( 3 , 3 ) , activation = ' relu ' , input_shape =( width , height , 1 ) ) ) <END> \n",
      "[94]\n",
      "Input sentence: 필터 의 수가 32 개 이고 필터 사이즈 가 3 x 3 이고 액티 베이 션 함수 가 렐루 인 컨볼루션 2 D 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Conv 2D ( 32 , ( 3 , 3 ) , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Conv 2D ( 32 , ( 3 , 3 ) , activation = ' relu ' , input_shape =( width , height , 1 ) ) ) <END> \n",
      "[95]\n",
      "Input sentence: 가로 세로 2 배 로 줄이는 맥 스풀링 2 D 레이어 를 모델 에 추가 합니다 .\n",
      "Target sentence: model.add ( MaxPooling 2D ( pool_size =( 2 , 2 ) ) )\n",
      "Decoded sentence: model.add ( Dropout ( 0.3 ) ) <END> \n",
      "[96]\n",
      "Input sentence: 드랍 비율 이 0.25 인 드랍 아웃 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Dropout ( 0.25 ) )\n",
      "Decoded sentence: model.add ( Dropout ( 0.3 ) ) <END> \n",
      "[97]\n",
      "Input sentence: 필터 의 수가 64 개 이고 필터 사이즈 가 3 곱하기 3 이고 활성화 함수 가 relu 인 컨볼루션 2 D 레이어 를 추가 합니다 .\n",
      "Target sentence: model.add ( Conv 2D ( 64 , ( 3 , 3 ) , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Conv 2D ( 32 , ( 3 , 3 ) , activation = ' relu ' , input_shape =( width , height , 1 ) ) ) <END> \n",
      "[98]\n",
      "Input sentence: 필터 의 수가 64 개 이고 필터 사이즈 가 3 곱하기 3 이고 활성화 함수 가 렐루 인 컨볼루션 2 D 층 을 추가 합니다 .\n",
      "Target sentence: model.add ( Conv 2D ( 64 , ( 3 , 3 ) , activation = ' relu ' ) )\n",
      "Decoded sentence: model.add ( Conv 2D ( 32 , ( 3 , 3 ) , activation = ' relu ' , input_shape =( width , height , 1 ) ) ) <END> \n",
      "[99]\n",
      "Input sentence: 가로 세로 2 배 로 줄이는 맥 스풀링 2 D 레이어 를 모델 에 추가 합니다 .\n",
      "Target sentence: model.add ( MaxPooling 2D ( pool_size =( 2 , 2 ) ) )\n",
      "Decoded sentence: model.add ( Dropout ( 0.3 ) ) <END> \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wgmiKqK1Vb75",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1651191130491,
     "user_tz": -540,
     "elapsed": 1189,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    },
    "outputId": "7dba311c-0c67-447a-9572-80e74e577b5d"
   },
   "source": [
    "test_source_texts = \"\"\"\n",
    "시퀀셜 모델을 하나 생성합니다.\n",
    "입력 벡터가 12개 이고 출력 벡터가 64개인 전결합층을 추가합니다 .\n",
    "활성화함수가 시그모이드이고 출력 벡터가 1개인 전결합층를 추가합니다.\n",
    "1000번 에포크와 64개 배치사이즈로 모델을 학습합니다.\n",
    "테스트셋으로 모델을 평가합니다.\n",
    "\"\"\"\n",
    "\n",
    "test_source_texts = test_source_texts.split('\\n')\n",
    "test_source_texts = list(filter(None, test_source_texts))\n",
    "\n",
    "tagger = Okt()\n",
    "\n",
    "for text in test_source_texts:\n",
    "    text = \" \".join(tagger.morphs(text))\n",
    "    tokens = text.split()\n",
    "\n",
    "    test_encoder_input_data = np.zeros((1, max_encoder_seq_length), dtype='float32')    \n",
    "\n",
    "    for t in range(max_encoder_seq_length):\n",
    "        if t < len(tokens):\n",
    "            token = tokens[t]\n",
    "            if source_token_to_index.get(token) is not None:\n",
    "                test_encoder_input_data[0, t] = source_token_to_index[token]\n",
    "            else:\n",
    "                test_encoder_input_data[0, t] = source_token_to_index[\"<UNKNOWN>\"]\n",
    "        else:\n",
    "            test_encoder_input_data[0, t] = source_token_to_index[\"<PAD>\"]\n",
    "\n",
    "    decoded_text = decode_sequence(test_encoder_input_data)\n",
    "    decoded_text = decoded_text.replace(\" \", \"\")\n",
    "    decoded_text = decoded_text.replace(\"<END>\", \"\")\n",
    "    print(decoded_text)"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-44fb33d4ef90>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0mtest_source_texts\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_source_texts\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m \u001B[0mtagger\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOkt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mtext\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtest_source_texts\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'Okt' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hINgoKjqMzie",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1651191130490,
     "user_tz": -540,
     "elapsed": 3,
     "user": {
      "displayName": "김태영",
      "userId": "15933429515795148017"
     }
    }
   },
   "source": [
    "test_source_texts = \"\"\"시퀀셜 모델을 하나 생성합니다.\n",
    "입력 형태가 너비, 높이, 채널이 1개이고 필터의 수가 32개이고 필터 사이즈가 3 곱하기 3이고 활성화 함수가 relu인 컨볼루션 2D 레이어를 추가합니다.\n",
    "가로 세로 2배로 줄이는 맥스풀링 2D 레이어를 모델에 추가합니다.\n",
    "필터의 수가 32개이고 필터 사이즈가 3 곱하기 3이고 활성화 함수가 relu인 컨볼루션 2D 레이어를 추가합니다.\n",
    "가로 세로 2배로 줄이는 맥스풀링 2D 레이어를 모델에 추가합니다.\n",
    "1차원 벡터로 바꿔주는 플래튼 레이어를 추가합니다.\n",
    "활성화 함수를 렐루로 출력 뉴런 수를 256로 설정한 덴스 레이어를 추가합니다.\n",
    "출력 벡터가 1개인 덴스 레이어를 모델에 추가합니다.\n",
    "로스를 mse로 옵터마이져를 adam으로 설정한 후 모델을 컴파일합니다.\n",
    "훈련셋으로 모델을 학습하고 검증셋으로 매 에포크마다 검증합니다. 에포크는 10번으로 배치사이즈는 32로 설정합니다.\n",
    "시험셋으로 모델을 평가합니다.\"\"\"\n",
    "\n",
    "\n",
    "test_source_texts = test_source_texts.split('\\n')\n",
    "test_source_texts = list(filter(None, test_source_texts))\n",
    "\n",
    "# 디코더 입력 데이터\n",
    "\n",
    "tagger = Okt()\n",
    "\n",
    "for text in test_source_texts:\n",
    "    text = \" \".join(tagger.morphs(text))\n",
    "    tokens = text.split()\n",
    "\n",
    "    test_encoder_input_data = np.zeros((1, max_encoder_seq_length), dtype='float32')    \n",
    "\n",
    "    for t in range(max_encoder_seq_length):\n",
    "        if t < len(tokens):\n",
    "            token = tokens[t]\n",
    "            if source_token_to_index.get(token) is not None:\n",
    "                test_encoder_input_data[0, t] = source_token_to_index[token]\n",
    "            else:\n",
    "                test_encoder_input_data[0, t] = source_token_to_index[\"<UNKNOWN>\"]\n",
    "        else:\n",
    "            test_encoder_input_data[0, t] = source_token_to_index[\"<PAD>\"]\n",
    "\n",
    "    decoded_text = decode_sequence(test_encoder_input_data)\n",
    "    decoded_text = decoded_text.replace(\" \", \"\")\n",
    "    decoded_text = decoded_text.replace(\"<END>\", \"\")\n",
    "    print(decoded_text)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "Ff6-mFl_qPCL"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}

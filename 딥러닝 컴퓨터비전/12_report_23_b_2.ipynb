{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "colab": {
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "699d979d"
   },
   "source": [
    "**`COPYRIGHT(C) 2023 THE CYBER UNIVERSITY OF KOREA ALL RIGHTS RESERVED.`**\n",
    "\n",
    "본 파일의 외부 배포를 금지합니다."
   ],
   "id": "699d979d"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3c7410fc"
   },
   "source": [
    "# 1번과제 - position regression 2.0 (50점)  "
   ],
   "id": "3c7410fc"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c37fb08"
   },
   "source": [
    "**템플릿 A**"
   ],
   "id": "0c37fb08"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "08ff61ec",
    "ExecuteTime": {
     "end_time": "2023-11-26T16:29:47.961691200Z",
     "start_time": "2023-11-26T16:29:02.720796500Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 재현 가능한 난수 생성\n",
    "np.random.seed(0)\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "\n",
    "#tf.random.set_seed(0)\n",
    "\n",
    "def load_data():\n",
    "    n = 2000\n",
    "    split = 0.8\n",
    "    n_train = (int)(split * n)\n",
    "\n",
    "    y = np.random.randint(28, size=(n, 2))\n",
    "    x = np.empty((n, 28, 28))\n",
    "\n",
    "    for i in tqdm(range(n)):\n",
    "        img = np.zeros((28, 28))\n",
    "        cv2.circle(img, (y[i][0], y[i][1]), 3, 255, -1)\n",
    "        x[i] = img\n",
    "    return ((x[:n_train], y[:n_train]), (x[n_train:], y[n_train:]))\n",
    "\n",
    "\n",
    "def label(y):\n",
    "    return np.around(y).astype('int')\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "# normalize image\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_test[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(str(label(y_test[i])))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def pos_accuracy(y_true, y_pred):\n",
    "    label_true = tf.round(y_true)\n",
    "    label_pred = tf.round(y_pred)\n",
    "    is_correct = tf.reduce_all(label_true == label_pred, axis=1)\n",
    "    is_correct = tf.cast(is_correct, 'float32')\n",
    "    score = tf.reduce_mean(is_correct)\n",
    "    return score\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(256, activation='relu'),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(2, activation=None)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='mse', metrics=[pos_accuracy])\n",
    "\n",
    "# 이 자리에 LrReducer snippet을 추가합니다.\n",
    "# def LrReducer():\n",
    "#    ...\n",
    "# lrreducer = LrReducer()\n",
    "\n",
    "# 이 자리에 ModelCheckpoint snippet을 추가합니다.\n",
    "\n",
    "n_batch = 32\n",
    "n_epochs = 400\n",
    "\n",
    "# 1. model.fit의 반환 결과를 history 변수에 저장합니다.\n",
    "# 2. lrreducer callback을 추가합니다.\n",
    "# 3. checkpoint callback을 추가합니다.\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    batch_size=n_batch,\n",
    "    epochs=n_epochs,\n",
    ")\n",
    "\n",
    "# 여기에 저장된 weights를 loading하는 코드를 추가합니다.\n",
    "\n",
    "# 여기에 Graph snippet 코드를 추가합니다.\n",
    "\n",
    "# LrReducer 중 patience의 역할은 무엇입니까?\n",
    "ans01 = \"\"\"\n",
    "여기에 적어주세요.\n",
    "\"\"\"\n",
    "\n",
    "# LrReducer 중 reduce_rate의 역할은 무엇입니까?\n",
    "ans02 = \"\"\"\n",
    "여기에 적어주세요.\n",
    "\"\"\"\n",
    "\n",
    "# LrReducer 중 reduce_nb의 역할은 무엇입니까?\n",
    "ans03 = \"\"\"\n",
    "여기에 적어주세요.\n",
    "\"\"\"\n",
    "\n",
    "#  4주차 과제와 차이가 발생하였는지에 대해, 여기에 결과와 이유를 적어봅시다.\n",
    "#  주의: 아래의 코드 중 \"\"\"으로 둘러져있는 것은 python에서 여러줄의 텍스트를 string으로 지정하는 문법입니다. 이 문법을 유지하면서 내용을 적어주세요\n",
    "ans04 = \"\"\"\n",
    "결과: 4주차 과제에서는 val_pos_accuracy가 0.000 이었다.\n",
    "이유: ???????? 라고 생각한다.\n",
    "\"\"\"\n",
    "\n",
    "# 모델을 ans02 변수에 기록합니다.\n",
    "ans05 = model\n",
    "\n",
    "# 모델의 성능 측정\n",
    "val_loss, val_pos_accuracy = model.evaluate(x_test, y_test)\n",
    "print('val_loss =', val_loss, 'val_pos_accuracy =', val_pos_accuracy)\n",
    "\n",
    "# val_loss의 저장\n",
    "ans06 = val_loss\n",
    "ans07 = val_pos_accuracy\n",
    "\n",
    "# Fully connnected Net의 경우와 ConvNet의 검증셋 성능 차이에 대해 그 이유를 추정해 봅시다.\n",
    "ans08 = \"\"\"\n",
    "여기에 적어주세요.\n",
    "\"\"\"\n",
    "\n",
    "# ans09는 내용 중복으로 삭제하였습니다. 답변 작성이 필요없습니다.\n",
    "ans09 = \"\""
   ],
   "id": "08ff61ec",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b88bdc26"
   },
   "source": [
    "이번 과제는 4주차 과제중 하나인 position regression을 개선하는 것입니다.  \n",
    "기본 소스에 대한 설명은 4주차 과제에 있으니, 필요하신 분들은 다시 다운로드 받아서 읽어봐 주십시오.  \n",
    "\n",
    "이 과제는 컨볼루션 신경망의 좋은 성능을 확인해 볼 수 있는 예제라고 하겠습니다.  \n",
    "Epoch에 따른 손실함수와 정확도의 추이를 볼 수 있는 그래픽 출력을 추가하였습니다.  \n",
    "또한, LrReducer 콜백 함수를 통하여 learning rate를 자동적으로 조절하는 기능을 추가하였습니다.  \n",
    "따라서 이번 과제에서는 learning rate의 조절은 필요없습니다.  \n",
    "가장 높은 성능에서 모델의 weights를 저장하는 콜백 기능도 추가 되었습니다.  \n",
    "이 기능까지를 추가하고 나면 FC(fully connected) 모델의 성능을 확인하고 이전 과제와 비교해 보시면 됩니다.  \n",
    "마지막으로는 코드를 컨볼루션 신경망으로 전환하고 튜닝하여 그 차이를 확인해 보도록 하겠습니다.  \n",
    "\n",
    "\n",
    "1. history의 저장\n",
    "\n",
    "   loss와 pos_accuracy를 그래프로 그리기 위해서 model.fit의 결과를 history변수로 먼저 저장합니다.  \n",
    "   ```python\n",
    "   history = model.fit( ... )\n",
    "   ```\n",
    "\n",
    "2. Learning Rate Reducer callback의 추가\n",
    "\n",
    "   템플릿에서 learning rate는 0.001로 시작하였습니다.  \n",
    "   학습이 진행되면 더이상 검증셋의 정확도(여기서는 val_pos_accuracy)가 증가하지 않을 수 있습니다.  \n",
    "   이때에는 fine tuning을 위하여 learning rate를 점차적으로 줄여줄 필요가 있습니다.  \n",
    "   다음의 callback 함수 snippet을 이용합니다.  \n",
    "   **LrReducer snippet:**  \n",
    "   ```python\n",
    "   class LrReducer(Callback):\n",
    "       def __init__(self, patience=5, reduce_rate=0.5, reduce_nb=5, verbose=1):\n",
    "           super(Callback, self).__init__()\n",
    "           self.patience = patience\n",
    "           self.wait = 0\n",
    "           self.best_score = -1.\n",
    "           self.reduce_rate = reduce_rate\n",
    "           self.current_reduce_nb = 0\n",
    "           self.reduce_nb = reduce_nb\n",
    "           self.verbose = verbose\n",
    "\n",
    "       def on_epoch_end(self, epoch, logs={}):\n",
    "           current_score = logs.get('val_pos_accuracy')\n",
    "           if current_score > self.best_score:\n",
    "               self.best_score = current_score\n",
    "               self.wait = 0\n",
    "               if self.verbose > 0:\n",
    "                   print('---current best score: %.3f' % current_score)\n",
    "           else:\n",
    "               if self.wait >= self.patience:\n",
    "                   self.current_reduce_nb += 1\n",
    "                   if self.current_reduce_nb <= self.reduce_nb:\n",
    "                       lr = keras.backend.get_value(self.model.optimizer.lr)\n",
    "                       keras.backend.set_value(self.model.optimizer.lr, lr*self.reduce_rate)\n",
    "                       self.wait = 0\n",
    "                       if self.verbose > 0:\n",
    "                           print('---lr decreasing: %e' % (lr*self.reduce_rate))\n",
    "                   else:\n",
    "                       if self.verbose > 0:\n",
    "                           print(\"Epoch %d: early stopping\" % (epoch))\n",
    "                       self.model.stop_training = True\n",
    "               self.wait += 1\n",
    "   lrreducer = LrReducer(patience=20)\n",
    "   ```\n",
    "   callback 함수를 이용하기 위해서는 다음과 같이 model.fit을 수정합니다.\n",
    "   ```python\n",
    "   history = model.fit(\n",
    "       x_train, y_train,\n",
    "       validation_data=(x_test, y_test),\n",
    "       batch_size=n_batch,\n",
    "       epochs=n_epochs,\n",
    "       callbacks = [lrreducer]\n",
    "   )\n",
    "   ```\n",
    "   실행을 해보고 정상적으로 동작하는 지 확인합시다.\n",
    "   LrReducer callback함수는 학습의 매 epoch이 끝날 때마다 호출됩니다.  \n",
    "   LrReducer의 소스를 읽고 그 원리를 파악합니다.  \n",
    "   `patience` 설정 값은 어떤 역할을 합니까? `ans01`에 적어주세요.  \n",
    "   `reduce_rate`(감소율)의 설정 값은 어떤 역할을 합니까? `ans02`에 적어주세요.  \n",
    "   `reduce_nb`(감소 수)의 설정 값은 어떤 역할을 합니까? `ans03`에 적어주세요.  \n",
    "\n",
    "3. checkpoint callback의 추가  \n",
    "   이번에는 `val_pos_accuracy`가 최고점을 갱신하면 자동으로 모델의 weights를 저장하도록 callback을 추가합니다.  \n",
    "   **ModelCheckpoint snippet:**  \n",
    "   ```python\n",
    "   checkpoint = keras.callbacks.ModelCheckpoint(\"best_model.h5\", monitor='val_pos_accuracy', verbose=1,\n",
    "       save_best_only=True, mode='auto', save_freq=\"epoch\")\n",
    "   ```\n",
    "   callback 사용을 위하여 최종적인 `model.fit`을 다음과 같이 한번 더 수정합니다.\n",
    "   ```python\n",
    "   history = model.fit(\n",
    "       x_train, y_train,\n",
    "       validation_data=(x_test, y_test),\n",
    "       batch_size=n_batch,\n",
    "       epochs=n_epochs,\n",
    "       callbacks = [lrreducer, checkpoint] # 두 가지의 callback 모두 사용\n",
    "   )\n",
    "   ```\n",
    "\n",
    "4. 학습이 끝난 후에 저장한 weights를 다시 loading하여 불러오는 코드를 `model.fit` 호출 다음에 추가합니다.  \n",
    "   최고 `val_pos_accuracy`가 저장되어 있을 것입니다.  \n",
    "   ```python\n",
    "   model.load_weights(\"best_model.h5\")\n",
    "   ```\n",
    "\n",
    "5. 이제 저장된 history를 이용하여 학습이 완료된 후 그래프를 출력합니다.  \n",
    "   다음의 코드 snippet을 이용합니다.  \n",
    "   **Graph snippet**  \n",
    "   ```python\n",
    "   # 학습 정확성 값과 검증 정확성 값을 그래프로 그립니다.\n",
    "   plt.plot(history.history['pos_accuracy'])\n",
    "   plt.plot(history.history['val_pos_accuracy'])\n",
    "   plt.title('Model pos_accuracy')\n",
    "   plt.ylabel('Pos_accuracy')\n",
    "   plt.xlabel('Epoch')\n",
    "   plt.legend(['Train', 'Test'], loc='upper left')\n",
    "   plt.show()\n",
    "   # Loss의 그래프\n",
    "   plt.plot(history.history['loss'])\n",
    "   plt.plot(history.history['val_loss'])\n",
    "   plt.title('Model loss')\n",
    "   plt.ylabel('loss')\n",
    "   plt.xlabel('epoch')\n",
    "   plt.ylim(0.0,1.0)\n",
    "   plt.legend(['loss', 'val_loss'], loc='upper left')\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "6. 이제 검증셋의 `val_loss`와 `val_pos_accuracy`를 확인해 봅시다.  \n",
    "   아래의 코드는 이미 템플릿에 포함되어 있으므로 별도의 수정이 필요없습니다.  \n",
    "   ```python\n",
    "   val_loss, val_pos_accuracy = model.evaluate(x_test, y_test)\n",
    "   print('val_loss =', val_loss, 'val_pos_accuracy =', val_pos_accuracy)\n",
    "   ```\n",
    "\n",
    "7. 결과의 분석  \n",
    "   이 모델의 목표였던 `val_pos_accuracy`는 약 0.88입니다. (물론 Dense모델 만으로도 더 높은 성능도 가능합니다.)    \n",
    "   이제 이 결과를 4주차 과제였던 여러분의 모델과 비교해 봅시다.  \n",
    "   (4주차 과제에서 `val_pos_accuracy` 대신에 `val_accuracy`를 측정했던 분들은 코드를 수정하여 다시 확인해 봅시다.)  \n",
    "   정확도(val_pos_accuracy)를 비교해 보니까 어떻습니까?  \n",
    "   여러분이 4주차에 만들었던 모델과 성능차이가 있다면, 어떤 부분에서 차이가 났는지에 대해 간단히 추정해 봅시다.  \n",
    "   추정한 내용을 `ans04`에 적어봅시다.  \n",
    "\n",
    "8. CNN으로의 전환  \n",
    "   이제 모델 정의 부분을 ConvNet으로 변경하여 봅시다.  \n",
    "   학습 속도가 느리다면, 런타임 -> 런타임 유형변경 -> GPU로 변경하여 봅시다.  \n",
    "   ```python\n",
    "   model = keras.Sequential([\n",
    "       # 이 부분은 수정하지 않도록 합시다.\n",
    "       keras.layers.Reshape(input_shape=(28, 28), target_shape=(28, 28, 1)),\n",
    "\n",
    "       # 다음은 첫번째 레이어의 예제입니다. 여러분이 수정해야 할 것은 이 레이어부터입니다.\n",
    "       keras.layers.Conv2D(16, (3,3), padding='valid', activation='relu'),\n",
    "       # ... 중간에 여러개의 다른 레이어를 추가해 봅시다.\n",
    "       # ...\n",
    "\n",
    "       # 이 부분은 수정하지 않도록 합시다.\n",
    "       keras.layers.Flatten(),\n",
    "       keras.layers.Dense(2, activation=None)\n",
    "   ])\n",
    "   ```\n",
    "   맨 처음 `Reshape`부분은 Conv2D를 위하여 채널을 추가하기 위한 것입니다. (수정하지 않도록 합시다.)  \n",
    "   맨 마지막 `Flatten`과 `Dense` 레이어는 x,y좌표의 결과 출력을 위한 것입니다. (이 코드는 수정하지 않도록 합시다.)  \n",
    "   `Reshape`과 `Flatten`사이에 복수개의 Conv2D 레이어를 추가합니다.  \n",
    "   이 부분에서는 다음의 하이퍼 파라미터를 조절할 수 있습니다.  \n",
    "   먼저 Conv2D의 레이어 갯수입니다.  \n",
    "   다음은 각 Conv2D 레이어의 필터 수 입니다.  \n",
    "   다음은 `padding`입니다. `same` or `valid` 중에 적절한 것을 선택합니다. (레이어마다 달라도 관계없습니다.)  \n",
    "   `Conv2D`의 활성함수는 `relu`나 `sigmoid` 등을 사용하도록 합시다.  \n",
    "   (`mish` 등 다른 활성함수를 사용하셔도 무방합니다.)  \n",
    "   kernel size는 모두 3x3으로 고정합니다.\n",
    "   maxpool2d 레이어도 사용하실 수 있습니다.  \n",
    "    \n",
    "   목표는 성능 최적화, 즉, `val_pos_accuracy`를 최대화 하는 것입니다.  \n",
    "   성능이 만족되면, 성능을 유지하도록 하면서 총 parameter의 수를 줄이도록 노력해 봅시다.  \n",
    "   model.summary()를 이용하여 parameter의 수를 확인해 봅시다.  \n",
    "   각 레이어의 parameter의 수를 손으로 직접 계산하여 model.summary()의 출력과 일치하는 지 비교해 봅시다.  \n",
    "   optimizer는 SGD를 사용하여도되고 Adam으로 변경하셔도 됩니다. (Adam의 경우 더 높은 성능이 나오는 지 확인하여 봅시다.)  \n",
    "    \n",
    "   완성된 모델을 `ans05` 변수에 기록합니다.  (이미 템플릿 코드에 포함되어 있으므로, 별도의 수정이 필요없습니다.)  \n",
    "   `val_loss`와 `val_pos_accuracy`를 각각 `ans06`과 `ans07`에 기록합니다.  (이미 템플릿 코드에 포함되어 있습니다.)  \n",
    "    \n",
    "   처음에 수행한 FC 네트워크와 ConvNet의 검증셋 성능을 비교해서 성능의 증가 여부와, 그 이유를 추정해 봅시다.\n",
    "   중요한 것은 학습셋(training set)의 성능은 98%-99% 정도로 두 네트워크 다 높다는 것입니다.  \n",
    "   하지만 테스트셋(혹은 검증셋)에서는 차이가 발생할 것입니다.  \n",
    "   왜 그럴까요?  \n",
    "   차이의 이유를 추정해서 `ans08`에 기입합니다.\n",
    "\n",
    "   `ans09`는 질문 중복으로 삭제하였습니다. 무시하시면 됩니다.\n",
    "\n",
    "9. 과제의 목적  \n",
    "   이 과제는 여러분이 ConvNet을 구성하고 그 Hyper parameter를 튜닝해 봄으로서 성능을 극대화하고, 모델을 최적화하는 것이 어떤 것인지를 맛볼 수 있는 toy 예제입니다.  \n",
    "   여기에 정답이란 것이 있을까요?  \n",
    "   여러분은 끊임없이 내가 만든 모델의 `val_pos_accuracy`가 정말 더이상 높일 수 없을 정도인지를 의심하게 될 것입니다.  \n",
    "   이 의구심을 갖고 이것을 개선하려고 노력하는 것이 이 과제의 주된 목적입니다.  \n",
    "   더이상 성능개선이 이루어 지지 않으면, 동일한 성능에서 학습 parameter의 수를 줄여보려 노력하는 것이 두번째 목적입니다.  "
   ],
   "id": "b88bdc26"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "001a50f1"
   },
   "source": [
    "**과제 기입란**  \n",
    "최종 코드는 모든 수정사항이 반영된 ConvNet이어야 합니다."
   ],
   "id": "001a50f1"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "88a8ce29",
    "ExecuteTime": {
     "end_time": "2023-11-27T02:16:26.742922Z",
     "start_time": "2023-11-27T02:14:28.772861Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 재현 가능한 난수 생성\n",
    "np.random.seed(0)\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "\n",
    "#tf.random.set_seed(0)\n",
    "\n",
    "def load_data():\n",
    "    n = 2000\n",
    "    split = 0.8\n",
    "    n_train = (int)(split * n)\n",
    "\n",
    "    y = np.random.randint(28, size=(n, 2))\n",
    "    x = np.empty((n, 28, 28))\n",
    "\n",
    "    for i in tqdm(range(n)):\n",
    "        img = np.zeros((28, 28))\n",
    "        cv2.circle(img, (y[i][0], y[i][1]), 3, 255, -1)\n",
    "        x[i] = img\n",
    "    return ((x[:n_train], y[:n_train]), (x[n_train:], y[n_train:]))\n",
    "\n",
    "\n",
    "def label(y):\n",
    "    return np.around(y).astype('int')\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "# normalize image\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(x_test[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(str(label(y_test[i])))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def pos_accuracy(y_true, y_pred):\n",
    "    label_true = tf.round(y_true)\n",
    "    label_pred = tf.round(y_pred)\n",
    "    is_correct = tf.reduce_all(label_true == label_pred, axis=1)\n",
    "    is_correct = tf.cast(is_correct, 'float32')\n",
    "    score = tf.reduce_mean(is_correct)\n",
    "    return score\n",
    "\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # 이 부분은 수정하지 않도록 합시다.\n",
    "    keras.layers.Reshape(input_shape=(28, 28), target_shape=(28, 28, 1)),\n",
    "\n",
    "    # 다음은 첫번째 레이어의 예제입니다. 여러분이 수정해야 할 것은 이 레이어부터입니다.\n",
    "    keras.layers.Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    # ... 중간에 여러개의 다른 레이어를 추가해 봅시다.\n",
    "    # ...\n",
    "\n",
    "    # 이 부분은 수정하지 않도록 합시다.\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(2, activation=None)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=[pos_accuracy])\n",
    "\n",
    "\n",
    "# 이 자리에 LrReducer snippet을 추가합니다.\n",
    "class LrReducer(Callback):\n",
    "    def __init__(self, patience=5, reduce_rate=0.5, reduce_nb=5, verbose=1):\n",
    "        super(Callback, self).__init__()\n",
    "        self.patience = patience\n",
    "        self.wait = 0\n",
    "        self.best_score = -1.\n",
    "        self.reduce_rate = reduce_rate\n",
    "        self.current_reduce_nb = 0\n",
    "        self.reduce_nb = reduce_nb\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current_score = logs.get('val_pos_accuracy')\n",
    "        if current_score > self.best_score:\n",
    "            self.best_score = current_score\n",
    "            self.wait = 0\n",
    "            if self.verbose > 0:\n",
    "                print('---current best score: %.3f' % current_score)\n",
    "        else:\n",
    "            if self.wait >= self.patience:\n",
    "                self.current_reduce_nb += 1\n",
    "                if self.current_reduce_nb <= self.reduce_nb:\n",
    "                    lr = keras.backend.get_value(self.model.optimizer.lr)\n",
    "                    keras.backend.set_value(self.model.optimizer.lr, lr * self.reduce_rate)\n",
    "                    self.wait = 0\n",
    "                    if self.verbose > 0:\n",
    "                        print('---lr decreasing: %e' % (lr * self.reduce_rate))\n",
    "                else:\n",
    "                    if self.verbose > 0:\n",
    "                        print(\"Epoch %d: early stopping\" % (epoch))\n",
    "                    self.model.stop_training = True\n",
    "            self.wait += 1\n",
    "\n",
    "\n",
    "lrreducer = LrReducer(patience=20)\n",
    "\n",
    "# 이 자리에 ModelCheckpoint snippet을 추가합니다.\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"best_model.h5\", monitor='val_pos_accuracy', verbose=1,\n",
    "                                             save_best_only=True, mode='auto', save_freq=\"epoch\")\n",
    "n_batch = 32\n",
    "n_epochs = 400\n",
    "\n",
    "# 1. model.fit의 반환 결과를 history 변수에 저장합니다.\n",
    "# 2. lrreducer callback을 추가합니다.\n",
    "# 3. checkpoint callback을 추가합니다.\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    batch_size=n_batch,\n",
    "    epochs=n_epochs,\n",
    "    callbacks=[lrreducer, checkpoint]\n",
    ")\n",
    "\n",
    "# 여기에 저장된 weights를 loading하는 코드를 추가합니다.\n",
    "\n",
    "# 여기에 Graph snippet 코드를 추가합니다.\n",
    "\n",
    "# LrReducer 중 patience의 역할은 무엇입니까?\n",
    "ans01 = \"\"\"\n",
    "patience는 주어진 코드에서 val_pos_accuracy에서 개선이 없는 연속된 에포크 수를 나타내는 매개변수입니다.\n",
    "\"\"\"\n",
    "\n",
    "# LrReducer 중 reduce_rate의 역할은 무엇입니까?\n",
    "ans02 = \"\"\"\n",
    "reduce_rate는 학습률을 얼마나 감소시킬지를 결정하는 매개변수\n",
    "\"\"\"\n",
    "\n",
    "# LrReducer 중 reduce_nb의 역할은 무엇입니까?\n",
    "ans03 = \"\"\"\n",
    "reduce_nb는 학습률을 얼마나 자주 감소시킬 것인지를 결정하는 매개변수입니다. 이 매개변수는 학습률을 감소시키는 횟수를 제한하는 역할\n",
    "\"\"\"\n",
    "\n",
    "#  4주차 과제와 차이가 발생하였는지에 대해, 여기에 결과와 이유를 적어봅시다.\n",
    "#  주의: 아래의 코드 중 \"\"\"으로 둘러져있는 것은 python에서 여러줄의 텍스트를 string으로 지정하는 문법입니다. 이 문법을 유지하면서 내용을 적어주세요\n",
    "ans04 = \"\"\"\n",
    "결과: 4주차 과제에서는 val_pos_accuracy가 0.9449999928474426 이었다.\n",
    "이유: 4주차 과제에서는 학습률을 감소시키는 코드가 없었기 때문에 학습률이 너무 커서 학습이 잘 되지 않았다고 생각한다.\n",
    "다만 여기까지에서는 4주차가 더 높은 수치가 나왔던 이유는 패러미터 튜닝과, 좋은결과를 내기위해 우연의 요소에 기대어 오랜시간을 쏟은 결과라고 할수있습니다.\n",
    "\"\"\"\n",
    "\n",
    "# 모델을 ans02 변수에 기록합니다.\n",
    "ans05 = model\n",
    "\n",
    "# 모델의 성능 측정\n",
    "val_loss, val_pos_accuracy = model.evaluate(x_test, y_test)\n",
    "print('val_loss =', val_loss, 'val_pos_accuracy =', val_pos_accuracy)\n",
    "\n",
    "# val_loss의 저장\n",
    "ans06 = val_loss\n",
    "ans07 = val_pos_accuracy\n",
    "\n",
    "# Fully connnected Net의 경우와 ConvNet의 검증셋 성능 차이에 대해 그 이유를 추정해 봅시다.\n",
    "ans08 = \"\"\"\n",
    "FC모델은 4주차 과제에서 val_pos_accuracy가 0.9449999928474426 이었는데, ConvNet은 0.995192289352417 이었다.\n",
    "이유는 ConvNet은 이미지의 특징을 추출하는 Conv2D 레이어를 사용했기 때문이라고 생각한다.\n",
    "\"\"\"\n",
    "\n",
    "# ans09는 내용 중복으로 삭제하였습니다. 답변 작성이 필요없습니다.\n",
    "ans09 = \"\"\n"
   ],
   "id": "88a8ce29",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 269279.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x1000 with 25 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 11:14:32.240189: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-27 11:14:32.240481: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 11:14:33.298461: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-11-27 11:14:33.623856: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - ETA: 0s - loss: 134.8873 - pos_accuracy: 0.0012    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 11:14:38.240055: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---current best score: 0.002\n",
      "\n",
      "Epoch 1: val_pos_accuracy improved from -inf to 0.00240, saving model to best_model.h5\n",
      "50/50 [==============================] - 7s 46ms/step - loss: 134.8873 - pos_accuracy: 0.0012 - val_loss: 39.2055 - val_pos_accuracy: 0.0024\n",
      "Epoch 2/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 17.7618 - pos_accuracy: 0.0210---current best score: 0.091\n",
      "\n",
      "Epoch 2: val_pos_accuracy improved from 0.00240 to 0.09135, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 17.5477 - pos_accuracy: 0.0213 - val_loss: 9.1365 - val_pos_accuracy: 0.0913\n",
      "Epoch 3/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 6.7394 - pos_accuracy: 0.0884\n",
      "Epoch 3: val_pos_accuracy did not improve from 0.09135\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 6.6318 - pos_accuracy: 0.0894 - val_loss: 5.8481 - val_pos_accuracy: 0.0649\n",
      "Epoch 4/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 4.5338 - pos_accuracy: 0.0866---current best score: 0.111\n",
      "\n",
      "Epoch 4: val_pos_accuracy improved from 0.09135 to 0.11058, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 4.5333 - pos_accuracy: 0.0862 - val_loss: 4.2028 - val_pos_accuracy: 0.1106\n",
      "Epoch 5/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 3.1887 - pos_accuracy: 0.1371---current best score: 0.154\n",
      "\n",
      "Epoch 5: val_pos_accuracy improved from 0.11058 to 0.15385, saving model to best_model.h5\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 3.1561 - pos_accuracy: 0.1388 - val_loss: 2.9785 - val_pos_accuracy: 0.1538\n",
      "Epoch 6/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 2.2355 - pos_accuracy: 0.2006---current best score: 0.185\n",
      "\n",
      "Epoch 6: val_pos_accuracy improved from 0.15385 to 0.18510, saving model to best_model.h5\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 2.2355 - pos_accuracy: 0.2006 - val_loss: 2.2021 - val_pos_accuracy: 0.1851\n",
      "Epoch 7/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 1.6405 - pos_accuracy: 0.2666---current best score: 0.236\n",
      "\n",
      "Epoch 7: val_pos_accuracy improved from 0.18510 to 0.23558, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 1.6262 - pos_accuracy: 0.2656 - val_loss: 1.6517 - val_pos_accuracy: 0.2356\n",
      "Epoch 8/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.2234 - pos_accuracy: 0.3056---current best score: 0.293\n",
      "\n",
      "Epoch 8: val_pos_accuracy improved from 0.23558 to 0.29327, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 1.2234 - pos_accuracy: 0.3056 - val_loss: 1.2831 - val_pos_accuracy: 0.2933\n",
      "Epoch 9/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.9737 - pos_accuracy: 0.3520---current best score: 0.351\n",
      "\n",
      "Epoch 9: val_pos_accuracy improved from 0.29327 to 0.35096, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.9705 - pos_accuracy: 0.3531 - val_loss: 1.0749 - val_pos_accuracy: 0.3510\n",
      "Epoch 10/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7929 - pos_accuracy: 0.3856---current best score: 0.382\n",
      "\n",
      "Epoch 10: val_pos_accuracy improved from 0.35096 to 0.38221, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.7929 - pos_accuracy: 0.3856 - val_loss: 0.9112 - val_pos_accuracy: 0.3822\n",
      "Epoch 11/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.6507 - pos_accuracy: 0.3820---current best score: 0.440\n",
      "\n",
      "Epoch 11: val_pos_accuracy improved from 0.38221 to 0.43990, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.6655 - pos_accuracy: 0.3837 - val_loss: 0.7754 - val_pos_accuracy: 0.4399\n",
      "Epoch 12/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.5675 - pos_accuracy: 0.4388---current best score: 0.495\n",
      "\n",
      "Epoch 12: val_pos_accuracy improved from 0.43990 to 0.49519, saving model to best_model.h5\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.5545 - pos_accuracy: 0.4431 - val_loss: 0.6448 - val_pos_accuracy: 0.4952\n",
      "Epoch 13/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.4867 - pos_accuracy: 0.4755---current best score: 0.507\n",
      "\n",
      "Epoch 13: val_pos_accuracy improved from 0.49519 to 0.50721, saving model to best_model.h5\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.4736 - pos_accuracy: 0.4781 - val_loss: 0.5547 - val_pos_accuracy: 0.5072\n",
      "Epoch 14/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.4022 - pos_accuracy: 0.5300---current best score: 0.562\n",
      "\n",
      "Epoch 14: val_pos_accuracy improved from 0.50721 to 0.56250, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.3981 - pos_accuracy: 0.5331 - val_loss: 0.4944 - val_pos_accuracy: 0.5625\n",
      "Epoch 15/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.3235 - pos_accuracy: 0.6052\n",
      "Epoch 15: val_pos_accuracy did not improve from 0.56250\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.3328 - pos_accuracy: 0.6019 - val_loss: 0.4285 - val_pos_accuracy: 0.4784\n",
      "Epoch 16/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.2856 - pos_accuracy: 0.6281---current best score: 0.637\n",
      "\n",
      "Epoch 16: val_pos_accuracy improved from 0.56250 to 0.63702, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.2856 - pos_accuracy: 0.6281 - val_loss: 0.3594 - val_pos_accuracy: 0.6370\n",
      "Epoch 17/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.2376 - pos_accuracy: 0.6667---current best score: 0.651\n",
      "\n",
      "Epoch 17: val_pos_accuracy improved from 0.63702 to 0.65144, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.2423 - pos_accuracy: 0.6681 - val_loss: 0.3094 - val_pos_accuracy: 0.6514\n",
      "Epoch 18/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.2144 - pos_accuracy: 0.6988---current best score: 0.671\n",
      "\n",
      "Epoch 18: val_pos_accuracy improved from 0.65144 to 0.67067, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.2144 - pos_accuracy: 0.6988 - val_loss: 0.2718 - val_pos_accuracy: 0.6707\n",
      "Epoch 19/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1921 - pos_accuracy: 0.7070---current best score: 0.704\n",
      "\n",
      "Epoch 19: val_pos_accuracy improved from 0.67067 to 0.70433, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1905 - pos_accuracy: 0.7081 - val_loss: 0.2477 - val_pos_accuracy: 0.7043\n",
      "Epoch 20/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1729 - pos_accuracy: 0.7412---current best score: 0.736\n",
      "\n",
      "Epoch 20: val_pos_accuracy improved from 0.70433 to 0.73558, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1729 - pos_accuracy: 0.7412 - val_loss: 0.2217 - val_pos_accuracy: 0.7356\n",
      "Epoch 21/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1525 - pos_accuracy: 0.7706\n",
      "Epoch 21: val_pos_accuracy did not improve from 0.73558\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1525 - pos_accuracy: 0.7706 - val_loss: 0.2062 - val_pos_accuracy: 0.7308\n",
      "Epoch 22/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1381 - pos_accuracy: 0.8037\n",
      "Epoch 22: val_pos_accuracy did not improve from 0.73558\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1381 - pos_accuracy: 0.8037 - val_loss: 0.2004 - val_pos_accuracy: 0.7260\n",
      "Epoch 23/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1299 - pos_accuracy: 0.8150---current best score: 0.772\n",
      "\n",
      "Epoch 23: val_pos_accuracy improved from 0.73558 to 0.77163, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1299 - pos_accuracy: 0.8150 - val_loss: 0.1706 - val_pos_accuracy: 0.7716\n",
      "Epoch 24/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1195 - pos_accuracy: 0.8398---current best score: 0.788\n",
      "\n",
      "Epoch 24: val_pos_accuracy improved from 0.77163 to 0.78846, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.1192 - pos_accuracy: 0.8394 - val_loss: 0.1608 - val_pos_accuracy: 0.7885\n",
      "Epoch 25/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1137 - pos_accuracy: 0.8464---current best score: 0.815\n",
      "\n",
      "Epoch 25: val_pos_accuracy improved from 0.78846 to 0.81490, saving model to best_model.h5\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.1118 - pos_accuracy: 0.8487 - val_loss: 0.1515 - val_pos_accuracy: 0.8149\n",
      "Epoch 26/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1037 - pos_accuracy: 0.8619---current best score: 0.829\n",
      "\n",
      "Epoch 26: val_pos_accuracy improved from 0.81490 to 0.82933, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1037 - pos_accuracy: 0.8619 - val_loss: 0.1427 - val_pos_accuracy: 0.8293\n",
      "Epoch 27/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.1049 - pos_accuracy: 0.8622\n",
      "Epoch 27: val_pos_accuracy did not improve from 0.82933\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.1041 - pos_accuracy: 0.8631 - val_loss: 0.1438 - val_pos_accuracy: 0.7933\n",
      "Epoch 28/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0938 - pos_accuracy: 0.8875---current best score: 0.839\n",
      "\n",
      "Epoch 28: val_pos_accuracy improved from 0.82933 to 0.83894, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0938 - pos_accuracy: 0.8875 - val_loss: 0.1270 - val_pos_accuracy: 0.8389\n",
      "Epoch 29/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0889 - pos_accuracy: 0.8838---current best score: 0.858\n",
      "\n",
      "Epoch 29: val_pos_accuracy improved from 0.83894 to 0.85817, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0889 - pos_accuracy: 0.8838 - val_loss: 0.1195 - val_pos_accuracy: 0.8582\n",
      "Epoch 30/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0824 - pos_accuracy: 0.8978\n",
      "Epoch 30: val_pos_accuracy did not improve from 0.85817\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0862 - pos_accuracy: 0.8981 - val_loss: 0.1210 - val_pos_accuracy: 0.8558\n",
      "Epoch 31/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0870 - pos_accuracy: 0.8913---current best score: 0.873\n",
      "\n",
      "Epoch 31: val_pos_accuracy improved from 0.85817 to 0.87260, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0870 - pos_accuracy: 0.8913 - val_loss: 0.1134 - val_pos_accuracy: 0.8726\n",
      "Epoch 32/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0779 - pos_accuracy: 0.9120---current best score: 0.875\n",
      "\n",
      "Epoch 32: val_pos_accuracy improved from 0.87260 to 0.87500, saving model to best_model.h5\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0777 - pos_accuracy: 0.9125 - val_loss: 0.1041 - val_pos_accuracy: 0.8750\n",
      "Epoch 33/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0744 - pos_accuracy: 0.9154\n",
      "Epoch 33: val_pos_accuracy did not improve from 0.87500\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0732 - pos_accuracy: 0.9162 - val_loss: 0.1179 - val_pos_accuracy: 0.8558\n",
      "Epoch 34/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0704 - pos_accuracy: 0.9237---current best score: 0.885\n",
      "\n",
      "Epoch 34: val_pos_accuracy improved from 0.87500 to 0.88462, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0704 - pos_accuracy: 0.9237 - val_loss: 0.0956 - val_pos_accuracy: 0.8846\n",
      "Epoch 35/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0678 - pos_accuracy: 0.9250---current best score: 0.901\n",
      "\n",
      "Epoch 35: val_pos_accuracy improved from 0.88462 to 0.90144, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0678 - pos_accuracy: 0.9250 - val_loss: 0.0876 - val_pos_accuracy: 0.9014\n",
      "Epoch 36/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0587 - pos_accuracy: 0.9362\n",
      "Epoch 36: val_pos_accuracy did not improve from 0.90144\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0638 - pos_accuracy: 0.9331 - val_loss: 0.0972 - val_pos_accuracy: 0.8822\n",
      "Epoch 37/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0622 - pos_accuracy: 0.9368---current best score: 0.909\n",
      "\n",
      "Epoch 37: val_pos_accuracy improved from 0.90144 to 0.90865, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0613 - pos_accuracy: 0.9381 - val_loss: 0.0831 - val_pos_accuracy: 0.9087\n",
      "Epoch 38/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0594 - pos_accuracy: 0.9401\n",
      "Epoch 38: val_pos_accuracy did not improve from 0.90865\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0593 - pos_accuracy: 0.9400 - val_loss: 0.0896 - val_pos_accuracy: 0.8870\n",
      "Epoch 39/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0574 - pos_accuracy: 0.9445---current best score: 0.913\n",
      "\n",
      "Epoch 39: val_pos_accuracy improved from 0.90865 to 0.91346, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0571 - pos_accuracy: 0.9444 - val_loss: 0.0760 - val_pos_accuracy: 0.9135\n",
      "Epoch 40/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0549 - pos_accuracy: 0.9471\n",
      "Epoch 40: val_pos_accuracy did not improve from 0.91346\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0546 - pos_accuracy: 0.9463 - val_loss: 0.0770 - val_pos_accuracy: 0.9014\n",
      "Epoch 41/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0521 - pos_accuracy: 0.9522\n",
      "Epoch 41: val_pos_accuracy did not improve from 0.91346\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0521 - pos_accuracy: 0.9519 - val_loss: 0.0866 - val_pos_accuracy: 0.8942\n",
      "Epoch 42/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0549 - pos_accuracy: 0.9395\n",
      "Epoch 42: val_pos_accuracy did not improve from 0.91346\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0536 - pos_accuracy: 0.9419 - val_loss: 0.0698 - val_pos_accuracy: 0.9135\n",
      "Epoch 43/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0436 - pos_accuracy: 0.9518---current best score: 0.921\n",
      "\n",
      "Epoch 43: val_pos_accuracy improved from 0.91346 to 0.92067, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0490 - pos_accuracy: 0.9506 - val_loss: 0.0648 - val_pos_accuracy: 0.9207\n",
      "Epoch 44/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0479 - pos_accuracy: 0.9573---current best score: 0.947\n",
      "\n",
      "Epoch 44: val_pos_accuracy improved from 0.92067 to 0.94712, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0477 - pos_accuracy: 0.9563 - val_loss: 0.0621 - val_pos_accuracy: 0.9471\n",
      "Epoch 45/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0446 - pos_accuracy: 0.9554\n",
      "Epoch 45: val_pos_accuracy did not improve from 0.94712\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0443 - pos_accuracy: 0.9563 - val_loss: 0.0636 - val_pos_accuracy: 0.9279\n",
      "Epoch 46/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0441 - pos_accuracy: 0.9588\n",
      "Epoch 46: val_pos_accuracy did not improve from 0.94712\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0441 - pos_accuracy: 0.9588 - val_loss: 0.0603 - val_pos_accuracy: 0.9351\n",
      "Epoch 47/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0419 - pos_accuracy: 0.9600\n",
      "Epoch 47: val_pos_accuracy did not improve from 0.94712\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0419 - pos_accuracy: 0.9600 - val_loss: 0.0646 - val_pos_accuracy: 0.9447\n",
      "Epoch 48/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0428 - pos_accuracy: 0.9661---current best score: 0.952\n",
      "\n",
      "Epoch 48: val_pos_accuracy improved from 0.94712 to 0.95192, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0425 - pos_accuracy: 0.9656 - val_loss: 0.0609 - val_pos_accuracy: 0.9519\n",
      "Epoch 49/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0387 - pos_accuracy: 0.9668\n",
      "Epoch 49: val_pos_accuracy did not improve from 0.95192\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0385 - pos_accuracy: 0.9669 - val_loss: 0.0605 - val_pos_accuracy: 0.9423\n",
      "Epoch 50/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0394 - pos_accuracy: 0.9630\n",
      "Epoch 50: val_pos_accuracy did not improve from 0.95192\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0391 - pos_accuracy: 0.9638 - val_loss: 0.0539 - val_pos_accuracy: 0.9495\n",
      "Epoch 51/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0356 - pos_accuracy: 0.9681---current best score: 0.954\n",
      "\n",
      "Epoch 51: val_pos_accuracy improved from 0.95192 to 0.95433, saving model to best_model.h5\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0350 - pos_accuracy: 0.9694 - val_loss: 0.0527 - val_pos_accuracy: 0.9543\n",
      "Epoch 52/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0323 - pos_accuracy: 0.9715\n",
      "Epoch 52: val_pos_accuracy did not improve from 0.95433\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0345 - pos_accuracy: 0.9700 - val_loss: 0.0513 - val_pos_accuracy: 0.9375\n",
      "Epoch 53/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0366 - pos_accuracy: 0.9701\n",
      "Epoch 53: val_pos_accuracy did not improve from 0.95433\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0370 - pos_accuracy: 0.9688 - val_loss: 0.0501 - val_pos_accuracy: 0.9399\n",
      "Epoch 54/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0344 - pos_accuracy: 0.9669\n",
      "Epoch 54: val_pos_accuracy did not improve from 0.95433\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0344 - pos_accuracy: 0.9669 - val_loss: 0.0484 - val_pos_accuracy: 0.9399\n",
      "Epoch 55/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0323 - pos_accuracy: 0.9739---current best score: 0.969\n",
      "\n",
      "Epoch 55: val_pos_accuracy improved from 0.95433 to 0.96875, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0320 - pos_accuracy: 0.9744 - val_loss: 0.0510 - val_pos_accuracy: 0.9688\n",
      "Epoch 56/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0310 - pos_accuracy: 0.9707\n",
      "Epoch 56: val_pos_accuracy did not improve from 0.96875\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0307 - pos_accuracy: 0.9706 - val_loss: 0.0447 - val_pos_accuracy: 0.9471\n",
      "Epoch 57/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0291 - pos_accuracy: 0.9792\n",
      "Epoch 57: val_pos_accuracy did not improve from 0.96875\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0292 - pos_accuracy: 0.9787 - val_loss: 0.0465 - val_pos_accuracy: 0.9543\n",
      "Epoch 58/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0303 - pos_accuracy: 0.9767\n",
      "Epoch 58: val_pos_accuracy did not improve from 0.96875\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0302 - pos_accuracy: 0.9762 - val_loss: 0.0409 - val_pos_accuracy: 0.9639\n",
      "Epoch 59/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0285 - pos_accuracy: 0.9769\n",
      "Epoch 59: val_pos_accuracy did not improve from 0.96875\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0285 - pos_accuracy: 0.9769 - val_loss: 0.0474 - val_pos_accuracy: 0.9447\n",
      "Epoch 60/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0311 - pos_accuracy: 0.9759\n",
      "Epoch 60: val_pos_accuracy did not improve from 0.96875\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0312 - pos_accuracy: 0.9750 - val_loss: 0.0396 - val_pos_accuracy: 0.9615\n",
      "Epoch 61/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0286 - pos_accuracy: 0.9751\n",
      "Epoch 61: val_pos_accuracy did not improve from 0.96875\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0287 - pos_accuracy: 0.9750 - val_loss: 0.0414 - val_pos_accuracy: 0.9639\n",
      "Epoch 62/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0271 - pos_accuracy: 0.9807---current best score: 0.974\n",
      "\n",
      "Epoch 62: val_pos_accuracy improved from 0.96875 to 0.97356, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0266 - pos_accuracy: 0.9819 - val_loss: 0.0365 - val_pos_accuracy: 0.9736\n",
      "Epoch 63/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0262 - pos_accuracy: 0.9809\n",
      "Epoch 63: val_pos_accuracy did not improve from 0.97356\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0263 - pos_accuracy: 0.9806 - val_loss: 0.0382 - val_pos_accuracy: 0.9736\n",
      "Epoch 64/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0252 - pos_accuracy: 0.9779\n",
      "Epoch 64: val_pos_accuracy did not improve from 0.97356\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0250 - pos_accuracy: 0.9781 - val_loss: 0.0356 - val_pos_accuracy: 0.9736\n",
      "Epoch 65/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0236 - pos_accuracy: 0.9809\n",
      "Epoch 65: val_pos_accuracy did not improve from 0.97356\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0235 - pos_accuracy: 0.9812 - val_loss: 0.0413 - val_pos_accuracy: 0.9688\n",
      "Epoch 66/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0269 - pos_accuracy: 0.9812\n",
      "Epoch 66: val_pos_accuracy did not improve from 0.97356\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0269 - pos_accuracy: 0.9812 - val_loss: 0.0369 - val_pos_accuracy: 0.9712\n",
      "Epoch 67/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0233 - pos_accuracy: 0.9803\n",
      "Epoch 67: val_pos_accuracy did not improve from 0.97356\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0236 - pos_accuracy: 0.9794 - val_loss: 0.0371 - val_pos_accuracy: 0.9736\n",
      "Epoch 68/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0257 - pos_accuracy: 0.9834\n",
      "Epoch 68: val_pos_accuracy did not improve from 0.97356\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0254 - pos_accuracy: 0.9837 - val_loss: 0.0312 - val_pos_accuracy: 0.9736\n",
      "Epoch 69/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0205 - pos_accuracy: 0.9869---current best score: 0.976\n",
      "\n",
      "Epoch 69: val_pos_accuracy improved from 0.97356 to 0.97596, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0205 - pos_accuracy: 0.9869 - val_loss: 0.0321 - val_pos_accuracy: 0.9760\n",
      "Epoch 70/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0208 - pos_accuracy: 0.9817\n",
      "Epoch 70: val_pos_accuracy did not improve from 0.97596\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0206 - pos_accuracy: 0.9825 - val_loss: 0.0344 - val_pos_accuracy: 0.9712\n",
      "Epoch 71/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0192 - pos_accuracy: 0.9863---current best score: 0.978\n",
      "\n",
      "Epoch 71: val_pos_accuracy improved from 0.97596 to 0.97837, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0201 - pos_accuracy: 0.9850 - val_loss: 0.0305 - val_pos_accuracy: 0.9784\n",
      "Epoch 72/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0197 - pos_accuracy: 0.9867\n",
      "Epoch 72: val_pos_accuracy did not improve from 0.97837\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0198 - pos_accuracy: 0.9856 - val_loss: 0.0322 - val_pos_accuracy: 0.9784\n",
      "Epoch 73/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0193 - pos_accuracy: 0.9866---current best score: 0.981\n",
      "\n",
      "Epoch 73: val_pos_accuracy improved from 0.97837 to 0.98077, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0192 - pos_accuracy: 0.9869 - val_loss: 0.0305 - val_pos_accuracy: 0.9808\n",
      "Epoch 74/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0189 - pos_accuracy: 0.9871\n",
      "Epoch 74: val_pos_accuracy did not improve from 0.98077\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0193 - pos_accuracy: 0.9869 - val_loss: 0.0289 - val_pos_accuracy: 0.9784\n",
      "Epoch 75/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0221 - pos_accuracy: 0.9879---current best score: 0.983\n",
      "\n",
      "Epoch 75: val_pos_accuracy improved from 0.98077 to 0.98317, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0220 - pos_accuracy: 0.9881 - val_loss: 0.0278 - val_pos_accuracy: 0.9832\n",
      "Epoch 76/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0206 - pos_accuracy: 0.9887\n",
      "Epoch 76: val_pos_accuracy did not improve from 0.98317\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0206 - pos_accuracy: 0.9887 - val_loss: 0.0319 - val_pos_accuracy: 0.9784\n",
      "Epoch 77/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0174 - pos_accuracy: 0.9896\n",
      "Epoch 77: val_pos_accuracy did not improve from 0.98317\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0174 - pos_accuracy: 0.9887 - val_loss: 0.0280 - val_pos_accuracy: 0.9832\n",
      "Epoch 78/400\n",
      "44/50 [=========================>....] - ETA: 0s - loss: 0.0181 - pos_accuracy: 0.9893---current best score: 0.986\n",
      "\n",
      "Epoch 78: val_pos_accuracy improved from 0.98317 to 0.98558, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0183 - pos_accuracy: 0.9887 - val_loss: 0.0299 - val_pos_accuracy: 0.9856\n",
      "Epoch 79/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0172 - pos_accuracy: 0.9922\n",
      "Epoch 79: val_pos_accuracy did not improve from 0.98558\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0170 - pos_accuracy: 0.9919 - val_loss: 0.0258 - val_pos_accuracy: 0.9856\n",
      "Epoch 80/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0152 - pos_accuracy: 0.9925---current best score: 0.988\n",
      "\n",
      "Epoch 80: val_pos_accuracy improved from 0.98558 to 0.98798, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0152 - pos_accuracy: 0.9925 - val_loss: 0.0237 - val_pos_accuracy: 0.9880\n",
      "Epoch 81/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0157 - pos_accuracy: 0.9923\n",
      "Epoch 81: val_pos_accuracy did not improve from 0.98798\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0156 - pos_accuracy: 0.9925 - val_loss: 0.0252 - val_pos_accuracy: 0.9880\n",
      "Epoch 82/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0155 - pos_accuracy: 0.9934\n",
      "Epoch 82: val_pos_accuracy did not improve from 0.98798\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0159 - pos_accuracy: 0.9931 - val_loss: 0.0251 - val_pos_accuracy: 0.9856\n",
      "Epoch 83/400\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.0185 - pos_accuracy: 0.9931\n",
      "Epoch 83: val_pos_accuracy did not improve from 0.98798\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0180 - pos_accuracy: 0.9937 - val_loss: 0.0292 - val_pos_accuracy: 0.9832\n",
      "Epoch 84/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0190 - pos_accuracy: 0.9923\n",
      "Epoch 84: val_pos_accuracy did not improve from 0.98798\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0190 - pos_accuracy: 0.9925 - val_loss: 0.0294 - val_pos_accuracy: 0.9832\n",
      "Epoch 85/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0156 - pos_accuracy: 0.9935---current best score: 0.990\n",
      "\n",
      "Epoch 85: val_pos_accuracy improved from 0.98798 to 0.99038, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0156 - pos_accuracy: 0.9937 - val_loss: 0.0240 - val_pos_accuracy: 0.9904\n",
      "Epoch 86/400\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.0149 - pos_accuracy: 0.9937\n",
      "Epoch 86: val_pos_accuracy did not improve from 0.99038\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0146 - pos_accuracy: 0.9931 - val_loss: 0.0226 - val_pos_accuracy: 0.9880\n",
      "Epoch 87/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0145 - pos_accuracy: 0.9937\n",
      "Epoch 87: val_pos_accuracy did not improve from 0.99038\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0145 - pos_accuracy: 0.9937 - val_loss: 0.0222 - val_pos_accuracy: 0.9856\n",
      "Epoch 88/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0141 - pos_accuracy: 0.9946\n",
      "Epoch 88: val_pos_accuracy did not improve from 0.99038\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0146 - pos_accuracy: 0.9925 - val_loss: 0.0229 - val_pos_accuracy: 0.9832\n",
      "Epoch 89/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0136 - pos_accuracy: 0.9949---current best score: 0.993\n",
      "\n",
      "Epoch 89: val_pos_accuracy improved from 0.99038 to 0.99279, saving model to best_model.h5\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0135 - pos_accuracy: 0.9950 - val_loss: 0.0214 - val_pos_accuracy: 0.9928\n",
      "Epoch 90/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0128 - pos_accuracy: 0.9930\n",
      "Epoch 90: val_pos_accuracy did not improve from 0.99279\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0129 - pos_accuracy: 0.9925 - val_loss: 0.0206 - val_pos_accuracy: 0.9904\n",
      "Epoch 91/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0143 - pos_accuracy: 0.9937\n",
      "Epoch 91: val_pos_accuracy did not improve from 0.99279\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0143 - pos_accuracy: 0.9937 - val_loss: 0.0241 - val_pos_accuracy: 0.9904\n",
      "Epoch 92/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0124 - pos_accuracy: 0.9944\n",
      "Epoch 92: val_pos_accuracy did not improve from 0.99279\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0124 - pos_accuracy: 0.9944 - val_loss: 0.0226 - val_pos_accuracy: 0.9880\n",
      "Epoch 93/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0117 - pos_accuracy: 0.9950\n",
      "Epoch 93: val_pos_accuracy did not improve from 0.99279\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0117 - pos_accuracy: 0.9950 - val_loss: 0.0231 - val_pos_accuracy: 0.9904\n",
      "Epoch 94/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0130 - pos_accuracy: 0.9937\n",
      "Epoch 94: val_pos_accuracy did not improve from 0.99279\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0130 - pos_accuracy: 0.9937 - val_loss: 0.0205 - val_pos_accuracy: 0.9880\n",
      "Epoch 95/400\n",
      "44/50 [=========================>....] - ETA: 0s - loss: 0.0138 - pos_accuracy: 0.9950\n",
      "Epoch 95: val_pos_accuracy did not improve from 0.99279\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0132 - pos_accuracy: 0.9956 - val_loss: 0.0189 - val_pos_accuracy: 0.9928\n",
      "Epoch 96/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0117 - pos_accuracy: 0.9937\n",
      "Epoch 96: val_pos_accuracy did not improve from 0.99279\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0117 - pos_accuracy: 0.9937 - val_loss: 0.0181 - val_pos_accuracy: 0.9928\n",
      "Epoch 97/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0124 - pos_accuracy: 0.9943\n",
      "Epoch 97: val_pos_accuracy did not improve from 0.99279\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0125 - pos_accuracy: 0.9937 - val_loss: 0.0208 - val_pos_accuracy: 0.9904\n",
      "Epoch 98/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0105 - pos_accuracy: 0.9949---current best score: 0.995\n",
      "\n",
      "Epoch 98: val_pos_accuracy improved from 0.99279 to 0.99519, saving model to best_model.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0105 - pos_accuracy: 0.9950 - val_loss: 0.0210 - val_pos_accuracy: 0.9952\n",
      "Epoch 99/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0125 - pos_accuracy: 0.9943\n",
      "Epoch 99: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0125 - pos_accuracy: 0.9944 - val_loss: 0.0291 - val_pos_accuracy: 0.9880\n",
      "Epoch 100/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0128 - pos_accuracy: 0.9969\n",
      "Epoch 100: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0128 - pos_accuracy: 0.9969 - val_loss: 0.0193 - val_pos_accuracy: 0.9952\n",
      "Epoch 101/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0115 - pos_accuracy: 0.9962\n",
      "Epoch 101: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0115 - pos_accuracy: 0.9962 - val_loss: 0.0223 - val_pos_accuracy: 0.9952\n",
      "Epoch 102/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0098 - pos_accuracy: 0.9962\n",
      "Epoch 102: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0098 - pos_accuracy: 0.9962 - val_loss: 0.0167 - val_pos_accuracy: 0.9928\n",
      "Epoch 103/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0109 - pos_accuracy: 0.9955\n",
      "Epoch 103: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0109 - pos_accuracy: 0.9956 - val_loss: 0.0226 - val_pos_accuracy: 0.9952\n",
      "Epoch 104/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0125 - pos_accuracy: 0.9953\n",
      "Epoch 104: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0122 - pos_accuracy: 0.9956 - val_loss: 0.0192 - val_pos_accuracy: 0.9928\n",
      "Epoch 105/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0101 - pos_accuracy: 0.9968\n",
      "Epoch 105: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0104 - pos_accuracy: 0.9962 - val_loss: 0.0200 - val_pos_accuracy: 0.9952\n",
      "Epoch 106/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0104 - pos_accuracy: 0.9968\n",
      "Epoch 106: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0104 - pos_accuracy: 0.9969 - val_loss: 0.0195 - val_pos_accuracy: 0.9952\n",
      "Epoch 107/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0095 - pos_accuracy: 0.9987\n",
      "Epoch 107: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0098 - pos_accuracy: 0.9981 - val_loss: 0.0162 - val_pos_accuracy: 0.9952\n",
      "Epoch 108/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0103 - pos_accuracy: 0.9973\n",
      "Epoch 108: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0101 - pos_accuracy: 0.9975 - val_loss: 0.0168 - val_pos_accuracy: 0.9952\n",
      "Epoch 109/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0083 - pos_accuracy: 0.9967\n",
      "Epoch 109: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0083 - pos_accuracy: 0.9969 - val_loss: 0.0148 - val_pos_accuracy: 0.9952\n",
      "Epoch 110/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0073 - pos_accuracy: 0.9974\n",
      "Epoch 110: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0077 - pos_accuracy: 0.9969 - val_loss: 0.0152 - val_pos_accuracy: 0.9952\n",
      "Epoch 111/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0106 - pos_accuracy: 0.9975\n",
      "Epoch 111: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0106 - pos_accuracy: 0.9975 - val_loss: 0.0237 - val_pos_accuracy: 0.9952\n",
      "Epoch 112/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0109 - pos_accuracy: 0.9959\n",
      "Epoch 112: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0107 - pos_accuracy: 0.9962 - val_loss: 0.0148 - val_pos_accuracy: 0.9952\n",
      "Epoch 113/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0097 - pos_accuracy: 0.9973\n",
      "Epoch 113: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0098 - pos_accuracy: 0.9975 - val_loss: 0.0214 - val_pos_accuracy: 0.9904\n",
      "Epoch 114/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0099 - pos_accuracy: 0.9959\n",
      "Epoch 114: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0096 - pos_accuracy: 0.9962 - val_loss: 0.0182 - val_pos_accuracy: 0.9952\n",
      "Epoch 115/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0083 - pos_accuracy: 0.9974\n",
      "Epoch 115: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0083 - pos_accuracy: 0.9975 - val_loss: 0.0143 - val_pos_accuracy: 0.9952\n",
      "Epoch 116/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0090 - pos_accuracy: 0.9973\n",
      "Epoch 116: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0089 - pos_accuracy: 0.9975 - val_loss: 0.0168 - val_pos_accuracy: 0.9952\n",
      "Epoch 117/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0077 - pos_accuracy: 0.9981\n",
      "Epoch 117: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0077 - pos_accuracy: 0.9981 - val_loss: 0.0179 - val_pos_accuracy: 0.9952\n",
      "Epoch 118/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0103 - pos_accuracy: 0.9980\n",
      "Epoch 118: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.0102 - pos_accuracy: 0.9981 - val_loss: 0.0166 - val_pos_accuracy: 0.9952\n",
      "Epoch 119/400\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.0091 - pos_accuracy: 0.9979---lr decreasing: 5.000000e-04\n",
      "\n",
      "Epoch 119: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0091 - pos_accuracy: 0.9981 - val_loss: 0.0207 - val_pos_accuracy: 0.9952\n",
      "Epoch 120/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0065 - pos_accuracy: 0.9975\n",
      "Epoch 120: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0065 - pos_accuracy: 0.9975 - val_loss: 0.0139 - val_pos_accuracy: 0.9952\n",
      "Epoch 121/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0055 - pos_accuracy: 0.9980\n",
      "Epoch 121: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0056 - pos_accuracy: 0.9975 - val_loss: 0.0141 - val_pos_accuracy: 0.9952\n",
      "Epoch 122/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0051 - pos_accuracy: 0.9975\n",
      "Epoch 122: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0051 - pos_accuracy: 0.9975 - val_loss: 0.0130 - val_pos_accuracy: 0.9952\n",
      "Epoch 123/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0055 - pos_accuracy: 0.9981\n",
      "Epoch 123: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0055 - pos_accuracy: 0.9981 - val_loss: 0.0123 - val_pos_accuracy: 0.9952\n",
      "Epoch 124/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0056 - pos_accuracy: 0.9981\n",
      "Epoch 124: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0055 - pos_accuracy: 0.9981 - val_loss: 0.0131 - val_pos_accuracy: 0.9952\n",
      "Epoch 125/400\n",
      "44/50 [=========================>....] - ETA: 0s - loss: 0.0053 - pos_accuracy: 0.9979\n",
      "Epoch 125: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0052 - pos_accuracy: 0.9981 - val_loss: 0.0121 - val_pos_accuracy: 0.9952\n",
      "Epoch 126/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0048 - pos_accuracy: 0.9980\n",
      "Epoch 126: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0047 - pos_accuracy: 0.9981 - val_loss: 0.0119 - val_pos_accuracy: 0.9952\n",
      "Epoch 127/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0051 - pos_accuracy: 0.9981\n",
      "Epoch 127: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0050 - pos_accuracy: 0.9981 - val_loss: 0.0135 - val_pos_accuracy: 0.9952\n",
      "Epoch 128/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0058 - pos_accuracy: 0.9980\n",
      "Epoch 128: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0057 - pos_accuracy: 0.9981 - val_loss: 0.0130 - val_pos_accuracy: 0.9952\n",
      "Epoch 129/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0058 - pos_accuracy: 0.9980\n",
      "Epoch 129: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0058 - pos_accuracy: 0.9981 - val_loss: 0.0147 - val_pos_accuracy: 0.9952\n",
      "Epoch 130/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0049 - pos_accuracy: 0.9986\n",
      "Epoch 130: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0049 - pos_accuracy: 0.9987 - val_loss: 0.0132 - val_pos_accuracy: 0.9952\n",
      "Epoch 131/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0052 - pos_accuracy: 0.9980\n",
      "Epoch 131: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0052 - pos_accuracy: 0.9981 - val_loss: 0.0120 - val_pos_accuracy: 0.9952\n",
      "Epoch 132/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0045 - pos_accuracy: 0.9980\n",
      "Epoch 132: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0045 - pos_accuracy: 0.9981 - val_loss: 0.0120 - val_pos_accuracy: 0.9952\n",
      "Epoch 133/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0047 - pos_accuracy: 0.9980\n",
      "Epoch 133: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0048 - pos_accuracy: 0.9981 - val_loss: 0.0123 - val_pos_accuracy: 0.9952\n",
      "Epoch 134/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0057 - pos_accuracy: 0.9986\n",
      "Epoch 134: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0059 - pos_accuracy: 0.9981 - val_loss: 0.0124 - val_pos_accuracy: 0.9952\n",
      "Epoch 135/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0059 - pos_accuracy: 0.9980\n",
      "Epoch 135: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0059 - pos_accuracy: 0.9981 - val_loss: 0.0119 - val_pos_accuracy: 0.9952\n",
      "Epoch 136/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0055 - pos_accuracy: 0.9980\n",
      "Epoch 136: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0056 - pos_accuracy: 0.9981 - val_loss: 0.0126 - val_pos_accuracy: 0.9952\n",
      "Epoch 137/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0058 - pos_accuracy: 0.9981\n",
      "Epoch 137: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0058 - pos_accuracy: 0.9981 - val_loss: 0.0115 - val_pos_accuracy: 0.9952\n",
      "Epoch 138/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0046 - pos_accuracy: 0.9987\n",
      "Epoch 138: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.0045 - pos_accuracy: 0.9987 - val_loss: 0.0108 - val_pos_accuracy: 0.9952\n",
      "Epoch 139/400\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.0043 - pos_accuracy: 0.9979---lr decreasing: 2.500000e-04\n",
      "\n",
      "Epoch 139: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0043 - pos_accuracy: 0.9981 - val_loss: 0.0114 - val_pos_accuracy: 0.9952\n",
      "Epoch 140/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0038 - pos_accuracy: 0.9993\n",
      "Epoch 140: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0038 - pos_accuracy: 0.9994 - val_loss: 0.0113 - val_pos_accuracy: 0.9952\n",
      "Epoch 141/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0036 - pos_accuracy: 0.9981\n",
      "Epoch 141: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0036 - pos_accuracy: 0.9981 - val_loss: 0.0107 - val_pos_accuracy: 0.9952\n",
      "Epoch 142/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0036 - pos_accuracy: 0.9987\n",
      "Epoch 142: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0036 - pos_accuracy: 0.9987 - val_loss: 0.0105 - val_pos_accuracy: 0.9952\n",
      "Epoch 143/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0036 - pos_accuracy: 0.9981\n",
      "Epoch 143: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0036 - pos_accuracy: 0.9981 - val_loss: 0.0116 - val_pos_accuracy: 0.9952\n",
      "Epoch 144/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0036 - pos_accuracy: 0.9994\n",
      "Epoch 144: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0036 - pos_accuracy: 0.9994 - val_loss: 0.0113 - val_pos_accuracy: 0.9952\n",
      "Epoch 145/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0037 - pos_accuracy: 0.9980\n",
      "Epoch 145: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0037 - pos_accuracy: 0.9981 - val_loss: 0.0124 - val_pos_accuracy: 0.9952\n",
      "Epoch 146/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0035 - pos_accuracy: 0.9994\n",
      "Epoch 146: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0035 - pos_accuracy: 0.9994 - val_loss: 0.0108 - val_pos_accuracy: 0.9952\n",
      "Epoch 147/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0033 - pos_accuracy: 0.9987\n",
      "Epoch 147: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0033 - pos_accuracy: 0.9987 - val_loss: 0.0104 - val_pos_accuracy: 0.9952\n",
      "Epoch 148/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0036 - pos_accuracy: 0.9987\n",
      "Epoch 148: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0035 - pos_accuracy: 0.9987 - val_loss: 0.0109 - val_pos_accuracy: 0.9952\n",
      "Epoch 149/400\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.0033 - pos_accuracy: 0.9993\n",
      "Epoch 149: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0034 - pos_accuracy: 0.9987 - val_loss: 0.0115 - val_pos_accuracy: 0.9952\n",
      "Epoch 150/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0037 - pos_accuracy: 0.9994\n",
      "Epoch 150: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0037 - pos_accuracy: 0.9994 - val_loss: 0.0105 - val_pos_accuracy: 0.9952\n",
      "Epoch 151/400\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.0036 - pos_accuracy: 0.9986\n",
      "Epoch 151: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0035 - pos_accuracy: 0.9987 - val_loss: 0.0106 - val_pos_accuracy: 0.9952\n",
      "Epoch 152/400\n",
      "44/50 [=========================>....] - ETA: 0s - loss: 0.0036 - pos_accuracy: 0.9993\n",
      "Epoch 152: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0036 - pos_accuracy: 0.9994 - val_loss: 0.0102 - val_pos_accuracy: 0.9952\n",
      "Epoch 153/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0034 - pos_accuracy: 0.9987\n",
      "Epoch 153: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0034 - pos_accuracy: 0.9987 - val_loss: 0.0105 - val_pos_accuracy: 0.9952\n",
      "Epoch 154/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0030 - pos_accuracy: 0.9993\n",
      "Epoch 154: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0033 - pos_accuracy: 0.9987 - val_loss: 0.0121 - val_pos_accuracy: 0.9952\n",
      "Epoch 155/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0035 - pos_accuracy: 0.9993\n",
      "Epoch 155: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0035 - pos_accuracy: 0.9994 - val_loss: 0.0106 - val_pos_accuracy: 0.9952\n",
      "Epoch 156/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0033 - pos_accuracy: 0.9993\n",
      "Epoch 156: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0033 - pos_accuracy: 0.9994 - val_loss: 0.0102 - val_pos_accuracy: 0.9952\n",
      "Epoch 157/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0034 - pos_accuracy: 0.9993\n",
      "Epoch 157: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0034 - pos_accuracy: 0.9994 - val_loss: 0.0106 - val_pos_accuracy: 0.9952\n",
      "Epoch 158/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0036 - pos_accuracy: 0.9993\n",
      "Epoch 158: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0036 - pos_accuracy: 0.9994 - val_loss: 0.0108 - val_pos_accuracy: 0.9952\n",
      "Epoch 159/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0032 - pos_accuracy: 0.9993---lr decreasing: 1.250000e-04\n",
      "\n",
      "Epoch 159: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0032 - pos_accuracy: 0.9994 - val_loss: 0.0117 - val_pos_accuracy: 0.9952\n",
      "Epoch 160/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0030 - pos_accuracy: 0.9993\n",
      "Epoch 160: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0030 - pos_accuracy: 0.9994 - val_loss: 0.0100 - val_pos_accuracy: 0.9952\n",
      "Epoch 161/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0029 - pos_accuracy: 0.9993\n",
      "Epoch 161: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0029 - pos_accuracy: 0.9994 - val_loss: 0.0100 - val_pos_accuracy: 0.9952\n",
      "Epoch 162/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0029 - pos_accuracy: 0.9994\n",
      "Epoch 162: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0029 - pos_accuracy: 0.9994 - val_loss: 0.0103 - val_pos_accuracy: 0.9952\n",
      "Epoch 163/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0028 - pos_accuracy: 0.9994\n",
      "Epoch 163: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0029 - pos_accuracy: 0.9994 - val_loss: 0.0098 - val_pos_accuracy: 0.9952\n",
      "Epoch 164/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0028 - pos_accuracy: 0.9993\n",
      "Epoch 164: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0028 - pos_accuracy: 0.9994 - val_loss: 0.0101 - val_pos_accuracy: 0.9952\n",
      "Epoch 165/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0028 - pos_accuracy: 0.9993\n",
      "Epoch 165: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0028 - pos_accuracy: 0.9994 - val_loss: 0.0098 - val_pos_accuracy: 0.9952\n",
      "Epoch 166/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0029 - pos_accuracy: 0.9993\n",
      "Epoch 166: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0028 - pos_accuracy: 0.9994 - val_loss: 0.0100 - val_pos_accuracy: 0.9952\n",
      "Epoch 167/400\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.0029 - pos_accuracy: 0.9993\n",
      "Epoch 167: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0029 - pos_accuracy: 0.9994 - val_loss: 0.0101 - val_pos_accuracy: 0.9952\n",
      "Epoch 168/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0029 - pos_accuracy: 0.9994\n",
      "Epoch 168: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0029 - pos_accuracy: 0.9994 - val_loss: 0.0098 - val_pos_accuracy: 0.9952\n",
      "Epoch 169/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0028 - pos_accuracy: 0.9994\n",
      "Epoch 169: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0028 - pos_accuracy: 0.9994 - val_loss: 0.0100 - val_pos_accuracy: 0.9952\n",
      "Epoch 170/400\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.0029 - pos_accuracy: 1.0000\n",
      "Epoch 170: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0031 - pos_accuracy: 0.9994 - val_loss: 0.0109 - val_pos_accuracy: 0.9952\n",
      "Epoch 171/400\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.0029 - pos_accuracy: 0.9993\n",
      "Epoch 171: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0029 - pos_accuracy: 0.9994 - val_loss: 0.0099 - val_pos_accuracy: 0.9952\n",
      "Epoch 172/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0030 - pos_accuracy: 0.9994\n",
      "Epoch 172: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0030 - pos_accuracy: 0.9994 - val_loss: 0.0106 - val_pos_accuracy: 0.9952\n",
      "Epoch 173/400\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.0029 - pos_accuracy: 1.0000\n",
      "Epoch 173: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0030 - pos_accuracy: 0.9994 - val_loss: 0.0100 - val_pos_accuracy: 0.9952\n",
      "Epoch 174/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0028 - pos_accuracy: 0.9994\n",
      "Epoch 174: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0028 - pos_accuracy: 0.9994 - val_loss: 0.0096 - val_pos_accuracy: 0.9952\n",
      "Epoch 175/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0029 - pos_accuracy: 0.9994\n",
      "Epoch 175: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0029 - pos_accuracy: 0.9994 - val_loss: 0.0100 - val_pos_accuracy: 0.9952\n",
      "Epoch 176/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0028 - pos_accuracy: 0.9993\n",
      "Epoch 176: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0028 - pos_accuracy: 0.9994 - val_loss: 0.0097 - val_pos_accuracy: 0.9952\n",
      "Epoch 177/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0027 - pos_accuracy: 0.9993\n",
      "Epoch 177: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0026 - pos_accuracy: 0.9994 - val_loss: 0.0102 - val_pos_accuracy: 0.9952\n",
      "Epoch 178/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0028 - pos_accuracy: 0.9994\n",
      "Epoch 178: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0028 - pos_accuracy: 0.9994 - val_loss: 0.0095 - val_pos_accuracy: 0.9952\n",
      "Epoch 179/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0027 - pos_accuracy: 0.9993---lr decreasing: 6.250000e-05\n",
      "\n",
      "Epoch 179: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0027 - pos_accuracy: 0.9994 - val_loss: 0.0096 - val_pos_accuracy: 0.9952\n",
      "Epoch 180/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0025 - pos_accuracy: 0.9993\n",
      "Epoch 180: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0025 - pos_accuracy: 0.9994 - val_loss: 0.0098 - val_pos_accuracy: 0.9952\n",
      "Epoch 181/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0025 - pos_accuracy: 0.9993\n",
      "Epoch 181: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0024 - pos_accuracy: 0.9994 - val_loss: 0.0095 - val_pos_accuracy: 0.9952\n",
      "Epoch 182/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0025 - pos_accuracy: 0.9993\n",
      "Epoch 182: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0025 - pos_accuracy: 0.9994 - val_loss: 0.0100 - val_pos_accuracy: 0.9952\n",
      "Epoch 183/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0025 - pos_accuracy: 0.9993\n",
      "Epoch 183: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0025 - pos_accuracy: 0.9994 - val_loss: 0.0096 - val_pos_accuracy: 0.9952\n",
      "Epoch 184/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0024 - pos_accuracy: 0.9993\n",
      "Epoch 184: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0024 - pos_accuracy: 0.9994 - val_loss: 0.0096 - val_pos_accuracy: 0.9952\n",
      "Epoch 185/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0025 - pos_accuracy: 0.9994\n",
      "Epoch 185: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0025 - pos_accuracy: 0.9994 - val_loss: 0.0099 - val_pos_accuracy: 0.9952\n",
      "Epoch 186/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0025 - pos_accuracy: 0.9993\n",
      "Epoch 186: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0025 - pos_accuracy: 0.9994 - val_loss: 0.0095 - val_pos_accuracy: 0.9952\n",
      "Epoch 187/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0025 - pos_accuracy: 0.9994\n",
      "Epoch 187: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0025 - pos_accuracy: 0.9994 - val_loss: 0.0095 - val_pos_accuracy: 0.9952\n",
      "Epoch 188/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0025 - pos_accuracy: 0.9993\n",
      "Epoch 188: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0025 - pos_accuracy: 0.9994 - val_loss: 0.0096 - val_pos_accuracy: 0.9952\n",
      "Epoch 189/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0025 - pos_accuracy: 0.9993\n",
      "Epoch 189: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0025 - pos_accuracy: 0.9994 - val_loss: 0.0095 - val_pos_accuracy: 0.9952\n",
      "Epoch 190/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0024 - pos_accuracy: 0.9994\n",
      "Epoch 190: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0024 - pos_accuracy: 0.9994 - val_loss: 0.0097 - val_pos_accuracy: 0.9952\n",
      "Epoch 191/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0025 - pos_accuracy: 0.9994\n",
      "Epoch 191: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0025 - pos_accuracy: 0.9994 - val_loss: 0.0096 - val_pos_accuracy: 0.9952\n",
      "Epoch 192/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0025 - pos_accuracy: 0.9993\n",
      "Epoch 192: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0024 - pos_accuracy: 0.9994 - val_loss: 0.0096 - val_pos_accuracy: 0.9952\n",
      "Epoch 193/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0025 - pos_accuracy: 0.9993\n",
      "Epoch 193: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0024 - pos_accuracy: 0.9994 - val_loss: 0.0097 - val_pos_accuracy: 0.9952\n",
      "Epoch 194/400\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.0023 - pos_accuracy: 1.0000\n",
      "Epoch 194: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0024 - pos_accuracy: 0.9994 - val_loss: 0.0100 - val_pos_accuracy: 0.9952\n",
      "Epoch 195/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0024 - pos_accuracy: 0.9993\n",
      "Epoch 195: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0025 - pos_accuracy: 0.9994 - val_loss: 0.0094 - val_pos_accuracy: 0.9952\n",
      "Epoch 196/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0024 - pos_accuracy: 0.9994\n",
      "Epoch 196: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0024 - pos_accuracy: 0.9994 - val_loss: 0.0095 - val_pos_accuracy: 0.9952\n",
      "Epoch 197/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0024 - pos_accuracy: 0.9994\n",
      "Epoch 197: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0024 - pos_accuracy: 0.9994 - val_loss: 0.0096 - val_pos_accuracy: 0.9952\n",
      "Epoch 198/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0024 - pos_accuracy: 0.9993\n",
      "Epoch 198: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.0024 - pos_accuracy: 0.9994 - val_loss: 0.0096 - val_pos_accuracy: 0.9952\n",
      "Epoch 199/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0024 - pos_accuracy: 0.9993---lr decreasing: 3.125000e-05\n",
      "\n",
      "Epoch 199: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0024 - pos_accuracy: 0.9994 - val_loss: 0.0100 - val_pos_accuracy: 0.9952\n",
      "Epoch 200/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0023 - pos_accuracy: 0.9994\n",
      "Epoch 200: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0023 - pos_accuracy: 0.9994 - val_loss: 0.0096 - val_pos_accuracy: 0.9952\n",
      "Epoch 201/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0022 - pos_accuracy: 0.9993\n",
      "Epoch 201: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.0094 - val_pos_accuracy: 0.9952\n",
      "Epoch 202/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0022 - pos_accuracy: 0.9993\n",
      "Epoch 202: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.0095 - val_pos_accuracy: 0.9952\n",
      "Epoch 203/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0022 - pos_accuracy: 0.9994\n",
      "Epoch 203: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.0094 - val_pos_accuracy: 0.9952\n",
      "Epoch 204/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0023 - pos_accuracy: 0.9994\n",
      "Epoch 204: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.0093 - val_pos_accuracy: 0.9952\n",
      "Epoch 205/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0022 - pos_accuracy: 0.9993\n",
      "Epoch 205: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.0095 - val_pos_accuracy: 0.9952\n",
      "Epoch 206/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0023 - pos_accuracy: 0.9994\n",
      "Epoch 206: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.0093 - val_pos_accuracy: 0.9952\n",
      "Epoch 207/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0023 - pos_accuracy: 0.9993\n",
      "Epoch 207: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0023 - pos_accuracy: 0.9994 - val_loss: 0.0097 - val_pos_accuracy: 0.9952\n",
      "Epoch 208/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0022 - pos_accuracy: 0.9993\n",
      "Epoch 208: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.0095 - val_pos_accuracy: 0.9952\n",
      "Epoch 209/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0023 - pos_accuracy: 0.9993\n",
      "Epoch 209: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0023 - pos_accuracy: 0.9994 - val_loss: 0.0096 - val_pos_accuracy: 0.9952\n",
      "Epoch 210/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0022 - pos_accuracy: 0.9993\n",
      "Epoch 210: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.0094 - val_pos_accuracy: 0.9952\n",
      "Epoch 211/400\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0022 - pos_accuracy: 0.9993\n",
      "Epoch 211: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.0094 - val_pos_accuracy: 0.9952\n",
      "Epoch 212/400\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.0022 - pos_accuracy: 0.9994\n",
      "Epoch 212: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.0094 - val_pos_accuracy: 0.9952\n",
      "Epoch 213/400\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.0024 - pos_accuracy: 0.9993\n",
      "Epoch 213: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.0023 - pos_accuracy: 0.9994 - val_loss: 0.0093 - val_pos_accuracy: 0.9952\n",
      "Epoch 214/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0022 - pos_accuracy: 0.9993\n",
      "Epoch 214: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.0095 - val_pos_accuracy: 0.9952\n",
      "Epoch 215/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0021 - pos_accuracy: 0.9993\n",
      "Epoch 215: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.0093 - val_pos_accuracy: 0.9952\n",
      "Epoch 216/400\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0022 - pos_accuracy: 0.9994\n",
      "Epoch 216: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.0093 - val_pos_accuracy: 0.9952\n",
      "Epoch 217/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0023 - pos_accuracy: 0.9993\n",
      "Epoch 217: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.0096 - val_pos_accuracy: 0.9952\n",
      "Epoch 218/400\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0022 - pos_accuracy: 0.9993\n",
      "Epoch 218: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0022 - pos_accuracy: 0.9994 - val_loss: 0.0094 - val_pos_accuracy: 0.9952\n",
      "Epoch 219/400\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0022 - pos_accuracy: 0.9993Epoch 218: early stopping\n",
      "\n",
      "Epoch 219: val_pos_accuracy did not improve from 0.99519\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.0021 - pos_accuracy: 0.9994 - val_loss: 0.0092 - val_pos_accuracy: 0.9952\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 0.0092 - pos_accuracy: 0.9952\n",
      "val_loss = 0.009188910946249962 val_pos_accuracy = 0.995192289352417\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5baba92"
   },
   "source": [
    "# 2번과제 - 객체 탐지의 성능 측정 (50점)"
   ],
   "id": "c5baba92"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcf5249a"
   },
   "source": [
    "**템플릿 B**"
   ],
   "id": "fcf5249a"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c0e69cbb",
    "is_executing": true
   },
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as mp_image\n",
    "from matplotlib import patches\n",
    "import numpy as np\n",
    "\n",
    "# YOLO의 객체 탐지 추론 결과 (coco dataset 기준)\n",
    "# data/VOC2012/JPEGImages/2007_000559.jpg 이미지의 YOLO 추론 결과\n",
    "pred_class = ['person', 'mouse', 'keyboard', 'bottle', 'cup', 'tvmonitor']\n",
    "\n",
    "# 실제 추론결과에 맞도록 데이터를 수정하세요.\n",
    "confidence = np.array([0.11111111, 0.9976528, 0.9980576, 0.9573481, 0.49765933, 0.9629159])\n",
    "pred_box = np.array(\n",
    "    [[176.16505921, 33.64296697, 341.38774166, 208.91536349],\n",
    "     [333.33333333, 311.9840765, 444.85818948, 348.11093758],\n",
    "     [79.10724729, 311.60360575, 380.4496864, 366.52458648],\n",
    "     [33.86356309, 247.92885244, 79.78276448, 354.13598821],\n",
    "     [405.60171008, 177.75161535, 461.23234462, 238.10186431],\n",
    "     [159.76904333, 26.10973097, 371.53357512, 219.45996917]]\n",
    ")\n",
    "\n",
    "ans10 = confidence[0]\n",
    "ans11 = pred_box[1][0]\n",
    "\n",
    "# data/VOC2012/JPEGImages/2007_000559.jpg 이미지의 Ground Truth\n",
    "y_class = ['bottle', 'tvmonitor', 'keyboard', 'cup', 'mouse']\n",
    "y_box = np.array(\n",
    "    [[111, 111, 222, 222],  # bottle 이 부분을 수정하세요\n",
    "     [50, 222, 122, 333],  # tvmonitor 이 부분을 수정하세요\n",
    "     [77, 313, 382, 366],  # keyboard\n",
    "     [405, 177, 461, 238],  # cup\n",
    "     [385, 311, 444, 348]]  # mouse\n",
    "    , dtype='float64'\n",
    ")\n",
    "\n",
    "ans12 = y_box[0:2]\n",
    "\n",
    "img_url = 'https://user-images.githubusercontent.com/38934308/141396886-34953093-8fb6-4984-a0e9-3598ba802828.jpg'\n",
    "img_path = keras.utils.get_file(\"2007_000559.jpg\", img_url)\n",
    "image = mp_image.imread(img_path)\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "for box in pred_box:\n",
    "    ax.add_patch(patches.Rectangle(\n",
    "        (box[0], box[1]), box[2] - box[0] + 1, box[3] - box[1] + 1,\n",
    "        edgecolor='tab:blue', linewidth=3,\n",
    "        fill=False\n",
    "    ))\n",
    "for box in y_box:\n",
    "    ax.add_patch(patches.Rectangle(\n",
    "        (box[0], box[1]), box[2] - box[0] + 1, box[3] - box[1] + 1,\n",
    "        edgecolor='orange', linewidth=3, alpha=0.7,\n",
    "        fill=False\n",
    "    ))\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 두 개의 bounding box의 iou 계산\n",
    "def bbox_iou(box1, box2):\n",
    "    # box의 넓이 계산: area = w * h\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    # 교집합 box의 upper-left 좌표\n",
    "    left_up = np.maximum(box1[:2], box2[:2])\n",
    "    # 교집합 box의 lower-right 좌표\n",
    "    right_down = np.minimum(box1[2:], box2[2:])\n",
    "\n",
    "    # 교집합 box의 width와 height\n",
    "    inter_section = np.maximum(right_down - left_up, 0.0)\n",
    "\n",
    "    # 교집합 box의 넓이: area = w * h\n",
    "    inter_area = inter_section[0] * inter_section[1]\n",
    "\n",
    "    # 합집합 면적\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    # IOU의 산출\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "\n",
    "# 교집합 box의 upper-left 좌표와 lower-right좌표를 산출하는 원리를 적어주세요.\n",
    "ans13 = \"\"\"\n",
    "여기에 적어주세요.\n",
    "\"\"\"\n",
    "\n",
    "# 어떤 경우에 right_down - left_up이 음수가 되는 지 확인해 봅시다.\n",
    "ans14 = \"\"\"\n",
    "여기에 적어주세요.\n",
    "\"\"\"\n",
    "\n",
    "tp = []\n",
    "for p_c, p_b in zip(pred_class, pred_box):\n",
    "    tp_tmp = 0\n",
    "    for y_c, y_b in zip(y_class, y_box):\n",
    "        if y_c == p_c and bbox_iou(p_b, y_b) > 0.5:\n",
    "            tp_tmp = 1\n",
    "            break\n",
    "    tp.append(tp_tmp)\n",
    "\n",
    "tp = np.array(tp)\n",
    "# False Positive\n",
    "fp = 1 - tp\n",
    "\n",
    "npos = len(y_box)  # 총 GT의 갯수\n",
    "\n",
    "# sort by confidence (confidence가 큰 순으로 정렬)\n",
    "sorted_ind = np.argsort(-confidence)\n",
    "tp = tp[sorted_ind]\n",
    "fp = fp[sorted_ind]\n",
    "\n",
    "print('sorted tp =', tp)\n",
    "print('sorted fp =', fp)\n",
    "\n",
    "# 누적 precision recall의 계산\n",
    "fp = np.cumsum(fp)\n",
    "tp = np.cumsum(tp)\n",
    "rec = tp / float(npos)  # 누적 tp를 GT의 갯수로 나눔\n",
    "prec = tp / (tp + fp)\n",
    "print('prec =', prec)\n",
    "print('rec =', rec)\n",
    "\n",
    "\n",
    "# AP (average precision의 계산)\n",
    "def voc_ap(rec, prec):\n",
    "    # 양끝에 시작값과 끝값을 추가한다.\n",
    "    # X축(recall)은 0과 1사이에 존재해야 하므로 양쪽 끝에 0,1을 추가한다.\n",
    "    # Y축(precision)은 recall이 1일때 0값을 추가한다. (이것은 면적에 영향을 주지 않는다.)\n",
    "    # Y축의 시작값은 0을 추가하지만 바로 아래부분의 precision이 항상 감소하도록 해야하는 조건\n",
    "    # 때문에, 자동으로 변경될 것이다. (시작값은 recision의 최대값으로 변화하게 된다.)\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # precision graph가 항상 감소하도록 값을 보정한다.\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # PR 곡선아래의 면적을 계산하기 위해서 X축(recall)이 변화하는 지점을 찾는다.\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # 그리고 다음의 합계를 구한다.\n",
    "    # sum ( (\\Delta recall) * prec )\n",
    "    # (\\Delta recall)이란 X축(recall)에서 recall이 변화하는 양이다.\n",
    "    # 즉, 직사각형의 밑변의 길이가 된다.\n",
    "    # 여기에 Y축 높이(precision)값을 곱하면 직사각형의 넓이가 된다.\n",
    "    # 이것을 모두 합(sum)하면, 그래프 하단의 넓이가 된다.\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "\n",
    "ap = voc_ap(rec, prec)\n",
    "print('ap =', ap)\n",
    "\n",
    "ans15 = ap\n",
    "\n",
    "# 본 과제를 수행한 시간이나 기타 의견 등이 있으시면 적어 주세요. (공란으로 두셔도 됩니다.)\n",
    "ans16 = \"\"\"\n",
    "여기에 적어주세요.\n",
    "\"\"\""
   ],
   "id": "c0e69cbb",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3e623ac"
   },
   "source": [
    "이번에는 YOLO 객체 탐지 모델을 실행하고 성능을 측정하는 과제입니다.  \n",
    "이번 과제는 길어 보이지만 실제로 코드를 수정하는 부분은 없고, 데이터 부분만 수정하여 구동하는 것과 소스의 분석이 주된 목표입니다.  \n",
    "이 과제를 통하여 객체탐지의 성능 지표인 mAP에 대한 구성요소를 이해할 수 있을 것입니다.  \n",
    "(TP, FP, FN의 이해)  \n",
    "(precision과 recall의 이해)   \n",
    "(precision-recall graph를 이용한 AP 산출식의 이해)    \n",
    "\n",
    "1. 샘플 이미지를 이용한 YOLO의 구동\n",
    "\n",
    "   먼저 **9주차** 실습자료인 `08-voc.ipynb`를 처음부터 실행해봅시다.\n",
    "    \n",
    "   과제 노트북에 복사하지 마시고, 반드시 별도 창에서 실행해봐 주십시오.\n",
    "   **convert YOLOv4 weights to Keras h5** 섹션까지만 실행합니다.  \n",
    "   (**mAP의 계산** 섹션은 많은 시간이 소요되기 때문입니다.)  \n",
    "\n",
    "   다음 cell까지 실행하면 성공입니다.  \n",
    "   ```python\n",
    "   image = cv2.imread('data/VOC2012/JPEGImages/2007_000187.jpg')\n",
    "   image, boxes, scores, classes = _decode.detect_image(image, True)\n",
    "   cv2_imshow(image)\n",
    "   ```\n",
    "\n",
    "   이제 이미지의 경로를 다음과 같이 변경하고 실행합니다.\n",
    "   ```python\n",
    "   image = cv2.imread('data/VOC2012/JPEGImages/2007_000559.jpg')\n",
    "   image, boxes, scores, classes = _decode.detect_image(image, True)\n",
    "   cv2_imshow(image)\n",
    "   ```\n",
    "\n",
    "   다음의 이미지가 출력될 것입니다.  \n",
    "\n",
    "   **그림 1.**  \n",
    "   ![그림1](https://user-images.githubusercontent.com/38934308/141392547-5fd69ae5-070a-49b9-8207-8ecbb4c2d50d.png)  \n",
    "\n",
    "   Cell에 다음 코드를 추가하고 실행합니다. (`08-voc.ipynb`에서 수행합니다.)  \n",
    "   ```python\n",
    "   class_names = get_class('model_data/coco_classes.txt')\n",
    "   for c, s in zip(classes, scores):\n",
    "       print(class_names[c], s)\n",
    "   print(boxes)\n",
    "   ```\n",
    "   그러면 다음과 같은 출력 결과를 얻을 수 있습니다. 소숫점 뒤쪽은 약간의 오차가 있을 수 있습니다.\n",
    "   (`????`부분은 여러분들이 직접 데이터를 확인해 주세요.)\n",
    "   ```\n",
    "   person ?.???????\n",
    "   mouse 0.9976528\n",
    "   keyboard 0.9980576\n",
    "   bottle 0.9573481\n",
    "   cup 0.49765933\n",
    "   tvmonitor 0.9629159\n",
    "   [[176.16505921  33.64296697 341.38774166 208.91536349]\n",
    "    [???.???????? 311.9840765  444.85818948 348.11093758]\n",
    "    [ 79.10724729 311.60360575 380.4496864  366.52458648]\n",
    "    [ 33.86356309 247.92885244  79.78276448 354.13598821]\n",
    "    [405.60171008 177.75161535 461.23234462 238.10186431]\n",
    "    [159.76904333  26.10973097 371.53357512 219.45996917]]\n",
    "   ```\n",
    "   윗 부분은 각각의 인식 결과와 그 confidence score입니다.  \n",
    "   아래의 list 부분은 객체의 위치 인식 결과 입니다.  \n",
    "   위치는 `[xmin, ymin, xmax, ymax]`의 형식입니다.  \n",
    "   결과부분을 복사하여 메모장에 붙여넣기 해 둡니다.\n",
    "\n",
    "   이제 템플릿 B의 코드를 복사하신 후, 과제기입란에서 `confidence`와 `pred_box`를 해당 데이터에 맞게 수정합시다.  \n",
    "   대부분의 값은 템플릿 B에 이미 있습니다.  \n",
    "   수정할 부분은 `pred_conficence` 중 `0.11111111` 부분과, box 중 `333.33333333` 두 군데 입니다.  \n",
    "   수정된 결과가 `ans10`와 `ans11`에 저장됩니다. (템플릿에 이미 포함되어 있습니다.)  \n",
    "   (이 과정은 객체탐지 모델을 구동하고 그 추론결과인 confidence score와 bounding box를 저장하는 것입니다.)\n",
    "\n",
    "   이제 이 이미지의 annotation 작업물(Ground Truth)을 확인해 보겠습니다.\n",
    "   (참고: annotation이란 학습을 위해서 미리 참값인 ground truth를 사람이 직접 라벨링하는 작업, 혹은 그 작업의 산출물을 뜻합니다. 다른 말로는 라벨링, 혹은 라벨링 데이터라고도 부릅니다.)\n",
    "\n",
    "   `08-voc.ipynb`에 cell을 추가하고 다음 명령을 수행해 봅시다.  \n",
    "   ```\n",
    "   !cat data/VOC2012/Annotations/2007_000559.xml\n",
    "   ```\n",
    "   xml 출력 결과를 복사하여 메모장에 붙여넣기 해주세요.  \n",
    "   여기까지 끝나셨으면 `08-voc.ipynb` 실습 노트북은 종료하셔도 됩니다.   \n",
    "\n",
    "   xml 데이터의 내용을 잘 보시면 bottle과 tvmonitor만이 들어있습니다.  \n",
    "   이것은 80개 class인 coco dataset과 달리, VOC에는 20개의 class 밖에 없기 때문입니다.  \n",
    "   나머지 객체들은 ground truth에서 빠져있습니다.  \n",
    "   그래서 keyboard, cup, mouse의 annotation결과는 템플릿 소스코드 y_box에 수동으로 미리 추가해 두었습니다.\n",
    "\n",
    "   원래 VOC xml 데이터에 있는 bottle과 tvmonitor의 bounding box의 좌표값을 `y_box`에 옮겨서 완성합니다.  \n",
    "   `y_box` 데이터의 처음 두 줄을 수정하시면 됩니다.  \n",
    "   (이 과정은 객체탐지의 ground truth를 지정하는 작업입니다.)\n",
    "\n",
    "   수정된 결과가 `ans12`에 저장됩니다. (템플릿에 이미 포함되어 있습니다.)  \n",
    "\n",
    "   데이터가 수정되었으면, 정상적으로 실행되는 지 확인해 봅시다.  \n",
    "   출력된 이미지에서 파란색은 prediction된 결과이고, 주황색은 ground truth입니다.  \n",
    "   인터넷에서 matplotlib를 이용하여 사각형을 그리는 함수인 `patches.Rectangle`에 대해 검색하고 파라미터의 의미를 이해해 봅시다.  \n",
    "   파란색 box가 **그림 1**의 box와 일치하는 지 확인합시다.  \n",
    "\n",
    "2. IOU의 계산  \n",
    "\n",
    "   다음은 IOU를 계산하는 `bbox_iou`함수입니다.  \n",
    "   ```python\n",
    "   # 두 개의 bounding box의 iou 계산\n",
    "   def bbox_iou(box1, box2): # box1, box2는 [xmin, ymin, xmax, ymax]의 형식\n",
    "       # box의 넓이 계산: area = w * h\n",
    "       box1_area = (box1[2]-box1[0])*(box1[3]-box1[1])\n",
    "       box2_area = (box2[2]-box2[0])*(box2[3]-box2[1])\n",
    "    \n",
    "       # 교집합 box의 upper-left 좌표\n",
    "       left_up = np.maximum(box1[:2], box2[:2])\n",
    "       # 교집합 box의 lower-right 좌표\n",
    "       right_down = np.minimum(box1[2:], box2[2:])\n",
    "    \n",
    "       # 교집합 box의 width와 height\n",
    "       inter_section = np.maximum(right_down - left_up, 0.0)\n",
    "    \n",
    "       # 교집합 box의 넓이: area = w * h\n",
    "       inter_area = inter_section[0]*inter_section[1]\n",
    "    \n",
    "       # 합집합 면적\n",
    "       union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "       # IOU의 산출\n",
    "       iou = inter_area/union_area\n",
    "       return iou\n",
    "   ```\n",
    "   소스 코드를 읽고 이해해 봅시다.  \n",
    "   코드 중 교집합 box의 좌상위치(`left_up`)과 우하위치(`right_down`)를 계산하는 코드는 다음과 같습니다.  \n",
    "   ```python\n",
    "   left_up = np.maximum(box1[:2], box2[:2])\n",
    "   right_down = np.minimum(box1[2:], box2[2:])\n",
    "   ```\n",
    "   위의 코드가 어떻게 교집합의 위치를 추출하는 지를 `ans13`에 적어주세요.  \n",
    "\n",
    "   교집합 box의 width와 height를 계산하는 코드는 다음과 같습니다.   \n",
    "   ```python\n",
    "   right_down - left_up\n",
    "   ```\n",
    "   여기서 `right_down`과 `left_up`은 모두 `x,y`의 데이터가 저장된 `(2,)` shape의 텐서입니다.    \n",
    "   따라서 두 값을 빼면, width와 height가 동시에 생성됩니다.   \n",
    "   여기에 추가적으로 `np.maximum(..., 0.0)`이 사용되었습니다.  \n",
    "   즉, width와 height가 음수가 나오면 0으로 변경한다는 의미입니다.  \n",
    "\n",
    "   어떤 경우에 width혹은 height가 음수가 나오는지 `ans14`에 적어주세요.  \n",
    "\n",
    "   필요하다면 다음의 디버깅 코드를 이용하여, 실제로 두 box의 값을 변경하면서 IOU가 정상적으로 출력되는 지 확인해 봅시다.  \n",
    "   ```python\n",
    "   # IOU 테스트 코드 (이부분의 실험은 별도 cell에서 각자 수행해 보세요.)\n",
    "   sample_box1 = np.array([0.0, 0.0, 2.0, 2.0])\n",
    "   sample_box2 = np.array([1.0, 1.0, 3.0, 3.0])\n",
    "   print(bbox_iou(sample_box1, sample_box2))\n",
    "   ```\n",
    "\n",
    "   이 IOU 계산 함수를 `09_yolo.ipynb` 실습 파일 내의 `bbox_iou`함수와 비교하여 봅시다.  \n",
    "   실습 파일의 bbox_iou는 입력 box의 형식이 중심좌표, w,h의 형식이고 여러개의 box들의 iou를 동시에 계산한다는 점이 다릅니다.  \n",
    "   (numpy 대신에 tensorflow 텐서 계산 함수가 사용되었습니다.)  \n",
    "   과제에서 IOU를 계산하는 코드에, if문이나, for문이 사용되지 않았음에 주목합시다.  \n",
    "   전체를 tensor operation만을 이용하여 해결하였습니다.  \n",
    "   이것은 더 많은 여러개의 box에 대한 IOU를 동시에 계산하는 코드로 확장할 수 있게 합니다. (10주차 `09_yolo.ipynb`내의 bbox_iou처럼)  \n",
    "   이와 같은 코드는 확장 가능할 뿐 아니라, GPU에서의 병렬처리 속도를 증가시킵니다.  \n",
    "   따라서 딥러닝의 핵심 코드(모델, 손실함수) 들은 텐서와 텐서 연산만을 사용하여 처리하는 것이 바람직하다고 하겠습니다.  \n",
    "\n",
    "3. mAP의 계산 (여기서는 AP까지만 계산합니다.)  \n",
    "   이제 `tp` (True Positive) 여부를 산정합니다.   \n",
    "   class가 일치하고 IOU가 0.5이상이면 True Positive로 판정합니다. (VOC의 판정기준을 따랐습니다.)\n",
    "   ```python\n",
    "   tp = []\n",
    "   for p_c, p_b in zip(pred_class, pred_box):\n",
    "       tp_tmp = 0\n",
    "       for y_c, y_b in zip(y_class, y_box):\n",
    "           if y_c == p_c and bbox_iou(p_b, y_b) > 0.5:\n",
    "               tp_tmp = 1\n",
    "               break\n",
    "       tp.append(tp_tmp)\n",
    "   ```\n",
    "   YOLO의 추론결과를 보면 `person`이 오인식 되었다는 것을 알 수 있습니다.  \n",
    "   따라서 `tp` 중 `person`에 해당하는 것은 0이어야 합니다.  \n",
    "   마찬가지로 `fp`(False Positive) 중 `person`에 해당하는 것은 1이어야 합니다.  \n",
    "\n",
    "   이것을 이용해서 AP를 구해보겠습니다.  \n",
    "   원래는 같은 class들에서 AP를 구하는 것이나, 편의상 모든 class에 대해 구하도록 하겠습니다.  \n",
    "\n",
    "   AP의 계산(`voc_ap()`)에 대한 상세 설명은 `08_voc.ipynb`의 맨 마지막 부분에 예제와 소스 코드를 추가해 놓았습니다.  \n",
    "   각자 읽어보고 실행해 보도록 합시다.  \n",
    "\n",
    "   `voc_ap()` 함수의 입력은 recall과 precision입니다.  08-voc 실습 파일의 맨 마지막 데이터를 그래프로 출력해보면 다음과 같습니다.  \n",
    "   ![ap01](https://user-images.githubusercontent.com/38934308/195763479-57e9c4b4-2480-432e-b121-6ca460be832c.png)   \n",
    "   `voc_ap()`의 첫번째 단계는 맨 끝점 두개를 추가하는 것입니다. 템플릿 코드에 상세 설명이 있으니 읽어봐 주십시오. 끝점 두 개를 추가하면 다음과 같습니다.  \n",
    "   ![ap02](https://user-images.githubusercontent.com/38934308/195763478-782f102a-a11d-422a-bf90-d4611922fe3a.png)  \n",
    "   다음은 그래프가 항상 감소하도록 중간 값들을 매꾸게 됩니다. (빨간색)  \n",
    "   ![ap03](https://user-images.githubusercontent.com/38934308/195764558-e54a8a68-a58f-4d8b-95de-295f88f36b60.png)  \n",
    "   최종적으로 AP는 빨간색 그래프 아래쪽 면적(주황색 영역)이 됩니다.  \n",
    "   ![ap04](https://user-images.githubusercontent.com/38934308/195764557-52a5a450-e530-4cfc-953e-bce4bdaec247.png)  \n",
    "   AP의 계산은 이미 템플릿에 포함되어 있으므로 소스코드를 읽고 이해해 봅시다.  \n",
    "   이 AP를 `ans15`에 저장합니다. (이미 템플릿에 포함되어 있습니다.)  \n",
    "\n",
    "4. 왜 mAP를 성능 측정 지표로 하였을까요?  \n",
    "   꽤나 복잡하고 난해한 과정들일 수 있습니다.  \n",
    "   이것은 객체탐지 모델의 성능을 비교하기 위한 많은 노력에서 비롯되었습니다.\n",
    "   (Zhu, Mu (2004). \"Recall, Precision and Average Precision\")  \n",
    "   현재의 모델들은 모델의 출력결과 뿐 아니라 Confidence score를 동시에 출력합니다.  \n",
    "   비록 틀린 객체(FP)가 포함되더라도 가능한 한 많은 객체를 탐지하고 싶다면, confidence의 턱값을 낮추게 됩니다. 그렇지 않고, 놓치는 객체가 있더라도 오탐을 방지하고 싶다면 턱값을 높이게 됩니다. 그래서 턱값에 따른 성능의 변화를 총괄적으로 평균할 수 있도록 AP(Average Precision)이라는 성능 지표를 사용하는 것입니다. AP를 모든 클래스에 대해 평균한 것을 mAP(mean Average Precision)라고 부릅니다.  \n",
    "   그런데 mean도 평균이고 average도 평균이라서 이 용어가 다소 잘못되었다고 생각하는 사람들도 있습니다. AP는 평균이라기 보다는 정확하게는 precision-recall 그래프의 아래쪽 면적이기 때문에 다른 말로 area under the precision-recall curve(AUPRC)라고 부르는 경우도 있습니다. (자연어나 신호처리쪽에서 이런 용어가 사용되는데 의미는 AP와 비슷합니다.)  \n",
    "   만일 confidence score가 출력되지 않는다면, 혹은 confidence값이 필수값이 아니라면, 모델간의 성능비교를 위해서는 다른 성능지표, 예를 들면 precision, recall 혹은 이것들을 조합한 **F1 score**등을 이용해야 합니다. F1 score 역시 중요한 성능지표이므로 각자 인터넷에서 검색해보고 공부해 보도록 합시다.\n",
    "\n",
    "5. 이 과제를 출제한 목적  \n",
    "\n",
    "   이 과제는 여러 AI 모델의 성능 지표중에서도 가장 까다로운 편에 속하는 객체탐지의 AP (혹은 mAP)를 이해하기 위함입니다. 영상, 자연어, 신호 처리 등 다양한 딥러닝 분야에서 여러가지 성능 지표를 만나게 될 것입니다. 여러분이 직접 딥러닝 모델을 개발하지 않더라도 이러한 성능지표를 이해하고 있다면, 다른 개발팀(혹은 타회사)과의 협업시 중요한 목표와 개념을 이해하고 공유할 수 있게 됩니다. 그래서 경우에 따라서는 모델에 대한 이해 자체 보다도 성능지표에 대한 이해가 실용적으로는 더 필요한 경우가 많습니다.\n",
    "\n",
    "\n"
   ],
   "id": "c3e623ac"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glDYfiehhYQX"
   },
   "source": [
    "**과제 기입란**"
   ],
   "id": "glDYfiehhYQX"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0a542662",
    "is_executing": true
   },
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as mp_image\n",
    "from matplotlib import patches\n",
    "import numpy as np\n",
    "\n",
    "# YOLO의 객체 탐지 추론 결과 (coco dataset 기준)\n",
    "# data/VOC2012/JPEGImages/2007_000559.jpg 이미지의 YOLO 추론 결과\n",
    "pred_class = ['person', 'mouse', 'keyboard', 'bottle', 'cup', 'tvmonitor']\n",
    "\n",
    "# 실제 추론결과에 맞도록 데이터를 수정하세요.\n",
    "confidence = np.array([0.7952806, 0.9976528, 0.9980576, 0.9573481, 0.49765933, 0.9629159])\n",
    "pred_box = np.array(\n",
    "    [[176.16505921, 33.64296697, 341.38774166, 208.91536349],\n",
    "     [385.4623735, 311.9840765, 444.85818948, 348.11093758],\n",
    "     [79.10724729, 311.60360575, 380.4496864, 366.52458648],\n",
    "     [33.86356309, 247.92885244, 79.78276448, 354.13598821],\n",
    "     [405.60171008, 177.75161535, 461.23234462, 238.10186431],\n",
    "     [159.76904333, 26.10973097, 371.53357512, 219.45996917]]\n",
    ")\n",
    "\n",
    "ans10 = confidence[0]\n",
    "ans11 = pred_box[1][0]\n",
    "\n",
    "# data/VOC2012/JPEGImages/2007_000559.jpg 이미지의 Ground Truth\n",
    "y_class = ['bottle', 'tvmonitor', 'keyboard', 'cup', 'mouse']\n",
    "y_box = np.array(\n",
    "    [[36, 250, 79, 354],  # bottle 이 부분을 수정하세요\n",
    "     [160, 26, 371, 241],  # tvmonitor 이 부분을 수정하세요\n",
    "     [77, 313, 382, 366],  # keyboard\n",
    "     [405, 177, 461, 238],  # cup\n",
    "     [385, 311, 444, 348]]  # mouse\n",
    "    , dtype='float64'\n",
    ")\n",
    "\n",
    "ans12 = y_box[0:2]\n",
    "\n",
    "img_url = 'https://user-images.githubusercontent.com/38934308/141396886-34953093-8fb6-4984-a0e9-3598ba802828.jpg'\n",
    "img_path = keras.utils.get_file(\"2007_000559.jpg\", img_url)\n",
    "image = mp_image.imread(img_path)\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "for box in pred_box:\n",
    "    ax.add_patch(patches.Rectangle(\n",
    "        (box[0], box[1]), box[2] - box[0] + 1, box[3] - box[1] + 1,\n",
    "        edgecolor='tab:blue', linewidth=3,\n",
    "        fill=False\n",
    "    ))\n",
    "for box in y_box:\n",
    "    ax.add_patch(patches.Rectangle(\n",
    "        (box[0], box[1]), box[2] - box[0] + 1, box[3] - box[1] + 1,\n",
    "        edgecolor='orange', linewidth=3, alpha=0.7,\n",
    "        fill=False\n",
    "    ))\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 두 개의 bounding box의 iou 계산\n",
    "def bbox_iou(box1, box2):\n",
    "    # box의 넓이 계산: area = w * h\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    # 교집합 box의 upper-left 좌표\n",
    "    left_up = np.maximum(box1[:2], box2[:2])\n",
    "    # 교집합 box의 lower-right 좌표\n",
    "    right_down = np.minimum(box1[2:], box2[2:])\n",
    "\n",
    "    # 교집합 box의 width와 height\n",
    "    inter_section = np.maximum(right_down - left_up, 0.0)\n",
    "\n",
    "    # 교집합 box의 넓이: area = w * h\n",
    "    inter_area = inter_section[0] * inter_section[1]\n",
    "\n",
    "    # 합집합 면적\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "\n",
    "    # IOU의 산출\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "\n",
    "# 교집합 box의 upper-left 좌표와 lower-right좌표를 산출하는 원리를 적어주세요.\n",
    "ans13 = \"\"\"\n",
    "upper-left 좌표는 두 box의 좌상단 좌표 중에서 큰 값을 취하고, lower-right 좌표는 두 box의 우하단 좌표 중에서 작은 값을 취합니다.\n",
    "\"\"\"\n",
    "\n",
    "# 어떤 경우에 right_down - left_up이 음수가 되는 지 확인해 봅시다.\n",
    "ans14 = \"\"\"\n",
    "right_down - left_up이 음수가 되는 경우는 두 box가 겹치지 않는 경우입니다.\n",
    "\"\"\"\n",
    "\n",
    "tp = []\n",
    "for p_c, p_b in zip(pred_class, pred_box):\n",
    "    tp_tmp = 0\n",
    "    for y_c, y_b in zip(y_class, y_box):\n",
    "        if y_c == p_c and bbox_iou(p_b, y_b) > 0.5:\n",
    "            tp_tmp = 1\n",
    "            break\n",
    "    tp.append(tp_tmp)\n",
    "\n",
    "tp = np.array(tp)\n",
    "# False Positive\n",
    "fp = 1 - tp\n",
    "\n",
    "npos = len(y_box)  # 총 GT의 갯수\n",
    "\n",
    "# sort by confidence (confidence가 큰 순으로 정렬)\n",
    "sorted_ind = np.argsort(-confidence)\n",
    "tp = tp[sorted_ind]\n",
    "fp = fp[sorted_ind]\n",
    "\n",
    "print('sorted tp =', tp)\n",
    "print('sorted fp =', fp)\n",
    "\n",
    "# 누적 precision recall의 계산\n",
    "fp = np.cumsum(fp)\n",
    "tp = np.cumsum(tp)\n",
    "rec = tp / float(npos)  # 누적 tp를 GT의 갯수로 나눔\n",
    "prec = tp / (tp + fp)\n",
    "print('prec =', prec)\n",
    "print('rec =', rec)\n",
    "\n",
    "\n",
    "# AP (average precision의 계산)\n",
    "def voc_ap(rec, prec):\n",
    "    # 양끝에 시작값과 끝값을 추가한다.\n",
    "    # X축(recall)은 0과 1사이에 존재해야 하므로 양쪽 끝에 0,1을 추가한다.\n",
    "    # Y축(precision)은 recall이 1일때 0값을 추가한다. (이것은 면적에 영향을 주지 않는다.)\n",
    "    # Y축의 시작값은 0을 추가하지만 바로 아래부분의 precision이 항상 감소하도록 해야하는 조건\n",
    "    # 때문에, 자동으로 변경될 것이다. (시작값은 recision의 최대값으로 변화하게 된다.)\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # precision graph가 항상 감소하도록 값을 보정한다.\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # PR 곡선아래의 면적을 계산하기 위해서 X축(recall)이 변화하는 지점을 찾는다.\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # 그리고 다음의 합계를 구한다.\n",
    "    # sum ( (\\Delta recall) * prec )\n",
    "    # (\\Delta recall)이란 X축(recall)에서 recall이 변화하는 양이다.\n",
    "    # 즉, 직사각형의 밑변의 길이가 된다.\n",
    "    # 여기에 Y축 높이(precision)값을 곱하면 직사각형의 넓이가 된다.\n",
    "    # 이것을 모두 합(sum)하면, 그래프 하단의 넓이가 된다.\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "\n",
    "ap = voc_ap(rec, prec)\n",
    "print('ap =', ap)\n",
    "\n",
    "ans15 = ap\n",
    "\n",
    "# 본 과제를 수행한 시간이나 기타 의견 등이 있으시면 적어 주세요. (공란으로 두셔도 됩니다.)\n",
    "ans16 = \"\"\"\n",
    "훌륭한 강의를 해주셔서 언제나 감사하고있습니다.\n",
    "\"\"\""
   ],
   "id": "0a542662",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20b45b10"
   },
   "source": [
    "**다음은 답안의 형식을 확인하는 코드입니다. 실행해서 오류가 없는 지 확인합시다.**"
   ],
   "id": "20b45b10"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "63d15caf",
    "is_executing": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "64d1f082-4993-4c6d-f3dc-e11b8badf6fe"
   },
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# 답안의 형식을 점검합니다.\n",
    "\n",
    "error = False\n",
    "\n",
    "try:\n",
    "    if type(ans01) != str:\n",
    "        raise\n",
    "except:\n",
    "    error = True\n",
    "    print('ans01 error')\n",
    "\n",
    "try:\n",
    "    if type(ans02) != str:\n",
    "        raise\n",
    "except:\n",
    "    error = True\n",
    "    print('ans02 error')\n",
    "\n",
    "try:\n",
    "    if type(ans03) != str:\n",
    "        raise\n",
    "except:\n",
    "    error = True\n",
    "    print('ans03 error')\n",
    "\n",
    "try:\n",
    "    if type(ans04) != str:\n",
    "        raise\n",
    "except:\n",
    "    error = True\n",
    "    print('ans04 error')\n",
    "\n",
    "try:\n",
    "    if ans05.inputs[0].shape.as_list() != [None, 28, 28]:\n",
    "        raise\n",
    "except:\n",
    "    error = True\n",
    "    print('ans05 error')\n",
    "\n",
    "try:\n",
    "    if type(ans06) != float:\n",
    "        raise\n",
    "except:\n",
    "    error = True\n",
    "    print('ans06 error')\n",
    "\n",
    "try:\n",
    "    if type(ans07) != float:\n",
    "        raise\n",
    "except:\n",
    "    error = True\n",
    "    print('ans07 error')\n",
    "\n",
    "try:\n",
    "    if type(ans08) != str:\n",
    "        raise\n",
    "except:\n",
    "    error = True\n",
    "    print('ans08 error')\n",
    "\n",
    "try:\n",
    "    if type(ans09) != str:\n",
    "        raise\n",
    "except:\n",
    "    error = True\n",
    "    print('ans09 error')\n",
    "\n",
    "try:\n",
    "    if not isinstance(ans10, np.float64):\n",
    "        raise\n",
    "except:\n",
    "    error = True\n",
    "    print('ans10 error')\n",
    "\n",
    "try:\n",
    "    if not isinstance(ans10, np.float64):\n",
    "        raise\n",
    "except:\n",
    "    error = True\n",
    "    print('ans11 error')\n",
    "\n",
    "try:\n",
    "    if not isinstance(ans12, np.ndarray):\n",
    "        raise\n",
    "except:\n",
    "    error = True\n",
    "    print('ans12 error')\n",
    "\n",
    "try:\n",
    "    if type(ans13) != str:\n",
    "        raise\n",
    "except:\n",
    "    error = True\n",
    "    print('ans13 error')\n",
    "\n",
    "try:\n",
    "    if type(ans14) != str:\n",
    "        raise\n",
    "except:\n",
    "    error = True\n",
    "    print('ans14 error')\n",
    "\n",
    "try:\n",
    "    if not isinstance(ans15, np.float64):\n",
    "        raise\n",
    "except:\n",
    "    error = True\n",
    "    print('ans15 error')\n",
    "\n",
    "if error:\n",
    "    print('답안을 확인하여 주세요')\n",
    "else:\n",
    "    print('답안의 형식 확인이 완료되었습니다.')"
   ],
   "id": "63d15caf",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "답안의 형식 확인이 완료되었습니다.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XpkANKpjOsm"
   },
   "source": [
    "# 과제 제출 방법\n",
    "\n",
    "1. **런타임** -> **다시 시작 및 모두 실행**을 수행하여 정상적으로 결과가 출력되는 지 다시 한번 확인합니다.  \n",
    "\n",
    "2. **수정** -> **모든 출력 지우기**를 선택하여 cell의 출력을 지웁니다.\n",
    "\n",
    "3. **파일** -> **`.ipynb`** 다운로드를 선택하여 노트북을 다운로드 합니다.\n",
    "\n",
    "4. 파일 이름을 학번으로 변경합니다. 예) `202099999.ipynb`\n",
    "\n",
    "5. 노트북 파일을 제출하시면 됩니다."
   ],
   "id": "8XpkANKpjOsm"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "xfVyKMqrmwXA",
    "is_executing": true
   },
   "id": "xfVyKMqrmwXA",
   "execution_count": null,
   "outputs": []
  }
 ]
}
